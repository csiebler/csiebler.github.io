[{"content":"","date":null,"permalink":"/","section":"Clemens Siebler's Blog","summary":"","title":"Clemens Siebler's Blog"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"Introduction # In this post we\u0026rsquo;ll be looking into simple and quick ways to implement load balancing for Azure OpenAI using LangChain. In my last poste A Guide to Azure OpenAI Service\u0026rsquo;s Rate Limits and Monitoring we discussed how rate limits can negatively affect throughput and latency when running completions, so in this post, we\u0026rsquo;ll dive deeper and look at a few simple, practical solutions.\nSolutions # To run the code examples, make sure you have the latest versions of openai and langchain installed:\npip install openai --upgrade pip install langchain --upgrade In this post, we\u0026rsquo;ll be using openai==0.27.9 and langchain==0.0.271.\nFalling back to a larger model # One simple way to deal with the token or rate limit is to use a different or larger model when this happens. So instead of retrying and waiting for the same model to become \u0026ldquo;useable\u0026rdquo; again, we just switch to a larger model.\nThis approach is especially useful when using gpt4 on Azure OpenAI, as the default TPM (token per minute) limit for gpt4-8k is only 20k TPM. However, since we get an additional 60k TPM for gpt4-32k, why not use them? Surely, there is some additional, higher cost associated with this (the 32k model is more expensive for both prompt and completion tokens), but if throughput is the limiting factor, this is an easy fix.\nTo get this going, let\u0026rsquo;s first make sure we have both models deployed: Then create an .env with the credentials to the endpoint:\nAOAI_ENDPOINT_01=https://xxxxxxx.openai.azure.com/ AOAI_KEY_01=xxxxxx And lastly run the following Python code:\nimport os from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.prompts.chat import ( ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ) load_dotenv() kwargs = { \u0026#34;openai_api_type\u0026#34;: \u0026#34;azure\u0026#34;, \u0026#34;openai_api_base\u0026#34;: os.getenv(\u0026#34;AOAI_ENDPOINT_01\u0026#34;), \u0026#34;openai_api_key\u0026#34;: os.getenv(\u0026#34;AOAI_KEY_01\u0026#34;), \u0026#34;openai_api_version\u0026#34;: \u0026#34;2023-05-15\u0026#34;, \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: 100 } # Create a connection to gpt4-8k and gpt4-32k models llm_8k = AzureChatOpenAI(deployment_name=\u0026#34;gpt4-8k\u0026#34;, max_retries=0, **kwargs) llm_32k = AzureChatOpenAI(deployment_name=\u0026#34;gpt4-32k\u0026#34;, **kwargs) # This is where the magic happens llm = llm_8k.with_fallbacks([llm_32k]) system_message_prompt = SystemMessagePromptTemplate.from_template(\u0026#34;You are an AI assistant that tells jokes.\u0026#34;) human_message_prompt = HumanMessagePromptTemplate.from_template(\u0026#34;{text}\u0026#34;) chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt]) chain = chat_prompt | llm for i in range (1,10): result = chain.invoke({\u0026#34;text\u0026#34;: \u0026#34;Tell me a dad joke\u0026#34;}) print(result) Easy! This code will run happily and choose the 8k model, but in case it fails (we set max_retries=0), it will fall back to the 32k model and will keep retrying until it gives up (per default 6 tries). All we needed to do was create an AzureChatOpenAI for each model, and then configure the fallback. The remainder of the LangChain code stayed the same, so adding this to an existing codebases is pretty easy.\nIn summary, this approach the following benefits:\nOnly requires one Azure OpenAI resource Great for gpt4, where the quota is lower and quota increases are harder to get Its downsides are:\nTypically more costly Limited scalability (mostly due to the fact that getting access to gpt4 can still take some time) Falling back to multiple, but the same models # If we do not want to use a larger model, we can try an equally simple approach: falling back to the same model, but deployed in a different region and/or subscription. For example, with gpt-35-turbo we\u0026rsquo;ll get 240k TPM per subscription and region, so we if leverage four regions at once, we get close to 1m TPM! All we need to do is create a Azure OpenAI resource in each region and deploy our gpt-35-turbo model.\nNext, create an .env pointing to our Azure OpenAI endpoints in the different regions (could be all in the same or different subscriptions):\nAOAI_ENDPOINT_01=https://xxxxxxx.openai.azure.com/ AOAI_KEY_01=xxxxxx AOAI_ENDPOINT_02=https://xxxxxxx.openai.azure.com/ AOAI_KEY_02=xxxxxx AOAI_ENDPOINT_03=https://xxxxxxx.openai.azure.com/ AOAI_KEY_03=xxxxxx Lastly, let\u0026rsquo;s fire up Python:\nimport os from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.prompts.chat import ( ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ) load_dotenv() # get all credentials for connections from .env keys = [] for key, value in os.environ.items(): if key.startswith(\u0026#34;AOAI_ENDPOINT_\u0026#34;) and value is not None: num = key.split(\u0026#34;_\u0026#34;)[-1] keys.append((value, os.environ[f\u0026#34;AOAI_KEY_{num}\u0026#34;])) # Init connections to LLMs llms = [AzureChatOpenAI(openai_api_type=\u0026#34;azure\u0026#34;, openai_api_base=x[0], openai_api_key=x[1], openai_api_version=\u0026#34;2023-05-15\u0026#34;, deployment_name=\u0026#34;gpt-35-turbo\u0026#34;, temperature=0.7, max_tokens=100, max_retries=0) for x in keys] # Let\u0026#39;s keep the last LLM as the backup with retries, in case all other LLMs failed llms[-1].max_retries = 6 # Create our main LLM object that can fall back to all other LLMs llm = llms[0].with_fallbacks(llms[1:]) # Perform prompting as normal system_message_prompt = SystemMessagePromptTemplate.from_template(\u0026#34;You are an AI assistant that tells jokes.\u0026#34;) human_message_prompt = HumanMessagePromptTemplate.from_template(\u0026#34;{text}\u0026#34;) chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt]) chain = chat_prompt | llm for i in range (1,10): result = chain.invoke({\u0026#34;text\u0026#34;: \u0026#34;Tell me a dad joke\u0026#34;}) print(result) Same as before, we use the with_fallbacks() option from LangChain to fall back to the next model, then the next model, and so on. For the last model, we keep the max_retries at 6, to have a last resort for retries.\nIn summary, this approach the following benefits:\nVery large scalability Its downsides are:\nRequires multiple Azure OpenAI resources, potentially multiple subscriptions Considerations # For both approaches, it is important to consider under which conditions the with_fallbacks() method should be invoked. In our code example, we fall back on any error, but there might be use cases where we want to limit this to only token or rate limiting events. This can be easily adapted by adding the exceptions_to_handle parameter.\nFurthermore, we should also consider setting the request_timeout for the model itself. In order to avoid slow API calls, we could directly fall back to the next model after a timeout of e.g., 10 seconds. This can be easily achieved by creating the AzureChatOpenAI() connections with the request_timeout=10 parameter.\nLastly, we should still handle our Azure OpenAI calls with a try/catch block, just to make sure that we have an escape route in case everything fails.\nKey Takeaways # In this post we\u0026rsquo;ve discussed two simple ways on how to implement simple fallback mechanisms with LangChain that easily help us avoiding any token or rate limits in Azure OpenAI.\n","date":"24 August 2023","permalink":"/posts/azure_openai_load_balancing_langchain_with_fallbacks/","section":"Posts","summary":"Introduction # In this post we\u0026rsquo;ll be looking into simple and quick ways to implement load balancing for Azure OpenAI using LangChain.","title":"Using LangChain with Azure OpenAI to implement load balancing via fallbacks"},{"content":"Introduction # In this post we\u0026rsquo;re looking into how Azure OpenAI Service performs rate limiting, as well as monitoring. For this, we\u0026rsquo;ll be looking at different scenarios for using gpt-35-turbo and discuss how usage can be optimized.\nRate Limiting Mechanisms # There are two principal rate limiting strategies within Azure OpenAI Service which we need to understand:\nTokens per minute (TPM) Requests per minute (RPM) Let\u0026rsquo;s delve into the details of these:\nTPMs are allocated to a model deployment (like gpt-35-turbo), defining the maximum number of tokens that can be processed per minute in an ideal scenario. TPM is measured over one-minute windows. RPMs are derived from the TPM settings and calculated as follows: 1000 TPM = 6 RPM When either the TPM or RPM limit is reached, the API begins to return 429 errors, indicating the rate limit has been exceeded.\n{ \u0026#34;error\u0026#34;:{ \u0026#34;code\u0026#34;:\u0026#34;429\u0026#34;, \u0026#34;message\u0026#34;:\u0026#34;Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 5 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\u0026#34; } } Complexities # Upon inspecting the documentation (Understanding rate limits), we quickly realize that it\u0026rsquo;s not as simple as it might seem. For instance, the explanation of the TPM rate limit might be slightly confusing:\nTPM rate limits are based on the maximum number of tokens that are estimated to be processed by a request at the time the request is received. It isn\u0026rsquo;t the same as the token count used for billing, which is computed after all processing is completed.\nSo, what does this imply? The TPM limit is estimated by Prompt text and count, max_tokens parameter setting, and best_of parameter setting. This is logical since the API needs to anticipate the total tokens of a request prior to execution. But as it can\u0026rsquo;t predict the completion length, it uses the max_tokens provided by the user. best_of acts as a multiplier if the user asks for additional completions. So, once the token limit based on these estimations is hit within a minute, the API starts returning 429.\nNow let\u0026rsquo;s focus on RPMs:\nRPM rate limits are based on the number of requests received over time. The rate limit expects that requests be evenly distributed over a one-minute period\u0026hellip;To implement this behavior, Azure OpenAI Service evaluates the rate of incoming requests over a small period of time, typically 1 or 10 seconds.\nConsider an example where we have 1000 TPMs, which equates to 6 RPM. This implies we should have one request in a 10-second window. If another request is sent within this 10-second window, a 429 error should be received.\nValidation # To validate our understanding, let\u0026rsquo;s put it to the test. I have deployed gpt-35-turbo with a limit of 1k TPM, translating to 6 RPM. I\u0026rsquo;ve chosen a low limit so that it\u0026rsquo;s easier to hit the limits (TPM or RPM) from my local machine.\nAzure OpenAI Service Model Deployment with 1k TPM Also, remember to save your ENDPOINT and KEY in a .env file:\nENDPOINT=https://xxxxx.api.cognitive.microsoft.com/ KEY=xxxxx Requests per Minute Rate Limit Test # We\u0026rsquo;ll use a basic script for calling the Azure OpenAI API, keeping the max_tokens at 1 and using a very short prompt (each call should cost less than 20 tokens).\nimport os import requests from dotenv import load_dotenv max_tokens = 1 load_dotenv() url = f\u0026#34;{os.getenv(\u0026#39;ENDPOINT\u0026#39;)}/openai/deployments/gpt-35-turbo/chat/completions?api-version=2023-05-15\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;api-key\u0026#34;: os.getenv(\u0026#34;KEY\u0026#34;) } for i in range(1000): data = {\u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;messages\u0026#34;:[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;},{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi\u0026#34;}]} try: response = requests.post(url, headers=headers, json=data) status = response.status_code if (status != 200): print(response.json()) print(f\u0026#34;Finished call with status: {status}\u0026#34;) except Exception as e: print(f\u0026#34;Call failed with exception: {e}\u0026#34;) The log output should display:\nAn API call is processed approximately every 10 seconds Roughly 6 calls are successfully completed per minute (this could be 5, depending on the exact timing) The resource\u0026rsquo;s monitoring tab in the Azure Portal confirms this behavior (\u0026ldquo;Successful calls\u0026rdquo; and \u0026ldquo;Blocked calls\u0026rdquo;):\nAzure OpenAI request per minute rate limit test Tokens per Minute Rate Limit Test # Let\u0026rsquo;s repeat the process, this time aiming to breach the TPM limit. We simply change max_tokens to 1000.\nmax_tokens = 1000 The log output will show:\nOnly one API call will be processed per minute All other calls will result in 429 errors The monitoring tab validates this:\nAzure OpenAI tokens per minute rate limit test Thus, everything operates as anticipated!\nKey Takeaways # Now that we understand how the rate limiting works in detail, what can we learn from it:\nMaintain the max_tokens parameter at the smallest feasible value while ensuring it is large enough for your requirements. Increase the quota assigned to your model or distribute the load across multiple subscriptions or regions to optimize performance (also refer to Optimizing latency for Azure OpenAI Service). Consider fallback options such as turbo-16k or gpt-4-32k when reaching the quota limits of turbo or gpt-4-8k. These alternatives have independent quota buckets within the Azure OpenAI Service. Implement retry logic into your code. This strategy is particularly beneficial when you encounter request rate limits, as these limits reset after each 10-second window. Depending on your quota, the reset time might be even faster. If you need practical advise on how to implemented the recommendation outlined here in LangChain, have a look at this post: Using LangChain with Azure OpenAI to implement load balancing via fallbacks.\nHappy \u0026ldquo;rate-limit-avoiding\u0026rdquo;!\n","date":"1 August 2023","permalink":"/posts/understanding-azure-openai-rate-limits-monitoring/","section":"Posts","summary":"Introduction # In this post we\u0026rsquo;re looking into how Azure OpenAI Service performs rate limiting, as well as monitoring.","title":"A Guide to Azure OpenAI Service's Rate Limits and Monitoring"},{"content":"Introduction # In this post we\u0026rsquo;ll be looking into measuring and optimizing Azure OpenAI Service response latency by evaluating the deployed endpoints Azure OpenAI endpoints on a global scale. By optimizing latency, we can enable more real-time use cases, as well as maximize throughput for batch workloads. Our main goal in this exercise to reduce latency peaks that might show up here and there if any of the regions experiences significant load (noisy neighbors) or if we\u0026rsquo;re running into API rate limits.\nThis can be used to optimize latency for gpt-35-turbo, but can also be applied to gpt-4 model series.\nA word of caution, the solution discussed here won\u0026rsquo;t be perfect and won\u0026rsquo;t be able to avoid latency peaks completely. If you want to run latency sensitive use cases on Azure OpenAI where you can\u0026rsquo;t tolerate any peaks, I\u0026rsquo;d suggest to talk to your Microsoft Sales person regarding the Provisioned Throughput Model, which offers dedicated Azure OpenAI throughput capacity.\nIdeas for optimizing latency # If we want to build a \u0026ldquo;latency-optimized\u0026rdquo; app using Azure OpenAI, we could do the following approach:\nMeasure latency against a range of worldwide regions using a short test prompt Based on the call\u0026rsquo;s status code, latency, and rolling average latency (for instance, a decay rate of 0.8), select the fastest regions for the actual API call Execute the API calls Repeat this check at intervals between 10 and 60 seconds But what about the latency added by using an Azure region far from our application? Yes, this can cause additional latency. However, the main goal here is to prevent abrupt latency spikes. To give you some idea, here are a few quick tests:\nLatency from central Germany to canadaeast: \u0026lt;110ms Latency from central Germany to uksouth: \u0026lt;20ms Latency from central Germany to japaneast: \u0026lt;250ms Even considering a long distance, such as from the East coast to Singapore, the worst-case scenario is ~300ms of latency. However, if your app runs on Azure, you should experience significantly lower latency due to the use of the Microsoft backbone, as opposed to the public internet.\nIn context, running a prompt with 1000 input tokens and 200 completion tokens likely takes between half a second and two seconds to complete, so adding 100ms, 200ms, or 300ms doesn\u0026rsquo;t significantly impact our aim to prevent spikes.\nAccess configuration # First, let\u0026rsquo;s create an accounts.json that holds the endpoints and access keys for all the regions we want to test. In this case, I\u0026rsquo;ve just created Azure OpenAI resources in all regions where I still had capacity left:\n[ { \u0026#34;endpoint\u0026#34;: \u0026#34;https://canadaeast.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://eastus2.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://francecentral.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://japaneast.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://northcentralus.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://uksouth.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; } ] Testing latency via Python # To begin, install requests:\npip install requests We opt for simple HTTP requests over the openai SDK due to easier management of call timeouts and status codes.\nHere\u0026rsquo;s a sample script for the job:\nimport json import time import requests decay_rate = 0.8 http_timeout = 10 test_interval = 15 with open(\u0026#39;accounts.json\u0026#39;, \u0026#39;r\u0026#39;) as f: accounts = json.load(f) def get_latency_for_endpoint(endpoint, key): url = f\u0026#34;{endpoint}/openai/deployments/gpt-35-turbo/chat/completions?api-version=2023-05-15\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;api-key\u0026#34;: key } data = {\u0026#34;max_tokens\u0026#34;: 1, \u0026#34;messages\u0026#34;:[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;},{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi\u0026#34;}]} try: t_start = time.time() response = requests.post(url, headers=headers, json=data, timeout=http_timeout) latency = (time.time() - t_start) status = response.status_code except Exception as e: status = 500 latency = http_timeout # print(response.json()) print(f\u0026#34;Endpoint: {endpoint}, Status: {status}, Latency: {latency}s\u0026#34;) return { \u0026#34;ts\u0026#34;: time.strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, time.localtime()), \u0026#34;status\u0026#34;: status, \u0026#34;latency\u0026#34;: latency, } stat = {} for account in accounts: stat[account[\u0026#39;endpoint\u0026#39;]] = { \u0026#39;last_updated\u0026#39;: None, \u0026#39;status\u0026#39;: None, \u0026#39;latency\u0026#39;: None, \u0026#39;latency_ra\u0026#39;: 0 } while(True): for account in accounts: endpoint = account[\u0026#39;endpoint\u0026#39;] key = account[\u0026#39;key\u0026#39;] result = get_latency_for_endpoint(endpoint, key) stat[endpoint][\u0026#39;last_updated\u0026#39;] = result[\u0026#39;ts\u0026#39;] stat[endpoint][\u0026#39;status\u0026#39;] = result[\u0026#39;status\u0026#39;] stat[endpoint][\u0026#39;latency\u0026#39;] = result[\u0026#39;latency\u0026#39;] stat[endpoint][\u0026#39;latency_ra\u0026#39;] = decay_rate * result[\u0026#39;latency\u0026#39;] + (1-decay_rate) * stat[endpoint][\u0026#39;latency_ra\u0026#39;] print(json.dumps(stat,indent=4)) time.sleep(test_interval) In this script, endpoints are checked every 15 seconds, with timeouts set at 10 seconds. A rolling average with a decay rate of 0.8 is calculated.\nA response from a single prompt will look like this:\n{ \u0026#34;id\u0026#34;:\u0026#34;chatcmpl-.....\u0026#34;, \u0026#34;object\u0026#34;:\u0026#34;chat.completion\u0026#34;, \u0026#34;created\u0026#34;:1690872556, \u0026#34;model\u0026#34;:\u0026#34;gpt-35-turbo\u0026#34;, \u0026#34;choices\u0026#34;:[ { \u0026#34;index\u0026#34;:0, \u0026#34;finish_reason\u0026#34;:\u0026#34;length\u0026#34;, \u0026#34;message\u0026#34;:{ \u0026#34;role\u0026#34;:\u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;:\u0026#34;Hello\u0026#34; } } ], \u0026#34;usage\u0026#34;:{ \u0026#34;completion_tokens\u0026#34;:1, \u0026#34;prompt_tokens\u0026#34;:14, \u0026#34;total_tokens\u0026#34;:15 } } Overall cost per call is 15 tokens, which would cost us 30 days * 24 hours * 60 minutes * 4 requests/minute * 15 tokens * $0.002 / 1000 tokens = $5.2 / month per region in a month. Not sure if we need to test every every 15 seconds, or if every minute is sufficient. In terms of requests/minute, Azure OpenAI gives 1440 requests/minute per region and subscription, so sacrificing 4 calls is less than 0.3%.\nRunning the script over a period yields data such as:\n{ \u0026#34;https://canadaeast.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:25\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.5866355895996094, \u0026#34;latency_ra\u0026#34;: 0.5867746781616211 }, \u0026#34;https://eastus2.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:25\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.5309584140777588, \u0026#34;latency_ra\u0026#34;: 0.5271010751342773 }, \u0026#34;https://francecentral.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:26\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.725212812423706, \u0026#34;latency_ra\u0026#34;: 0.6279167041015624 }, \u0026#34;https://japaneast.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:27\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 1.0203375816345215, \u0026#34;latency_ra\u0026#34;: 1.0150870689697267 }, \u0026#34;https://northcentralus.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:28\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.7335877418518066, \u0026#34;latency_ra\u0026#34;: 0.7090948748168945 }, \u0026#34;https://uksouth.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:28\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.2238612174987793, \u0026#34;latency_ra\u0026#34;: 0.22408719714355468 } } We can clearly see that japaneast is the slowest, but as discussed before, latency from my machine to this region is already ~250ms, which probably explains it.\nMoving forward # While the above script is functional, a practical application should account for:\nExecution: The script could be executed with a timer in an Azure Function, persisting results into Azure Blob or Azure CosmosDB. The app would then query the current status periodically, caching responses and making regional choices based on current latency and the rolling average. Rate-limiting: Azure OpenAI Service defaults to 240k tokens per minute (TPMs) for gpt-35-turbo per region and subscription (as of 08/01/2023). If the test prompt encounters a limit for a region, it will be marked with status 429. Consequently, the app should then pick the next best option. Fallback measures: In case limits are reached across most regions, ensure it\u0026rsquo;s not because of the http timeout set for the POST request. In such unlikely scenarios, temporarily increase the http timeout value to identify regions still responding. Summary # This post has presented an easy approach to measure Azure OpenAI response latency across the globe. By sending a tiny prompt, waiting for its completion, and then choosing the best-performing region, we can optimize our actual API calls and hopefully minimize latency spikes. While this will likely reduce the latency spikes you\u0026rsquo;ll see, it won\u0026rsquo;t fully eliminate them. If your workload can\u0026rsquo;t tolerate any any spikes, I\u0026rsquo;d suggest you to talk to your Microsoft Sales person regarding the Provisioned Throughput Model.\n","date":"1 August 2023","permalink":"/posts/optimizing-latency-azure-openai/","section":"Posts","summary":"Introduction # In this post we\u0026rsquo;ll be looking into measuring and optimizing Azure OpenAI Service response latency by evaluating the deployed endpoints Azure OpenAI endpoints on a global scale.","title":"Optimizing latency for Azure OpenAI Service"},{"content":"Introduction # Recently, Azure Cognitive Search introduced vector search for indexing, storing, and retrieving vector embeddings from a search index. In this post, we\u0026rsquo;ll look into how we can use this to chat with your private data, similar to ChatGPT. So besides Azure Cognitive Search we\u0026rsquo;ll be using LangChain and Azure OpenAI Service. As the underlying Large Language Model, we\u0026rsquo;ll be using gpt-3.5-turbo (the \u0026ldquo;ChatGPT\u0026rdquo; model).\nTutorial # First, create a .env and add your Azure OpenAI Service details and Azure Cognitive Search details:\nOPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_KEY=xxxxxx OPENAI_API_VERSION=2023-05-15 AZURE_COGNITIVE_SEARCH_SERVICE_NAME=https://xxxxxxx.search.windows.net AZURE_COGNITIVE_SEARCH_API_KEY=xxxxxx AZURE_COGNITIVE_SEARCH_INDEX_NAME=xxxxxx Next, make sure that you have gpt-35-turbo and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service Model Deployments Let\u0026rsquo;s install the latest versions of openai, langchain, and azure-search-documents (which is used under the hood by LangChain) via pip:\npip install openai --upgrade pip install langchain --upgrade pip install azure-search-documents --pre --upgrade For azure-search-documents, we need the preview version, as only this one includes vector search capabilities. In this post, we\u0026rsquo;re using openai==0.27.8, langchain==0.0.245, and azure-search-documents==11.4.0b6.\nOk, let\u0026rsquo;s start writing some code. First, let\u0026rsquo;s initialize our Azure OpenAI Service connection, create the LangChain objects, and create our Azure Search connection:\nimport os import openai from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import AzureSearch from langchain.document_loaders import DirectoryLoader from langchain.document_loaders import TextLoader from langchain.text_splitter import TokenTextSplitter from langchain.chains import ConversationalRetrievalChain from langchain.prompts import PromptTemplate # Load environment variables load_dotenv() # Configure OpenAI API openai.api_type = \u0026#34;azure\u0026#34; openai.api_base = os.getenv(\u0026#39;OPENAI_API_BASE\u0026#39;) openai.api_key = os.getenv(\u0026#39;OPENAI_API_KEY\u0026#39;) openai.api_version = os.getenv(\u0026#39;OPENAI_API_VERSION\u0026#39;) # Initialize gpt-35-turbo and our embedding model llm = AzureChatOpenAI(deployment_name=\u0026#34;gpt-35-turbo\u0026#34;) embeddings = OpenAIEmbeddings(deployment_id=\u0026#34;text-embedding-ada-002\u0026#34;, chunk_size=1) # Connect to Azure Cognitive Search acs = AzureSearch(azure_search_endpoint=os.getenv(\u0026#39;AZURE_COGNITIVE_SEARCH_SERVICE_NAME\u0026#39;), azure_search_key=os.getenv(\u0026#39;AZURE_COGNITIVE_SEARCH_API_KEY\u0026#39;), index_name=os.getenv(\u0026#39;AZURE_COGNITIVE_SEARCH_INDEX_NAME\u0026#39;), embedding_function=embeddings.embed_query) If the Azure Search index does not exist yet, it will be created automatically.\nNext, we can load up a bunch of text files, chunk them up and embed them. LangChain supports a lot of different document loaders, which makes it easy to adapt to other data sources and file formats. You can download the sample data here.\nloader = DirectoryLoader(\u0026#39;data/qna/\u0026#39;, glob=\u0026#34;*.txt\u0026#34;, loader_cls=TextLoader, loader_kwargs={\u0026#39;autodetect_encoding\u0026#39;: True}) documents = loader.load() text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0) docs = text_splitter.split_documents(documents) # Add documents to Azure Search acs.add_documents(documents=docs) Lastly, we can create our document question-answering chat chain. In this case, we specify the condense question prompt, which converts the user\u0026rsquo;s question to a standalone question (using the chat history), in case the user asked a follow-up question:\n# Adapt if needed CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\u0026#34;\u0026#34;\u0026#34;Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\u0026#34;\u0026#34;\u0026#34;) qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=acs.as_retriever(), condense_question_prompt=CONDENSE_QUESTION_PROMPT, return_source_documents=True, verbose=False) Let\u0026rsquo;s ask a question:\nchat_history = [] query = \u0026#34;what is Azure OpenAI Service?\u0026#34; result = qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: chat_history}) print(\u0026#34;Question:\u0026#34;, query) print(\u0026#34;Answer:\u0026#34;, result[\u0026#34;answer\u0026#34;]) From where, we can also ask follow up questions:\nchat_history = [(query, result[\u0026#34;answer\u0026#34;])] query = \u0026#34;Which regions does the service support?\u0026#34; result = qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: chat_history}) print(\u0026#34;Question:\u0026#34;, query) print(\u0026#34;Answer:\u0026#34;, result[\u0026#34;answer\u0026#34;]) This should yield the following (or similar) output (btw, this information is outdated!):\nQuestion: what is Azure OpenAI Service? Answer: Azure OpenAI Service is a service provided by Microsoft that gives users access to OpenAI\u0026#39;s language models such as GPT-3, Codex and Embeddings series. Users can access the service through REST APIs, Python SDK, or our web-based interface in the Azure OpenAI Studio. The service can be used for content generation, summarization, semantic search, and natural language to code translation. Azure OpenAI offers private networking, regional availability, and responsible AI content filtering. Access to the service is currently limited due to high demand and upcoming product improvements. Question: Which regions does the service support? Answer: Azure OpenAI Service is currently available in the East US, South Central US, and West Europe regions. Looks good! Since we use the follow-up question prompt, LangChain converts the latest question to a follow up question by resolving it via the context.\nSummary # In this blog post, we discussed how we can use Azure Cognitive Search, LangChain, and Azure OpenAI Service to build a ChatGPT-like experience, but over private data. We used embeddings and Azure Cognitive Search to enable the document retrieval step and then used the gpt-3.5-turbo model to generate an answer from the retrieved documents.\nSpecial thanks goes to Fabrizio Ruocco who pushed the PR for Azure Cognitive Search integration to LangChain!\n","date":"27 July 2023","permalink":"/posts/azure-search-vector-search-openai-langchain/","section":"Posts","summary":"Introduction # Recently, Azure Cognitive Search introduced vector search for indexing, storing, and retrieving vector embeddings from a search index.","title":"Using Azure Search for vector search with Azure OpenAI and LangChain"},{"content":"Introduction # In this post we briefly discuss how LangChain and gpt-35-turbo (the model behind ChatGPT) can be used with Azure OpenAI Service.\nTutorial # First, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_VERSION=2023-05-15 Next, make sure that you have gpt-35-turbo deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service ChatGPT Model Deployment Let\u0026rsquo;s install/upgrade to the latest versions of openai and langchain via pip:\npip install openai --upgrade pip install langchain --upgrade Here, we\u0026rsquo;re using openai==0.27.8 and langchain==0.0.240. This is super important, as older versions of the openai Python SDK do not support the API version needed to access gpt-35-turbo.\nFinally, we can run our sample code:\nBy setting the openai configuration, we force LangChain (which uses the OpenAI Python SDK under the hood) to talk to Azure OpenAI instead of OpenAI directly.\nSummary # In this blog post, we discussed how we can use the ChatGPT API (gpt-35-turbo model) with Azure OpenAI Service and LangChain.\n","date":"4 May 2023","permalink":"/posts/using-langchain-and-chatgpt-turbo-with-azure-openai-service/","section":"Posts","summary":"Introduction # In this post we briefly discuss how LangChain and gpt-35-turbo (the model behind ChatGPT) can be used with Azure OpenAI Service.","title":"Using LangChain and gpt-3.5-turbo (ChatGPT API) with Azure OpenAI Service"},{"content":"Introduction # In this post we briefly discuss how LlamaIndex 🦙 (GPT Index) and gpt-35-turbo (the model behind ChatGPT) can be used with Azure OpenAI Service.\nIf you want a short into to using Azure OpenAI Service with Llama-Index, have a look at this post: posts/using-gpt-index-llamaindex-with-azure-openai-service/\nTutorial # First, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_VERSION=2023-05-15 Next, make sure that you have gpt-35-turbo and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service ChatGPT Model Deployment Let\u0026rsquo;s install/upgrade to the latest versions of openai, langchain, and llama-index via pip:\npip install openai --upgrade pip install langchain --upgrade pip install llama-index --upgrade Here, we\u0026rsquo;re using openai==0.27.8, langchain==0.0.240, and llama-index==0.7.11.post1.\nSummary # In this blog post, we discussed how we can use the ChatGPT API (gpt-35-turbo model) with Azure OpenAI Service and Llama-Index.\n","date":"4 May 2023","permalink":"/posts/using-llamaindex-with-turbo-and-azure-openai-service/","section":"Posts","summary":"Introduction # In this post we briefly discuss how LlamaIndex 🦙 (GPT Index) and gpt-35-turbo (the model behind ChatGPT) can be used with Azure OpenAI Service.","title":"Using LlamaIndex and gpt-3.5-turbo (ChatGPT API) with Azure OpenAI Service"},{"content":"Introduction video # This video post is a short introduction OpenAI\u0026rsquo;s embeddings using the Azure OpenAI Service:\nCode samples # Initial Embedding Testing # Initialize text-embedding-ada-002 on Azure OpenAI Service using LangChain:\nimport os import openai from dotenv import load_dotenv from langchain.embeddings import OpenAIEmbeddings from openai.embeddings_utils import cosine_similarity # Load environment variables (contains OPENAI_API_BASE and OPENAI_API_KEY) load_dotenv() # Configure Azure OpenAI Service API openai.api_type = \u0026#34;azure\u0026#34; openai.api_version = \u0026#34;2022-12-01\u0026#34; openai.api_base = os.getenv(\u0026#39;OPENAI_API_BASE\u0026#39;) # looks like https://********.openai.azure.com/ openai.api_key = os.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) # Initialize embedding model embeddings = OpenAIEmbeddings(model=\u0026#34;text-embedding-ada-002\u0026#34;, chunk_size=1) Do some quick testing with embeddings:\ntext = \u0026#34;This is just a test\u0026#34; # Embed a single document e = embeddings.embed_query(text) print(e) print(len(e)) Try out a few more examples:\ndef get_similarity_from_text(text1, text2): e1 = embeddings.embed_query(text1) e2 = embeddings.embed_query(text2) s = cosine_similarity(e1, e2) print(f\u0026#34;Similarity between \u0026#39;{text1}\u0026#39; and \u0026#39;{text2}\u0026#39;: {s}\u0026#34;) get_similarity_from_text(\u0026#34;boy\u0026#34;, \u0026#34;girl\u0026#34;) get_similarity_from_text(\u0026#34;boy\u0026#34;, \u0026#34;man\u0026#34;) get_similarity_from_text(\u0026#34;boy\u0026#34;, \u0026#34;woman\u0026#34;) get_similarity_from_text(\u0026#34;Germany\u0026#34;, \u0026#34;Berlin\u0026#34;) get_similarity_from_text(\u0026#34;France\u0026#34;, \u0026#34;Paris\u0026#34;) get_similarity_from_text(\u0026#34;Germany\u0026#34;, \u0026#34;Paris\u0026#34;) And with longer text:\nstory1 = \u0026#34;Once upon a time, there was a little girl named Sarah. She lived with her family in a small village near the woods. Every morning Sarah would wake up early, get dressed, and go outside to play.\u0026#34; story2 = \u0026#34;One day, while Sarah was playing in the woods, she noticed a small rabbit hopping around in the grass. She decided to follow it, hoping to see where it would go. The rabbit kept hopping until it reached the entrance of a small cave.\u0026#34; insurance_clause = \u0026#34;In the event of any losses or damages incurred by either party due to unforeseen circumstances, both parties agree to be liable for their respective liabilities and hold the other harmless for any and all damages and losses sustained.\u0026#34; get_similarity_from_text(story1, story2) get_similarity_from_text(story1, insurance_clause) Movie Description Embedding - Recommendations # Load movie csv:\nimport pandas as pd df = pd.read_csv(\u0026#39;../data/movies/movies.csv\u0026#39;) # only keep original_title and overview column df = df[[\u0026#39;original_title\u0026#39;, \u0026#39;overview\u0026#39;]] # add embedding column with embedding df[\u0026#39;embedding\u0026#39;] = df[\u0026#39;overview\u0026#39;].apply(lambda x: embeddings.embed_query(x)) df.head() Calculate most similar movies (according to their description embedding):\n# Let\u0026#39;s pick a movie that exists in df, keeping in mind we only have 500 movies in it! movie = \u0026#34;Frozen\u0026#34; # get embedding for movie e = df[df[\u0026#39;original_title\u0026#39;] == movie][\u0026#39;embedding\u0026#39;].values[0] # get cosine similarity between movie and all other movies and sort ascending similarities = df[\u0026#39;embedding\u0026#39;].apply(lambda x: cosine_similarity(x, e)) # combine original_title from df and similiaries and sort ascending by similarity recommendations = pd.concat([df[\u0026#39;original_title\u0026#39;], similarities], axis=1).sort_values(by=\u0026#39;embedding\u0026#39;, ascending=False) recommendations.head() City Name Embeddings Example # Take a bunch of city names and create embeddings from them:\nimport pandas as pd # generate an array with different city names cities = [\u0026#39;Jakarta\u0026#39;, \u0026#39;Hong Kong\u0026#39;, \u0026#39;Tokyo\u0026#39;, \u0026#39;Bangkok\u0026#39;, \u0026#39;Shanghai\u0026#39;, \u0026#39;Ho Chi Minh City\u0026#39;, \u0026#39;Beijing\u0026#39;, \u0026#39;New York\u0026#39;, \u0026#39;Los Angeles\u0026#39;, \u0026#39;San Francisco\u0026#39;, \u0026#39;Chicago\u0026#39;, \u0026#39;Paris\u0026#39;, \u0026#39;Rome\u0026#39;, \u0026#39;Barcelona\u0026#39;, \u0026#39;Madrid\u0026#39;, \u0026#39;Amsterdam\u0026#39;, \u0026#39;Berlin\u0026#39;] df = pd.DataFrame({\u0026#34;city\u0026#34;: cities}) # generate embeddings for all cities df[\u0026#39;cities_embeddings\u0026#39;] = [embeddings.embed_query(city) for city in cities] Use PCA (or tSNE) to reduce the dimensionality for visualization:\n# use PCA to reduce dimensionality to 3 from sklearn.decomposition import PCA pca = PCA(n_components=3) t = pca.fit_transform(df[\u0026#39;cities_embeddings\u0026#39;].tolist()) Plot to 3d scatter plot:\n# draw t in a scatter plot and put names on each point import matplotlib.pyplot as plt fig = plt.figure(figsize=(10,10)) ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.scatter(t[:,0], t[:,1], t[:,2]) for i, txt in enumerate(cities): ax.text(t[i,0], t[i,1], t[i,2], txt) plt.show() Results:\nCities embeddings visualized ","date":"18 April 2023","permalink":"/posts/azure-openai-service-embeddings-tutorial/","section":"Posts","summary":"Introduction video # This video post is a short introduction OpenAI\u0026rsquo;s embeddings using the Azure OpenAI Service:","title":"Embeddings Tutorial using Azure OpenAI Service"},{"content":"Introduction # In this post we discuss how we can build a system that allows you to chat with your private data, similar to ChatGPT. For this, we\u0026rsquo;ll be using LangChain, Azure OpenAI Service, and Faiss as our vector store. As the underlying Large Language Model, we\u0026rsquo;ll be using gpt-3.5-turbo (the \u0026ldquo;ChatGPT\u0026rdquo; model).\nTutorial # First, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_VERSION=2023-05-15 Next, make sure that you have gpt-35-turbo and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service Model Deployments Let\u0026rsquo;s install the latest versions of openai and langchain via pip:\npip install openai --upgrade pip install langchain --upgrade In this post, we\u0026rsquo;re using openai==0.27.8 and langchain==0.0.240.\nOk, let\u0026rsquo;s start writing some code. First, let\u0026rsquo;s initialize our Azure OpenAI Service connection and create the LangChain objects:\nimport os import openai from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.embeddings import OpenAIEmbeddings # Load environment variables (set OPENAI_API_KEY, OPENAI_API_BASE, and OPENAI_API_VERSION in .env) load_dotenv() # Configure OpenAI API openai.api_type = \u0026#34;azure\u0026#34; openai.api_base = os.getenv(\u0026#39;OPENAI_API_BASE\u0026#39;) openai.api_key = os.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) openai.api_version = os.getenv(\u0026#39;OPENAI_API_VERSION\u0026#39;) # Initialize gpt-35-turbo and our embedding model llm = AzureChatOpenAI(deployment_name=\u0026#34;gpt-35-turbo\u0026#34;) embeddings = OpenAIEmbeddings(deployment_id=\u0026#34;text-embedding-ada-002\u0026#34;, chunk_size=1) Next, we can load up a bunch of text files, chunk them up and embed them. LangChain supports a lot of different document loaders, which makes it easy to adapt to other data sources and file formats. You can download the sample data here.\nfrom langchain.document_loaders import DirectoryLoader from langchain.document_loaders import TextLoader from langchain.text_splitter import TokenTextSplitter loader = DirectoryLoader(\u0026#39;data/qna/\u0026#39;, glob=\u0026#34;*.txt\u0026#34;, loader_cls=TextLoader, loader_kwargs={\u0026#39;autodetect_encoding\u0026#39;: True}) documents = loader.load() text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0) docs = text_splitter.split_documents(documents) Next, let\u0026rsquo;s ingest documents into Faiss so we can efficiently query our embeddings:\nfrom langchain.vectorstores import FAISS db = FAISS.from_documents(documents=docs, embedding=embeddings) Lastly, we can create our document question-answering chat chain. In this case, we specify the question prompt, which converts the user\u0026rsquo;s question to a standalone question, in case the user asked a follow-up question:\nfrom langchain.chains import ConversationalRetrievalChain from langchain.prompts import PromptTemplate # Adapt if needed CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\u0026#34;\u0026#34;\u0026#34;Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\u0026#34;\u0026#34;\u0026#34;) qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), condense_question_prompt=CONDENSE_QUESTION_PROMPT, return_source_documents=True, verbose=False) Let\u0026rsquo;s ask a question:\nchat_history = [] query = \u0026#34;what is Azure OpenAI Service?\u0026#34; result = qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: chat_history}) print(\u0026#34;Question:\u0026#34;, query) print(\u0026#34;Answer:\u0026#34;, result[\u0026#34;answer\u0026#34;]) From where, we can also ask follow up questions:\nchat_history = [(query, result[\u0026#34;answer\u0026#34;])] query = \u0026#34;Which regions does the service support?\u0026#34; result = qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: chat_history}) print(\u0026#34;Question:\u0026#34;, query) print(\u0026#34;Answer:\u0026#34;, result[\u0026#34;answer\u0026#34;]) This should yield the following (or similar) output:\nQuestion: what is Azure OpenAI Service? Answer: Azure OpenAI is a service that provides REST API access to OpenAI\u0026#39;s language models, including GPT-3, Codex, and Embeddings. These models can be used for content generation, summarization, semantic search, and natural language to code translation. The service can be accessed through REST APIs, Python SDK, or a web-based interface in the Azure OpenAI Studio. Azure OpenAI offers virtual network support, managed identity, and responsible AI content filtering. However, access to the service is currently limited due to high demand and Microsoft\u0026#39;s commitment to responsible AI. Question: Which regions does the service support? Answer: Azure OpenAI Service is currently available in the following regions: East US, South Central US, and West Europe. Looks good! Since we use the follow-up question prompt, LangChain converts the latest question to a follow up question, hence resolving it via the context.\nSummary # In this blog post, we discussed how we can use LangChain, Azure OpenAI Service, and Faiss to build a ChatGPT-like experience, but over private data. We used embeddings and Faiss to enable the document retrieval step and then used the gpt-3.5-turbo model to generate an answer from the retrieved documents.\n","date":"3 April 2023","permalink":"/posts/chatting-private-data-langchain-azure-openai-service/","section":"Posts","summary":"Introduction # In this post we discuss how we can build a system that allows you to chat with your private data, similar to ChatGPT.","title":"Chatting with your private data using LangChain with Azure OpenAI Service"},{"content":"Introduction # In this post we briefly discuss how LangChain can be used with Azure OpenAI Service.\nLangChain is a powerful tool for building language models that can be used for a variety of applications, from personal assistants to question answering and chatbots. Its modules provide support for different model types, prompt management, memory, indexes, chains, and agents, making it easy to customize and create unique language models. LangChain also offers guidance and assistance for use cases such as interacting with APIs, extracting structured information from text, summarization, and evaluation.\nAzure OpenAI Service is a cloud-based platform that enables you to access and use OpenAI\u0026rsquo;s powerful LLMs, such as GPT-4, GPT-3 and Codex. It offers a simple and secure way to integrate these models into your applications, with features such as authentication, encryption, scaling, monitoring, etc.\nTutorial # First, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_VERSION=2023-05-15 Next, make sure that you have text-davinci-003 and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service Model Deployments Let\u0026rsquo;s install the latest versions of openai and langchain via pip:\npip install openai --upgrade pip install langchain --upgrade Here, we\u0026rsquo;re using openai==0.27.8 and langchain==0.0.240.\nFinally, we can run our test code:\nBy setting the openai configuration, we force LangChain (which uses the OpenAI Python SDK under the hood) to talk to Azure instead of OpenAI directly. From here, we can initialize our LLM and use it. For embeddings, we need to make sure to set the chunk_size to 1, as Azure OpenAI Service API does not support embedding multiple pieces of text in one API call at once.\nSummary # In this blog post, we discussed how to use LangChain and Azure OpenAI Service together to build complex LLM-based applications with just a few lines of code. In order to use Azure OpenAI Service, we only needed a few lines of configuration for using text-davinci-003 and text-embedding-ada-002, instead of relying on models hosted on openai.com.\n","date":"27 March 2023","permalink":"/posts/using-langchain-with-azure-openai-service/","section":"Posts","summary":"Introduction # In this post we briefly discuss how LangChain can be used with Azure OpenAI Service.","title":"Using LangChain with Azure OpenAI Service"},{"content":"Introduction # In this post we briefly discuss how LlamaIndex 🦙 (GPT Index) can be used with Azure OpenAI Service.\nLlamaIndex 🦙 (GPT Index) is a project that provides a central interface to connect your large language models (LLMs) with external data. It allows you to index your data for various LLM tasks, such as text generation, summarization, question answering, etc., and remove concerns over prompt size limitations. It also supports data connectors to your common data sources and provides cost transparency and tools that reduce cost while increasing performance.\nAzure OpenAI Service is a cloud-based platform that enables you to access and use OpenAI\u0026rsquo;s powerful LLMs, such as GPT-3 and Codex. It offers a simple and secure way to integrate these models into your applications, with features such as authentication, encryption, scaling, monitoring, etc.\nTutorial # You can find the full example in the following notebook qna-quickstart-with-llama-index.ipynb.\nFirst, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ Next, make sure that you have text-davinci-003 and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service Model Deployments Once we\u0026rsquo;ve installed openai and llama-index via pip, we can run the following code:\nThis will initialize llama-index to use Azure OpenAI Service, by setting a custom LLMPredictor. For this code to work, we\u0026rsquo;ll need to have OPENAI_API_KEY and OPENAI_API_BASE set in our env (in this example we use dotenv). Once we have that, we use the SimpleDirectoryReader to read all text files from the data/ directory. We use GPTSimpleVectorIndex to create our index and lastly query it with a question.\nSummary # In this blog post, we discussed how to use LlamaIndex 🦙 (GPT Index) and Azure OpenAI Service together to quickly index data and perform queries on it. Luckily, we only needed a few lines of configuration over using text-davinci-003 and text-embedding-ada-002 directly from openai.com.\n","date":"9 March 2023","permalink":"/posts/using-gpt-index-llamaindex-with-azure-openai-service/","section":"Posts","summary":"Introduction # In this post we briefly discuss how LlamaIndex 🦙 (GPT Index) can be used with Azure OpenAI Service.","title":"Using LlamaIndex (GPT Index) with Azure OpenAI Service"},{"content":"Introduction video # This vide post shares 5 tips you can use to write better and more effective prompts in Azure OpenAI Service, OpenAI Playground and in ChatGPT:\nThe the sections below for copy/pasting the example prompts.\nExamples # Tip 1 # Classify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. Also add the revenue for 2020. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Tip 2 # JSON:\nClassify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. Also add the revenue for 2020. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Answer in JSON using the keys company, vertical, revenue. Make it an array. YAML:\nClassify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Also add the revenue for 2020. Answer as YML with keys company, vertical, revenue. Do proper newline formatting. CSV:\nClassify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Also add the revenue for 2020. Answer as CSV and print the header \u0026#34;company, vertical, revenue\u0026#34;. Markdown as table:\nClassify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Also add the revenue for 2020. Answer in markdown using a table. Tip 3 # From the following NDA, please extract the following information: - When the contract was signed - Who signed the contract - Address of the person signing the contract - Fine for breaching the contract Answer in JSON. Use the keys signing_date, name, address, and fine_amount. Format the signing_date as MM/DD/YYYY. NDA: On the date of August 17th, 2019, as an employee of Contoso Restaurant, I, Mateo Gomez, residing in 1234 Hollywood Boulevard Los Angeles CA, with social security number: 123-45-6789, hereby declare to fully support and promote the top priorities delegated to me at Contoso Restaurant, and vow to never disclose any information including but not limited to trade secrets, finances, delivery schedules, and recipes. If I, Mateo Gomez, accidentally or with intent breach the conditions set forth in this contract, understand fully that I shall receive a written termination to the following email address mateo@contosorestaurant.com as well as a fine of up to $10,000. JSON: Tip 4 # Content: On the date of August 17th, 2019, as an employee of Contoso Restaurant, I, Mateo Gomez, residing in 1234 Hollywood Boulevard Los Angeles CA, with social security number: 123-45-6789, hereby declare to fully support and promote the top priorities delegated to me at Contoso Restaurant, and vow to never disclose any information including but not limited to trade secrets, finances, delivery schedules, and recipes. If I, Mateo Gomez, accidentally or with intent breach the conditions set forth in this contract, understand fully that I shall receive a written termination to the following email address mateo@contosorestaurant.com as well as a fine of up to $10,000. Given the content above, please answer the following question. If you can\u0026#39;t find the answer in the content, then write \u0026#34;not found\u0026#34; Q: Was the signer of the contract married? A: Tip 5 # Extract the following information from the news article below. 1. One sentence summary in German 2. All German States News article: Microsoft hat eine starke Präsenz im Bereich der Künstlichen Intelligenz (KI). In Deutschland hat das Unternehmen Büros in Bayern, Baden-Württemberg, Berlin, NRW und anderen Bundeslängern. Das Unternehmen hat eine Reihe von Initiativen gestartet, um KI-Technologien zu entwickeln und zu nutzen. Microsoft hat einige der leistungsstärksten KI-Tools auf dem Markt, einschließlich der Azure Machine Learning-Plattform, die es Entwicklern ermöglicht, Machine-Learning-Modelle zu erstellen und zu trainieren. Microsoft hat auch ein KI-Forschungszentrum eröffnet, das sich auf die Entwicklung von KI-Technologien konzentriert. Microsoft hat auch einige Partnerschaften mit anderen Unternehmen geschlossen, um KI-Technologien zu nutzen. Beispielsweise hat Microsoft eine Partnerschaft mit dem chinesischen Unternehmen Baidu geschlossen, um KI-Technologien für die Entwicklung von Autonomen Fahrzeugen zu nutzen. Microsoft hat auch einige KI-Startups gegründet, um neue KI-Technologien zu entwickeln und zu nutzen. Beispielsweise hat Microsoft ein Unternehmen namens Maluuba gegründet, das sich auf die Entwicklung von KI-Technologien zur Verbesserung der Mensch-Maschine-Interaktion konzentriert. Microsoft hat sich zu einem der führenden Unternehmen im Bereich der Künstlichen Intelligenz entwickelt. Mit seinen verschiedenen Initiativen, Partnerschaften und Investitionen in KI-Technologien hat Microsoft gezeigt, dass es entschlossen ist, KI-Technologien voranzutreiben. ","date":"17 February 2023","permalink":"/posts/5-tips-for-writing-better-prompts-in-openai-chatgpt/","section":"Posts","summary":"Introduction video # This vide post shares 5 tips you can use to write better and more effective prompts in Azure OpenAI Service, OpenAI Playground and in ChatGPT:","title":"5 tips for writing better prompts in Azure OpenAI Studio, OpenAI Playground and ChatGPT"},{"content":"Introduction video # This video post is a short introduction to Azure OpenAI Service and its browser-based Studio capability:\n","date":"16 February 2023","permalink":"/posts/getting-started-with-azure-openai-studio/","section":"Posts","summary":"Introduction video # This video post is a short introduction to Azure OpenAI Service and its browser-based Studio capability:","title":"Getting started with Azure OpenAI Studio"},{"content":"Introduction # This post explains how you can authenticate to Azure OpenAI Service by using Azure Active Directory. This enables you to use the service without any API key which has the following advantages:\nEasy on- and offboarding of new users via Azure Access Control (IAM) Avoids key leakage or key re-use in other apps Authentication for an (interactive) AAD user # First, give your user the Cognitive Services OpenAI User role in Access Control (IAM) in the Azure Portal. This role allows to use the Azure OpenAI Studio, but does not allow to deploy models and change anything. Furthermore, this role does not have permission to retrieve the access keys.\nNext, install the Azure Identity client library for Python and the OpenAI SDK:\npip install azure-identity pip install openai Then run the following Python script, but replace endpoint and deployment with your Azure OpenAI Service name/deployment. However, for this code to run you will need to be logged into the Azure CLI and selected the correct tenant/subscription.\nimport os import openai from azure.identity import AzureCliCredential, ChainedTokenCredential, ManagedIdentityCredential, EnvironmentCredential # Define strategy which potential authentication methods should be tried to gain an access token credential = ChainedTokenCredential(ManagedIdentityCredential(), EnvironmentCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/.default\u0026#34;) # Configure OpenAI SDK to use the access token openai.api_base = \u0026#34;https://\u0026lt;replace with your name\u0026gt;.openai.azure.com\u0026#34; openai.api_version = \u0026#39;2022-12-01\u0026#39; openai.api_type = \u0026#39;azure_ad\u0026#39; openai.api_key = access_token.token deployment = \u0026#34;text-davinci-003\u0026#34; # Execute completion prompt = \u0026#34;\u0026#34;\u0026#34;Write a tagline for an ice cream brand:\u0026#34;\u0026#34;\u0026#34; response = openai.Completion.create(engine=deployment, prompt=prompt, max_tokens=100) text = response[\u0026#39;choices\u0026#39;][0][\u0026#39;text\u0026#39;] print(f\u0026#34;Response was: {text}\u0026#34;) If all worked, you should see a similar response:\nResponse was: \u0026#34;Taste the Sweetness of Summer with our Creamy Ice Cream!\u0026#34; Authentication using a Service Principal (app registration) # Alternatively, you can use a Service Principal (system-user) to authenticate at the Azure OpenAI Service. In this case, create a new App Registration in your AAD. Next set the following environment variables on your console:\nAZURE_TENANT_ID=xxxx AZURE_CLIENT_ID=xxxx AZURE_CLIENT_SECRET=xxxx Then, run the same code as above - EnvironmentCredential() will pull the environment variables and use it to authenticate at the service.\nSummary # This post showed how we can use Azure Active Directory to authenticate at the Azure OpenAI Service. This allows to onboard or offboard users to the service for testing and also avoids potential key leakage. This is not only useful during development phase, but also when moving use cases to production, like e.g., the use case we discussed in this post: Using Azure OpenAI Service for processing claim calls.\n","date":"3 February 2023","permalink":"/posts/using-aad-for-azure-openai-service-authentication/","section":"Posts","summary":"Introduction # This post explains how you can authenticate to Azure OpenAI Service by using Azure Active Directory.","title":"Using Azure Active Directory (AAD) to authenticate with Azure OpenAI Service"},{"content":"Introduction # In this post, we\u0026rsquo;ll quickly look into how we can use Azure OpenAI Service to automatically process claim calls. For example, let\u0026rsquo;s assume we have the following call, which we transcribed from the phone conversation using Azure Speech API:\nCaller: Hi, I just had a car accident and wanted to report it. Agent: OK, I hope you\u0026#39;re alright, what happened? Caller: I was driving on the I-18 and I hit another car. Agent: Are you OK? Caller: Yeah, I\u0026#39;m just a little shaken up. Agent: That\u0026#39;s understandable. Can you give me your full name? Caller: Sure, it\u0026#39;s Sarah Standl. Agent: Do you know what caused the accident? Caller: I think I might have hit a pothole. Agent: OK, where did the accident take place? Caller: On the I-18 freeway. Agent: Agent: Was anyone else injured? Caller: I don\u0026#39;t think so. But I\u0026#39;m not sure. Agent: OK, well we\u0026#39;ll need to do an investigation. Can you give me the other drivers information? Caller: Sure, his name is John Radley. Agent: And your insurance number. Caller: OK. Give me a minute. OK, it\u0026#39;s 546452. Agent: OK, what type of damages has the car? Caller: Headlights are broken and the airbags went off. Agent: Are you going to be able to drive it? Caller: I don\u0026#39;t know. I\u0026#39;m going to have to have it towed. Agent: Well, we\u0026#39;ll need to get it inspected. I\u0026#39;ll go ahead and start the claim and we\u0026#39;ll get everything sorted out. Caller: Thank you. From this call, let\u0026rsquo;s try to extract data points like call reason, accident location, involved persons, insurance numbers, damages and a short summary. If we can do this, this would allow to automatically ingest the details into the CRM. Typically, this is often done manually by agents - however, if we can automate it we can save the agent multiple minutes of work per call.\nAzure OpenAI Service Studio Information extraction with Azure OpenAI Service # To extract all the data points, we\u0026rsquo;ll be using OpenAI\u0026rsquo;s most powerful GPT-3 model text-davinci-002 with a temperature of 0.7. We\u0026rsquo;ll give it a generous 1500 tokens (the query typically uses under 1000 tokens). For the prompt, we\u0026rsquo;ll use a zero-shot training approach by very precisely specifying what we want GPT-3 to extract:\nYou must extract the following information from the phone conversation below: 1. Call reason (key: reason) 2. Cause of the incident (key: cause) 3. Names of all drivers as an array (key: driver_names) 4. Insurance number (key: insurance_number) 5. Accident location (key: location) 6. Car damages as an array (key: damages) 7. A short, yet detailed summary (key: summary) Make sure fields 1 to 6 are answered very short, e.g. for location just say the location name. Please answer in JSON machine-readable format, using the keys from above. Pretty print the JSON and make sure that is properly closed at the end. Phone conversation: Hi I just had a car accident and wanted to report it. OK, I hope you\u0026#39;re alright, what happened? I was driving on the I-18 and I hit another car. Are you OK? Yeah, I\u0026#39;m just a little shaken up. That\u0026#39;s understandable. Can you give me your full name? Sure, it\u0026#39;s Sarah standl. Do you know what caused the accident? I think I might have hit a pothole. OK, where did the accident take place? On the I-18 freeway. Was anyone else injured? I don\u0026#39;t think so. But I\u0026#39;m not sure. OK, well we\u0026#39;ll need to do an investigation. Can you give me the other drivers information? Sure, his name is John Radley. And your insurance number. OK. Give me a minute. OK, it\u0026#39;s 546452. OK, what type of damages has the car? Headlights are broken and the airbags went off. Are you going to be able to drive it? I don\u0026#39;t know. I\u0026#39;m going to have to have it towed. Well, we\u0026#39;ll need to get it inspected. I\u0026#39;ll go ahead and start the claim and we\u0026#39;ll get everything sorted out. Thank you. Once we send the example, we will receive the following completion:\n{ \u0026#34;reason\u0026#34;: \u0026#34;To report a car accident\u0026#34;, \u0026#34;cause\u0026#34;: \u0026#34;Hitting a pothole\u0026#34;, \u0026#34;driver_names\u0026#34;: [ \u0026#34;Sarah Standl\u0026#34;, \u0026#34;John Radley\u0026#34; ], \u0026#34;insurance_number\u0026#34;: 546452, \u0026#34;location\u0026#34;: \u0026#34;I-18 freeway\u0026#34;, \u0026#34;damages\u0026#34;: [ \u0026#34;Headlights are broken\u0026#34;, \u0026#34;Airbags went off\u0026#34; ], \u0026#34;summary\u0026#34;: \u0026#34;Sarah Standl was driving on the I-18 when she hit a pothole and caused damage to her car. She exchanged insurance information with the other driver, John Radley.\u0026#34; } This looks pretty good and is easily suitable for some further downstream consumption. But let\u0026rsquo;s discuss a bit, why this actually works.\nPrompt Explanation # Firstly, it is important to tell GPT-3, what we expect it to do:\nYou must extract the following information from the phone conversation below: \u0026lt;more details on what to extract\u0026gt; Phone conversation: \u0026lt;payload\u0026gt; Instructing it to \u0026ldquo;extract information\u0026rdquo; from the \u0026ldquo;phone conversation\u0026rdquo; below helps the model to understand what it is asked to do and where it can actually find the payload on which it should operate. It is important to state where the phone conversation starts by writing \u0026ldquo;Phone conversation:\u0026rdquo;.\nNext, we specify what GPT-3 needs to extract:\n... 1. Call reason (key: reason) 2. Cause of the incident (key: cause) 3. Names of all drivers as an array (key: driver_names) 4. Insurance number (key: insurance_number) 5. Accident location (key: location) 6. Car damages as an array (key: damages) 7. A short, yet detailed summary (key: summary) ... This section is pretty straight forward, but it is important to note that since we want a machine-readable JSON as an output, it helps to specify where we expect an array and what keys we want it to use.\nLastly, we give it even more precise information on what to do:\n... Make sure fields 1 to 6 are answered very short, e.g. for location just say the location name. Please answer in JSON machine-readable format, using the keys from above. Pretty print the JSON and make sure that is properly closed at the end. ... Telling the model to be concise with the answer helps to shorten the responses and avoids that GPT-3 extracts longer chains of words for its answers. For extracting JSON, it helps to not only say that we want JSON, but also to tell it that it should use the \u0026ldquo;keys from above\u0026rdquo; and that the JSON should be properly closed. Otherwise, GPT-3 tends to sometimes forget to close the JSON document.\nNext Steps # While this works well, it might not automatically generalize well across all kinds of call center transcripts, especially for more complex ones. Therefore, two things should be considered before deploying this to production:\nA good validation strategy Using fine-tuning to further enhance extraction accuracy Validation strategy # For validating some of the extracted key-value pairs (e.g., person names, insurance number, etc.), historic data can be used to easily validate the output of the prompt. In this case, a simple string matching approach can be applied to calculate accuracy.\nHowever, for more open-ended key-value pairs (e.g., the summary) it is more challenging to automatically \u0026ldquo;judge\u0026rdquo; how accurate the field is. In this case, it might make sense to calculate the results for maybe a few hundred calls and then have them reviewed by humans. Surely, this is cost and labor-intensive, but allows for a good validation regarding if a human derives value from the summary.\nFine-tuning # One other option is to fine-tune the model. This also requires historic data, so that the prompt and the expected completion form a training dataset. From this, text-davinci-002 is fine-tuned. This helps to further improve the quality of extraction, but also helps to avoid hallucination and other challenges that GPT-3 might introduce. Surely, this is more effort but will ultimately result in a, likely, more robust model.\nSummary # This post showed how we can use Azure OpenAI Service to extract machine-readable information from unstructured call center transcripts. We used a zero-shot learning approach, that can easily extract a variety of data points and even generate a short summary of what happened in the call. This can be used to e.g., automatically populate the CRM after a call, open a ticket, file a claim, etc. For productionizing the prompt, we also discussed how this model can be evaluated and further improved by fine-tuning it.\n","date":"28 November 2022","permalink":"/posts/using-azure-openai-service-for-processing-claim-calls/","section":"Posts","summary":"Introduction # In this post, we\u0026rsquo;ll quickly look into how we can use Azure OpenAI Service to automatically process claim calls.","title":"Using Azure OpenAI Service for processing claim calls"},{"content":"Introduction # In this post we\u0026rsquo;ll show how you can use Azure Active Directory to authenticate against Azure Cognitive Services. This greatly improves security over using key-based authentication methods. This is because AAD authentication allows us to use Cognitive Services without any passwords or secrets, but rather relies on the identity of the user or Managed Identity (e.g., a VM). Furthermore, it helps with tightening resource access: An access key for Cognitive Services can easily be passed on to somebody else, but passing on a identity is typically not (easily) possible.\nFor this post, we\u0026rsquo;ll demonstrate how this can be done for the following three Cognitive Services:\nSpeech API Read API (via Computer Vision API) Language API However, if you want to authenticate to other Azure Cognitive Services or Azure Search have a look at this repo.\nInstructions # Let\u0026rsquo;s directly get started with provisioning some Cognitive Service resources.\nCreate resources # First, let\u0026rsquo;s create a new Computer Vision resource in the Azure Portal:\nCreate a new Computer Vision Cognitive Services resource Once done, we can repeat the same steps for \u0026ldquo;Speech\u0026rdquo; and \u0026ldquo;Language service\u0026rdquo; resources.\nAssign permissions # Once your resources have been provisioned, go into each of them, select \u0026ldquo;Access Control (IAM)\u0026rdquo; and then click \u0026ldquo;Add\u0026rdquo; to perform a \u0026ldquo;Add Role Assignment\u0026rdquo;:\nAdd new Role Assignment under Access Control In the role selection screen, we need to select Cognitive Services User. This role allows us later to access the APIs using Azure Active Directory authentication.\nSelect the \u0026quot;Cognitive Service User\u0026quot; role Under the role assignment screen, assign the role to yourself, or alternatively to a Managed Identity (e.g. the identity of a VM).\nAssign role to your own user or a Managed Identity Click through the review and assign the permission. Optionally repeat the same for your \u0026ldquo;Speech\u0026rdquo; and \u0026ldquo;Language service\u0026rdquo; resources.\nAuthenticate # We\u0026rsquo;re nearly there, we just need to retrieve the Endpoint URL for our Cognitive Service. Since we\u0026rsquo;re not authenticating with a key, the hostname is used to understand against which resource the API should go. You can find the Endpoint URL in your resource\u0026rsquo;s overview screen in the Azure Portal:\nEndpoint URL That\u0026rsquo;s it, let\u0026rsquo;s try to authenticate at the resources using just our user or a Managed Identity. For this code to run, make sure you do a pip install azure-identity first.\nimport os import time import requests from azure.identity import AzureCliCredential, ChainedTokenCredential, ManagedIdentityCredential # Update this with your computer vision endpoint endpoint = \u0026#39;https://\u0026lt;your_computer_vision_hostname\u0026gt;.cognitiveservices.azure.com\u0026#39;) # Define strategy which potential authentication methods should be tried to gain an access token credential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/\u0026#34;) headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {access_token.token}\u0026#34; } json = { \u0026#34;url\u0026#34;: \u0026#34;https://github.com/Azure-Samples/cognitive-services-python-sdk-samples/raw/master/samples/vision/images/make_things_happen.jpg\u0026#34; } params = { \u0026#34;language\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;pages\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;readingOrder\u0026#34;: \u0026#34;natural\u0026#34; } r = requests.post(url=f\u0026#34;{endpoint}/vision/v3.2/read/analyze\u0026#34;, params=params, headers=headers, json=json) print(f\u0026#34;Status code: {r.status_code}\u0026#34;) if (r.status_code == 202): # Too lazy to query status of the job, so just wait - do not use this in your code :) time.sleep(5) r = requests.get(url=r.headers[\u0026#34;Operation-Location\u0026#34;], headers=headers) print(f\u0026#34;Status code: {r.status_code}\u0026#34;) print(f\u0026#34;Response body: {r.json()}\u0026#34;) So what does this code do?\nFirst of all, our code tries to retrieve an oauth token from AAD, using either a Managed Identity (e.g., when running the code on VM) or an Azure CLI-based credential. For the latter to work, you will need to have the Azure CLI installed and also successfully logged in using az login. In this case, we rely on the new Azure Identity library which does all the heavy lifting for us. This allows us to easily request a token against the scope of Cognitive Services:\ncredential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/\u0026#34;) Now, instead of sending a key using key-based access, we can use the token in the header to authenticate:\nheaders = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {access_token.token}\u0026#34; } Using the same approach with Language Service # This approach will work for most Cognitive Services APIs. For example, we can use the same approach to authenticate to the Language service:\nimport os import time import requests from azure.identity import AzureCliCredential, ChainedTokenCredential, ManagedIdentityCredential # Update this with your language service endpoint endpoint = \u0026#39;https://\u0026lt;your_language_hostname\u0026gt;.cognitiveservices.azure.com\u0026#39; # Define strategy which potential authentication methods should be tried to gain an access token credential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/\u0026#34;) headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {access_token.token}\u0026#34; } json = { \u0026#34;analysisInput\u0026#34;: { \u0026#34;documents\u0026#34;: [ { \u0026#34;language\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Translator is a cloud-based neural machine translation service that is part of the Azure Cognitive Services family of REST APIs. Translator can be used with any operating system and powers many Microsoft products and services used by thousands of businesses worldwide to perform language translation and other language-related operations. In this overview, you\u0026#39;ll learn how Translator can enable you to build intelligent, multi-language solutions for your applications across all supported languages.\u0026#34; }, { \u0026#34;language\u0026#34;: \u0026#34;de\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Der Übersetzer ist ein cloudbasierter Dienst für neuronale maschinelle Übersetzung, der zur Azure Cognitive Services-Familie der REST-APIs gehört. Der Übersetzer kann mit jedem Betriebssystem verwendet werden und unterstützt viele Microsoft-Produkte und -Dienste, die von Tausenden von Unternehmen weltweit verwendet werden, um Sprachübersetzungen und andere sprachbezogene Vorgänge durchzuführen. In dieser Übersicht erfahren Sie, wie Sie mit dem Übersetzer intelligente, mehrsprachige Lösungen für Ihre Anwendungen in allen unterstützten Sprachen erstellen können.\u0026#34; } ] }, \u0026#34;tasks\u0026#34;: { \u0026#34;extractiveSummarizationTasks\u0026#34;: [ { \u0026#34;parameters\u0026#34;: { \u0026#34;model-version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;sentenceCount\u0026#34;: 2, \u0026#34;sortBy\u0026#34;: \u0026#34;Rank\u0026#34; } } ] } } r = requests.post(url=f\u0026#34;{endpoint}/text/analytics/v3.2-preview.1/analyze\u0026#34;, headers=headers, json=json) print(f\u0026#34;Status code: {r.status_code}\u0026#34;) if (r.status_code == 202): # Too lazy to query status of the job, so just wait - do not use this in your code :) time.sleep(5) r = requests.get(url=r.headers[\u0026#34;Operation-Location\u0026#34;], headers=headers) print(f\u0026#34;Status code: {r.status_code}\u0026#34;) print(f\u0026#34;Response body: {r.json()}\u0026#34;) If you want to try it out with other services, have a look at this repo.\nAuthenticating to Speech API # For accessing most (potentially all) Cognitive Services directly via their REST API, this approach will work fine. However, Speech API for example offers a Python SDK that relies on their WebSocket streaming interface, so we can\u0026rsquo;t do a plain REST-based request. In this case, the authentication is slightly different.\nFor this, first retrieve the region and the resource id of your Speech resource:\nRetrieve the resource id and region via the \u0026quot;Properties\u0026quot; tab Once retrieved, we can update the following code and use it to authenticate to the Speech API using AAD. Make sure to run pip install azure-cognitiveservices-speech first.\nimport os from azure.identity import AzureCliCredential, ChainedTokenCredential, ManagedIdentityCredential import azure.cognitiveservices.speech as speechsdk # Update with your Speech API resource id resource_id = \u0026#39;/subscriptions/\u0026lt;subscription_id\u0026gt;/resourceGroups/\u0026lt;resource_group\u0026gt;/providers/Microsoft.CognitiveServices/accounts/\u0026lt;speech_resource_name\u0026gt;\u0026#39; region = \u0026#34;westeurope\u0026#34; # update to your region credential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/\u0026#34;) authorization_token = \u0026#34;aad#\u0026#34; + resource_id + \u0026#34;#\u0026#34; + access_token.token def from_mic(): speech_config = speechsdk.SpeechConfig(auth_token=authorization_token, region=region) speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config) print(\u0026#34;Say something...\u0026#34;) result = speech_recognizer.recognize_once_async().get() if result.reason == speechsdk.ResultReason.RecognizedSpeech: print(\u0026#34;Recognized: {}\u0026#34;.format(result.text)) elif result.reason == speechsdk.ResultReason.NoMatch: print(\u0026#34;No speech could be recognized: {}\u0026#34;.format(result.no_match_details)) elif result.reason == speechsdk.ResultReason.Canceled: cancellation_details = result.cancellation_details print(\u0026#34;Speech Recognition canceled: {}\u0026#34;.format(cancellation_details.reason)) if cancellation_details.reason == speechsdk.CancellationReason.Error: print(\u0026#34;Error details: {}\u0026#34;.format(cancellation_details.error_details)) from_mic() In this case, the Speech Python SDK does not seem to allow using the custom hostname, hence relies on the resource id to retrieve the correct Cognitive Service to authenticate against.\nSummary # This post showed how we can use Azure Active Directory-based authentication with Azure Cognitive Services. For this we relied on the new Azure Identity library and then used REST-based API calls to access the services. If you want to authenticate to other Azure Cognitive Services or Azure Search have a look at this repo.\nWith this approach, we can keep our code clear of any credentials and do not need to use any credentials during development either. This greatly increases security and removes unnecessary attack vectors (e.g., by using Managed Identities in production).\nIf you want to learn how to use Cognitive Services to read data that is protected by VNETs, have a look at the next post: Using Cognitive Services Read API with data secured by VNET.\n","date":"22 February 2022","permalink":"/posts/azure-active-directory-aad-authentication-for-azure-cognitive-services/","section":"Posts","summary":"Introduction # In this post we\u0026rsquo;ll show how you can use Azure Active Directory to authenticate against Azure Cognitive Services.","title":"Azure Active Directory (AAD) authentication for Azure Cognitive Services"},{"content":"Introduction # This post discusses how Cognitive Services can be used to process data that is securely stored behind a VNET. This allows to improve security further when processing sensitive data that is stored in Storage Accounts. In this post, we\u0026rsquo;ll look into using Read API (from the Azure Computer Vision API) to analyze documents that are heavily protected using networking rules.\nThis post also applies to other Cognitive Services, such as for example Form Recognizer or Speech API.\nData behind VNET – what does it mean? # In Azure, many users protect their data using a set of security perimeters. While these are typically not all measurements users take to store their data securely, those are the most common ones that most people use:\nAuthentication layer – requirement to authenticate at the storage layer, e.g., via a access key or based on an identity (could be a real user or a system managed identity) Authorization layer – what is the identity allowed to do with the data (just read certain folders, write data, delete, etc.) Networking layer – from where is the identity allowed to access the data (from the internet, from a range of IP addresses, only from within a VNET, from \u0026ldquo;nowhere\u0026rdquo;) For example, your Networking settings on your Storage Account might look like this: Typical Networking settings on a Storage Account In this case, the data can only be access, when the access request originates from within subnet default inside the VNET vnet-test.\nUsing these security measurements makes it incredibly hard/potentially impossible to have an attacker access the data. But this also creates a problem: what if an Azure service, like for example a Cognitive Service needs to access this data? The service might be authenticated (e.g., using a SAS URL), but from a networking perspective, the service is obviously not coming from within your VNET:\nRead API can\u0026rsquo;t access the storage In the drawing above the Read API is:\nAuthenticated to access the Storage Account and read the data (using the SAS URL from the request) Blocked by the networking rules of the Storage account If we would remove the networking rule on the storage account, the data access would obviously be allowed:\nRead API can access the storage However, this is obviously not the desired setup as the Storage Account might hold sensitive data.\nDesired state # Obviously, we want to make sure that we do everything possible to protect our data as much as we can. Furthermore, it would also be even more secure if we did not have to use a SAS URL at all. Sure, this URL might be short lived, but why not get rid of it if we can? But most importantly, we want to make sure we can use our Cognitive Services, despite the data being sitting behind a VNET.\nThe solution – Managed Identity and Resource Service Instances to the rescue! # The solution to make this scenario work requires two components:\nManaged Identities Resource Service Instances Managed Identities # Most Azure services can \u0026ldquo;assume\u0026rdquo; a Managed Identity. A Managed Identity is a system user that is tied to that specific Azure resource. Since it is a \u0026ldquo;user\u0026rdquo;, the identity lives in your Azure Active Directory. That means we can assign that identity other privileges, such as access to storage or other Azure services. This means we can firstly assign our Cognitive Service a Managed Identity. Secondly, we can allow that identity to be able to read/write to our storage account using IAM. As a result, the Cognitive Service can access data without the need for access keys. Instead, it automatically requests an OAuth token from AAD and uses it to authenticate to the storage.\nResource Service Instances # However, the networking layer will still block the request and this is where Resource Service Instances come into play. With this feature we can specify a list of Azure services that are allowed to connect to the storage account regardless of its networking settings. This means, we can explicitly permit our Cognitive Service resource to \u0026ldquo;tunnel through\u0026rdquo; the firewall. Is this a security issue? No, because it is only allowed for the identity of the Cognitive Service itself. And as this identity is a system user, it is automatically secured.\nOverall Architecture # Putting both together, we get to this:\nRead API can access the storage account using its Managed Identity In this case:\nRead API uses its Managed Identity to authenticate at the Storage Account (no SAS URL required!) The Resource Service Instance configuration on the Storage Account allows the Managed Identity to \u0026ldquo;get through\u0026rdquo; the VNET-firewall Step-by-step guide # To try this out, first create a new Computer Vision API (this includes the Read API):\nCreate a new Computer Vision API During the creation, make sure to enable Managed Identity. You can also always later enable it under the Identity tab:\nManaged Identity on Computer Vision API Next, click Azure Role Assignments on the same screen and select Add role assignment. Then, assign the Storage Blob Data Reader role to your Storage Account. Once done, you could send plain storage URLS without SAS tokens to Read API and it could read the data.\nNext, navigate to your Storage Account, select Networking and check the network settings. In our example here, we only allow access from selected networks. Ironically, we did not select any VNET, so the data can\u0026rsquo;t be accessed from anywhere, including Cognitive Services. However, we\u0026rsquo;ll add the Cognitive Services Resource type and then name of our Cognitive Service instance. This means our Cognitive Service can tunnel through this super-restrictive networking setting!\nAllowing our Cognitive Service resource to tunnel through the firewall Don\u0026rsquo;t forget to hit the save button.\nTesting the whole setup # Once done, we can fire a few REST API calls to send a document to the Read API:\n# @name read_document POST https://computer-vision-demo124.cognitiveservices.azure.com/vision/v3.2/read/analyze ?language=en \u0026amp;pages=1 \u0026amp;readingOrder=natural Content-Type: application/json Ocp-Apim-Subscription-Key: \u0026lt;secret key\u0026gt; { \u0026#34;url\u0026#34;: \u0026#34;https://dgjt35hksss.blob.core.windows.net/data/description.png\u0026#34; } ##### This request queries the status of the Read API operation # @name get_results GET {{read_document.response.headers.Operation-Location}} Content-Type: application/json Ocp-Apim-Subscription-Key: \u0026lt;secret key\u0026gt; The result looks successful:\n{ \u0026#34;status\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;createdDateTime\u0026#34;: \u0026#34;2022-02-22T12:52:59Z\u0026#34;, \u0026#34;lastUpdatedDateTime\u0026#34;: \u0026#34;2022-02-22T12:53:00Z\u0026#34;, \u0026#34;analyzeResult\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;3.2.0\u0026#34;, \u0026#34;modelVersion\u0026#34;: \u0026#34;2021-04-12\u0026#34;, \u0026#34;readResults\u0026#34;: [ { \u0026#34;page\u0026#34;: 1, \u0026#34;angle\u0026#34;: -0.7095, ... Despite us just sending a regular URL (https://dgjt35hksss.blob.core.windows.net/data/description.png), the Read API can access the data:\nAuthenticated through its Managed Identity and allowed through the firewall by the Resource Instance configuration If we remove the Resource instance definition, we would get the following message, as the URL would return a 403 error to the Cognitive Service:\n{ \u0026#34;error\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;InvalidImageURL\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Failed to download the image from the submitted URL. The URL may either be invalid or the server hosting the image is experiencing some technical difficulties.\u0026#34; } } Great, looks like it is working fine now!\nBy the way, we can entirely avoid the secret to call the Cognitive Service itself. If you are interested, have a look at this blog post: Azure Active Directory (AAD) authentication for Azure Cognitive Services.\nSummary # Many users want to protect their sensitive data using as many measurements there are available on the Azure platform:\nFirstly, this will be using authentication and authorization for accessing data on storage Secondly, this will include networking rules to limit from where data can be accessed This creates a unique challenge for accessing this VNET-protected data using Cognitive Services. However, combing the usage of Managed Identities and Resource Service Instances solve the problem. They enable users to keep their data well protected, but still allows them to process it by Cognitive Services.\n","date":"22 February 2022","permalink":"/posts/using-cognitive-services-read-api-with-data-secured-by-vnet/","section":"Posts","summary":"Introduction # This post discusses how Cognitive Services can be used to process data that is securely stored behind a VNET.","title":"Using Cognitive Services Read API with data secured by VNET"},{"content":"Introduction # The Azure Read API allows you to perform OCR on scanned documents of all kinds. This is great for digitalizing contracts, books, or research documents. With support for features like natural reading order support, it can be used for a wide range of tasks.\nHowever, how we can use Azure Read API for processing many documents at scale? Let\u0026rsquo;s say we need to run 100000 documents through Read API. What is the quickest way that we can do it? What image resolution should we use? What are the tradeoffs we need to consider?\nIn this post, we\u0026rsquo;ll look at all these questions and discuss how things like image resolution and parallel processing affect the overall processing time for large OCR jobs.\nAzure Read API example document Image resolution # Let\u0026rsquo;s first evaluate if the image resolution makes a difference in terms of processing time and accuracy. For comparing recognition results, we will use the Levenshtein distance to measure the distance between the ground truth of the document and the Read API results.\nImage resolution Image size Processing time Levenshtein distance 500×667 100KB 1.2s 11 1000×1333 300KB 1.5s 6 2000×2667 1.3MB 1.7s 3 3000×4000 3.1MB 1.9s 0 A few things come to mind when looking at these numbers:\nLarger image resolution gives better results. We can see that Levenshtein distance drops with larger image size, and that is typically the main thing we\u0026rsquo;ll care about for OCR: the most accuracy recognition results. Azure Read API does not charge extra for larger images, so why not leverage this? Larger image resolution only minorly affects processing time. Despite increasing the total pixel count by 35x, processing time increased only by around 1.5x. We do not know for sure, but the Read API most likely performs some image auto-scaling before processing the document. As a first learning, we should rather use high-resolution images as Read API will produce better OCR results, does not take significantly longer, and most importantly, does not cost more! Next, let\u0026rsquo;s look at optimizing the overall processing time.\nOptimizing processing time # When calling the Read API, we first have to call the analyze API and then subsequently the analyzeResults API to fetch the results. Obviously, processing will happen in between these two calls. But is there anything else? While we do not know for certain, most likely the following two steps will happen in the backend:\nUpon calling analyze, our request is first put into a queue (service-side) An idle worker will fetch our request from the queue an process it Upon calling analyzeResults, the cached results will be returned to us Again, this is speculation and might not happen exactly like this in reality, but one way or the other, it is very likely that there will be some form of queueing happening. Once unqueued, one of the many workers will process it. Having that said, what is the best strategy to use when having to run OCR on many documents?\nTo find it out, let\u0026rsquo;s compare three strategies:\nOption 1 – Sequential: Process each document from start to finish in a gigantic for-loop Option 2 – Optimized: First call analyze for a mini-batch of documents in a for-loop (e.g., 100), then start calling analyzeResults for the mini-batch in a for-loop Option 3 – Multithreading: Run multiple threads, each thread processes one document from start to finish Intuitively, we\u0026rsquo;d suspect that option 3 would be the fastest, followed by option 2, and option 1 should be the slowest. Let\u0026rsquo;s see if this is actually true!\nFor testing this, we run the test code from a VM in Azure in the same region as the Read API endpoint. Test documents are stored on Azure Blob in the same region. As Read API is limited to 10 TPS (transactions per second) per default, we add sleep() statement the code to obey that limit. However, this limit can be increased through a support request or alternatively, we could just provision multiple Read API resources. For the multi-threaded test, we run 10 threads in parallel, each sequentially processing 10 documents using its own Cognitive Services resource (=total of 100 TPS).\nLet\u0026rsquo;s look at the results for processing 100 documents in various sizes with the three different approaches:\nProcessing times for 100 documents compared These results look not fully what we expected, so let\u0026rsquo;s discuss them:\nSame as in our prior tests, image size does not affect processing time in a significant way – everything is within the same ballpark. Sequential execution is the slowest. This was expected, as waiting for each document to finish before moving on does not take advantage of the parallel backend of the Read API. Furthermore, we might get a \u0026ldquo;disadvantageous\u0026rdquo; position in the queue for each new document. Option 2 seems to be the quickest. This seems reasonable, as by starting all analyze calls in one batch, we hopefully will get approximately very similar positions in the queue. Once we start querying for the results, most of them will be finished as they have been processed in parallel by the backend. This results in the overall lowest processing time. Using a multi-threaded approach (option 3) did not perform as fast as expected. This is most likely because each thread will process documents one by one. This will result in \u0026ldquo;disadvantageous\u0026rdquo; positions in the queue, which makes it inferior to option 2. There is some variance in execution time. For option 3, we can see that one of the documents properly got a very deep position in the queue, therefore increasing overall processing time significantly. Let\u0026rsquo;s look at the average processing time per document:\nAverage processing time per image Again, these results look a bit different to what we\u0026rsquo;ve probably expected:\nThere is a large inconstancy in the average processing time. For option 1 and option 3, we should see comparable numbers, but it looks like when the tests for option 3 were executed, the queues was \u0026ldquo;more busy\u0026rdquo; than during the first test. Average processing time for option 2 is obviously much slower. This seems weird, as we expect a FIFO approach by Read API, but it could have been that a document or analyzeResult request \u0026ldquo;got stuck\u0026rdquo;, hence blocking the for-loop. When it comes to learnings, we can see that using an optimized sequential approach where we decouple the analyze and analyzeResult calls provides the overall quickest processing times.\nSummary # When using Azure Read API for processing many documents at scale (e.g., 1000\u0026rsquo;s, 10k\u0026rsquo;s or even 100k\u0026rsquo;s of documents), it makes sense to figure out a good strategy for processing them in parallel. In this post, we figured out a few highlights, so let\u0026rsquo;s summarize them:\nUse the highest available image resolution available. This will provide better recognition results, won\u0026rsquo;t cost more, and only takes slightly longer. Do not process files sequentially, as this does not take advantage of the parallel backend of the Read API. For quickest processing time, start calling the analyze API with a large mini-batch (e.g., 100 documents), then query the results using analyzeResults for the mini-batch in a for-loop. This is a simple strategy, allows for easy retry-mechanism (if needed), and you avoid the hassle of dealing with multi-threading. Leverage multiple Cognitive Services resources to get around the 10 TPS limit. Alternatively, open a support request and ask for an increased throughput limit. With this information, you should be easily able to analyze vast amounts of documents in no time! Lastly, as Form Recognizer uses Read API under the hood, we can probably use the same strategy when recognizing forms!\n","date":"17 November 2021","permalink":"/posts/azure-read-api-for-processing-many-documents-at-scale/","section":"Posts","summary":"Introduction # The Azure Read API allows you to perform OCR on scanned documents of all kinds.","title":"Using Azure Read API for processing many documents at scale"},{"content":"Introduction # This post explains how we can search and analyze call center transcripts using Azure Cognitive Search and PowerBI. This can be helpful to analyze what is going on in your call center and allows to easily answer questions like:\nWhy are people calling? What are typical topics? What is the sentiment? How does sentiment and how do topics change over time? Having these answers can help optimize your call center strategy, but also potentially allows to offload easier tasks to voice-enabled bots.\nBefore we get started, all json documents and data samples can be found in this repo: csiebler/cognitive-search-call-transcripts\nCognitive Search Setup # Data pre-processing # Let\u0026rsquo;s first look at a short data sample that we\u0026rsquo;ll use for this tutorial:\nData sample from our transcripts As you can see, our input data follows a quite simple schema. However, in reality you might want to use transcripts from the Azure Speech-to-Text Batch Transcription API. However, since these transcripts are very detailed and verbose, I\u0026rsquo;d recommend to reduce them down a bit first. For example, you can run this basic scripts against a few of your sample documents: cognitive-search-call-transcripts/preprocess.py. Just put them in a folder, update the path and let the script do its work.\nOne you have the transcripts converted, upload them to a new container in an Azure Storage Account.\nIn case you do not have any documents ready to use, feel free to use my wonderful, hand-written transcripts from this GitHub repo: cognitive-search-call-transcripts/data/2021/11\nRequired Azure Resources # This tutorial assumes that you have already created a Cognitive Search instance, as well as a Storage Account and a Cognitive Services resource.\nRequired services for this tutorial Furthermore, be sure to have the following information at hand:\nThe Connection String, Name, and Access Key of your Storage Account The account key and resource id of your Cognitive Services Resource In general, I\u0026rsquo;d recommend to use Managed Identities for data access, but for the sake of this tutorial it might be easier to just rely on keys. For production, always use Managed Identities if possible!\nFurthermore, for this tutorial I suggest to use the VSCode Cognitive Search extension. This allows you to super easily create indexes, indexers, data sources, skillsets and synonym maps. Furthermore, it allows you to reset and run an indexer with a few clicks, so it makes development much faster!\nCognitive Search VSCode Extension Data Source creation # First, let\u0026rsquo;s upload your transcripts to a new container on the Storage Account. Once done, you can create a new data source in Cognitive Search. For this, update the credentials and container name in the following json document and use the VSCode Cognitive Search extension to create the data source.\nUpdate your data source settings You can find the full json here.\nIndex creation # Next, we create our index. There is not really the need to adapt anything here, unless you want to add more fields:\nIndex creation You can find the full json here.\nSkillset creation # Next, we create our skillset. You will need to add your Cognitive Services resource id and also the key. The resource id is in the following format:\n/subscriptions/\u0026lt;subscription id\u0026gt;/resourceGroups/\u0026lt;resource group\u0026gt;/providers/Microsoft.CognitiveServices/accounts/\u0026lt;cognitive service name\u0026gt; You can find it in the Azure Portal or via the CLI:\nResource ID in Azure Portal Lastly, you will also need to add your Storage Connection String, so Cognitive Search can store its projections in an Azure Table. We\u0026rsquo;ll use this table to later generate real-time dashboards using PowerBI.\nUpdate your skillset definition You can find the full json here.\nIndexer creation # Lastly, we create our indexer. Unless you added more fields to the index, you won\u0026quot;t need to touch this file.\nIndexer creation You can find the full json here.\nFinally, we can run the indexer and check if our index contains documents:\nDocuments have been indexed Perfect, that looks good! Finally, we can run queries against our search index.\nVisualization with PowerBI # Since your skillset creates a Table Projection of the data, we can point PowerBI to read from the Azure Table where the data is stored. From there, we can build a KPI dashboard for our transcripts.\nFollow this tutorial to connect PowerBI to your Azure Table with the projections. Once done, we can start building dashboards. In this case, the dashboards look a bit \u0026ldquo;lonely\u0026rdquo;, but once we\u0026rsquo;d have real and more data in Cognitive Search, we\u0026rsquo;d be able to derive a lot of insight from it!\nPowerBI dashboard example For example, we can easily build queries like:\nWhich key phrases have positive/negative sentiment associated with them? Which key phrases are most common? Are there trends in terms of sentiment or key phrases over time? \u0026hellip;and a lot more! This new insight will hopefully help to identify strong areas, as well as areas for improvement.\nSummary # In this post we showed how Azure Cognitive Search can be used to index call center transcripts (generated from Azure Speech-to-Text). We can use the solution to search for calls (e.g., educational purposes), but then also use this data to build rich KPI dashboards using PowerBI.\n","date":"16 November 2021","permalink":"/posts/analyzing-call-center-transcripts-azure-cognitive-search-powerbi/","section":"Posts","summary":"Introduction # This post explains how we can search and analyze call center transcripts using Azure Cognitive Search and PowerBI.","title":"Analyzing Call Center Transcripts with Azure Cognitive Search and PowerBI"},{"content":" Introduction # Azure offers a rich set of pre-trained AI models called Cognitive Services which can help you solving a large variety of tasks. For example, services like OCR (Optical Character Recognition), form recognition or Speech-to-Text enable you to automate otherwise labor-intensive business processes.\nLet\u0026rsquo;s take invoice processing as an example. Historically, this has been performed manually and the turnaround time was probably a few hours or even a few days. It was highly asynchronous and it was \u0026ldquo;clear\u0026rdquo; that you had to wait. As we start automating this use case using e.g., Azure Form Recognizer, we can make this process not only significantly faster, but we can make it real-time (synchronous). But what does real-time really mean? 1 second? 5 seconds? 1 minute?\nIf the user is e.g., uploading a document in an app, how long can we have the user wait for the processing? I personally believe it should be either really fast (less than a few seconds), or it should be asynchronous. So if we want to make it fast, how fast can we make it? Cloud hosted APIs might show larger variance in terms of processing times, so does it make sense to self-host if supported? This could potentially give faster and more predictable response times. Let\u0026rsquo;s find out if this is really the case!\nService vs. Self-hosted # For figuring out if it really makes sense to self-host one of the Cognitive Services, we will take the Read, Form Recognizer Invoice and Speech-to-Text APIs as examples. These API are often used for automating a large variety of business processes, such as OCR, invoice processing or call center transcription. All APIs can be consumed as a service in Azure (hosted by Microsoft) or self-hosted as a Docker container (user is responsible for hosting it). So, which one is faster?\nTest Results # All the APIs we have tested are asynchronous, as they might take a few seconds to reply. Therefore, we tested with the following methodology:\nCall API to perform the task (POST) Query until task was finished successfully (GET), with a wait of 10ms between each check Wait for 1 second (probably not needed, but let\u0026rsquo;s avoid running into the rate limit at any cost) Repeat 100 times Read API # For this test, we used a 1600×1200 pixel PNG image (100KB) and we ran it through the Read API for 100 times. We ran these tests from a F16sv2 instance inside Azure, which hosted the Read API container with the recommended resource configuration (8 cores, 24GB of memory). Here are the results, compared to the Azure hosted version:\nHosting Type Azure hosted Container hosted Average processing time 1.257s 1.029s Variance of processing time 0.128 0.003 Minimum processing time 0.852s 0.940s Maximum processing time 3.409s 1.187s Overall, we can see that the self-hosted version is ~20% faster and the variance is ~40x lower. We\u0026rsquo;ve ran this test once in the morning and once in the afternoon (Azure region was West Europe) and observed similar results for both tests.\nIn conclusion, for the Read API we can observe that:\nThe container hosted version is slightly faster and has lower variance, i.e. it is more deterministic in terms of processing time per document (good) At the same time, the container hosted version does not need to handle the vast request amount of requests the hosted version processes every second Hence, it does not have to deal with spikes (at least not in our test) – this obviously explains the lower variance Overall, the average processing time per document is fairly similar, the min/max time are also in a similar ballpark, so there is no big advantage of using the one over the other. However, if your data can\u0026rsquo;t go to the cloud, running the container-hosted version is definitely a big plus point!\nForm Recognizer Invoice API # Next, let\u0026rsquo;s see how the Form Recognizer Invoice API compares for the hosted vs container version. For this, we used a 750×1000 pixel large invoice document (100KB). We ran the required containers on a F32sv2 instance with the recommended settings of:\nLayout container – 8 cores and 24GB of memory (required by the invoice container) Invoice container – 8 cores and 8GB of memory Here are the results after applying the same testing methodology as for the Read API:\nHosting Type Azure hosted Container hosted Average processing time 2.721s 5.527s Variance of processing time 0.457 0.239 Minimum processing time 1.731s 5.084s Maximum processing time 5.318s 6.925s Most notably, the container-hosted version is around 2x slower than the Azure hosted. Wow! This is surprising, given the high resource requirements. Again, processing time variance is lower and minimum and maximum processing time are closer together. This aligns with the results we saw for the Read API. So in summary, unless you want to process data that is not allowed to travel to Azure, relying on the hosted version is just fine.\nSpeech-to-Text Batch Transcription API # For Speech-to-Text Batch Transcription, we\u0026rsquo;ve tested with a single Speech-to-Text container running with various settings. To enable batch transcription, we ran it alongside with the batch transcription container in daemon mode on a F16sv2. We compared this with the average processing time of the hosted batch transcription service in Azure.\nHosting Type Azure hosted Container hosted (4 cores, 4GB of memory) Container hosted (8 cores, 8GB of memory) Processing duration 2x real-time 2x real-time ~3.2x real-time Since the performance looked pretty comparable during my testing, I choose not to publish detailed results. However, especially for transcription of long audio files (e.g., call center recordings), it is important that we can either:\nscale out a self-hosted deployment for transcribing many files in parallel or increase container resources to get a x-fold real-time speed for transcription (or a combination of both) In this case, doubling the resources sped up the transcription by +50%. In this case, it would be more economical to scale out to multiple containers. Again, unless you need to rely on a self-hosted container for data privacy reasons (e.g., transcribing audio containing PII data), there is little value of using the self-hosted version.\nSummary # Overall, the primary reason why Cognitive Services exist in form of containers is to enable scenarios where data privacy is important. Those containers allow to process sensitive data on-premises or on approved cloud providers.\nHowever, our findings show that from a performance perspective the Azure hosted version performs superior and offers much easier access to scale. While the variance in processing time varies a bit, the worst and best-case processing times where all within the same ballpark. Except for extremely time sensitive applications, this probably does not justify the additional effort/cost that is required to host the containers.\n","date":"21 September 2021","permalink":"/posts/azure-cognitive-services-processing-time-comparison/","section":"Posts","summary":"Introduction # Azure offers a rich set of pre-trained AI models called Cognitive Services which can help you solving a large variety of tasks.","title":"Azure Cognitive Services Containers processing time comparison"},{"content":"Introduction # This is a quick post for showing how to call Azure Machine Learning Pipelines from Azure Data Factory. This includes passing data dynamically into the Machine Learning Pipeline using DataPath.\nPipeline Creation # First, let\u0026rsquo;s create an AzureML Pipeline that we can use for this example. Please note that this code is syntactically correct, but probably won\u0026rsquo;t run unless you adapt a few parameters, e.g., change the environment, adapt the data path, add a training script, etc.\nimport os import azureml.core from azureml.core import Workspace, Experiment, Dataset, RunConfiguration, Environment from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.data.dataset_consumption_config import DatasetConsumptionConfig from azureml.data.datapath import DataPath, DataPathComputeBinding from azureml.pipeline.core import PublishedPipeline, PipelineEndpoint # Connect to workspace ws = Workspace.from_config() # Get default datastore default_datastore = ws.get_default_datastore() # Define default DataPath for training data input and make it configurable via PipelineParameter data_path = DataPath(datastore=default_datastore, path_on_datastore=\u0026#39;training_data/\u0026#39;) datapath_parameter = PipelineParameter(name=\u0026#34;training_data_path\u0026#34;, default_value=data_path) datapath_input = (datapath_parameter, DataPathComputeBinding(mode=\u0026#39;download\u0026#39;)) # Configure runtime environment for our pipeline using AzureML Environment runconfig = RunConfiguration() runconfig.environment = Environment.get(workspace=ws, name=\u0026#39;training-env\u0026#39;) train_step = PythonScriptStep(name=\u0026#34;train-step\u0026#34;, source_directory=\u0026#34;./\u0026#34;, script_name=\u0026#39;train.py\u0026#39;, arguments=[\u0026#39;--data-path\u0026#39;, datapath_input], inputs=[datapath_input], runconfig=runconfig, compute_target=\u0026#39;cpu-cluster\u0026#39;, allow_reuse=False) steps = [train_step] # Create pipeline pipeline = Pipeline(workspace=ws, steps=steps) pipeline.validate() # Publish pipeline to AzureML published_pipeline = pipeline.publish(\u0026#39;prepare-training-pipeline-datapath\u0026#39;) # Publish pipeline to PipelineEndpoint (optional, but recommended when using the pipeline with Azure Data Factory) endpoint_name = \u0026#39;training-pipeline-endpoint\u0026#39; try: print(f\u0026#39;Pipeline Endpoint with name {endpoint_name} already exists, will add pipeline to it\u0026#39;) pipeline_endpoint = PipelineEndpoint.get(workspace=ws, name=endpoint_name) pipeline_endpoint.add_default(published_pipeline) except Exception: print(f\u0026#39;Will create Pipeline Endpoint with name {endpoint_name}\u0026#39;) pipeline_endpoint = PipelineEndpoint.publish(workspace=ws, name=endpoint_name, pipeline=published_pipeline, description=\u0026#34;New Training Pipeline Endpoint\u0026#34;) Most notably, we publish the pipeline as a PublishedPipeline and then add it to a PipelineEndpoint. A PipelineEndpoint acts as a \u0026ldquo;router\u0026rdquo; for multiple PublishedPipelines, and presents a static URL to its callers. As we re-run this code, it\u0026rsquo;ll just add our new pipeline behind the current endpoint and sets it as the new default.\nFurthermore, we are using DataPath and PipelineParameter to make the data input dynamic. DataPath allows us to specify an arbitrary path on a datastore as an input, and PipelineParameter allows to dynamically pass in the DataPath when invoking the pipeline.\nIn the next step, we\u0026rsquo;ll call the PipelineEndpoint from Azure Data Factory.\nSetup in Data Factory # In Data Factory, first create a Linked Service to your Azure Machine Learning Workspace. Then create a new Pipeline and add the Machine Learning Execute Pipeline activity.\nCreating a new ADF Pipeline Next, we can configure the Machine Learning component:\nConfiguring our ADF Pipeline From the workspace, we can first select the pipeline we would like to execute. For this, we select our newly created PiplineEndpoint as it allows swapping out the active AzureML Pipeline in the backend – without touching Azure Data Factory. Under Experiment name, we pass in the name under which the pipeline should be executed in AzureML. Lastly, we need to pass in the DataPath via a Data path assignment. For this, we need to put the name of the pipeline parameter(s) for the DataPath in the big text box, then click the small down arrow left to it and add:\nDataStoreName: point to your AzureML Datastore name RelativePath: point to your path inside the Datastore In this example, training_data_path was defined in our code in line 18 (datapath_parameter = PipelineParameter(name=\u0026quot;training_data_path\u0026quot;, default_value=data_path)).\nFinally, we can publish the ADF pipeline, and run it using Add trigger, then select Trigger now. Once it ran, we should see the results in our experiment in Azure Machine Learning Studio:\nSuccessful Azure Machine Learning Pipeline run Looks good! We can see that the experiment was named properly and that the data was correctly pulled from what we set in Azure Data Factory.\nHope this quick tip was helpful!\n","date":"23 July 2021","permalink":"/posts/azure-machine-learning-pipelines-azure-data-factory-datapath/","section":"Posts","summary":"Introduction # This is a quick post for showing how to call Azure Machine Learning Pipelines from Azure Data Factory.","title":"Invoking Azure Machine Learning Pipelines from Azure Data Factory using DataPath"},{"content":"Introduction # This post is a quick tip, showing how you can automatically enforce an Init Script during Azure Machine Learning Compute Instance provisioning. This ensures that even when the user did not specify a script, a default script is always being executed.\nFor more details on the init script capabilities, have a look at the documentation.\nInstructions # Firstly, create your desired init script – in our example, we\u0026rsquo;ll just use a simple example:\n#!/bin/bash echo \u0026#34;Hello World\u0026#34; Then base64-encode the script and replace the value in line 26 with your encoded script:\n{ \u0026#34;mode\u0026#34;: \u0026#34;All\u0026#34;, \u0026#34;policyRule\u0026#34;: { \u0026#34;if\u0026#34;: { \u0026#34;allOf\u0026#34;: [ { \u0026#34;field\u0026#34;: \u0026#34;type\u0026#34;, \u0026#34;equals\u0026#34;: \u0026#34;Microsoft.MachineLearningServices/workspaces/computes\u0026#34; }, { \u0026#34;field\u0026#34;: \u0026#34;Microsoft.MachineLearningServices/workspaces/computes/computeType\u0026#34;, \u0026#34;in\u0026#34;: [ \u0026#34;ComputeInstance\u0026#34; ] } ] }, \u0026#34;then\u0026#34;: { \u0026#34;effect\u0026#34;: \u0026#34;append\u0026#34;, \u0026#34;details\u0026#34;: [{ \u0026#34;field\u0026#34;: \u0026#34;Microsoft.MachineLearningServices/workspaces/computes/setupScripts.scripts.creationScript.scriptSource\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;inline\u0026#34; }, { \u0026#34;field\u0026#34;: \u0026#34;Microsoft.MachineLearningServices/workspaces/computes/setupScripts.scripts.creationScript.scriptData\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IyEvYmluL2Jhc2gKCmVjaG8gIkhlbGxvIFdvcmxkIg==\u0026#34; } ] } } } Next, create a new Policy definition in Azure Policy:\nCreate a new Policy definition You\u0026rsquo;ll need to define in which subscription the definition should live in, give it a name and a description and then finally paste the policy JSON into the Policy Rule. Then hit Save.\nOur Policy definition Now that we have a Policy, we can create an assignment. This will apply the policy to Azure resources. Inside Azure Policy, navigate to Assignments and select Assign policy.\nCreating our assignment Then define the scope of the assignment (which subscriptions it should affect), select our Policy definition and hit Create.\nAsssigning our policy That\u0026rsquo;s it, our policy is live!\nMaking sure it works # Lastly, go to Azure Machine Learning Studio and provision a new Compute Instance without selecting an init script. Once the instance has been provisioned, open the instance details and you should see the stdout under the Logs tab.\nOur init script successfully executed Hope this helps!\nHappy instance creating!\n","date":"19 July 2021","permalink":"/posts/enforcing-init-scripts-on-azure-machine-learning-compute-instances/","section":"Posts","summary":"Introduction # This post is a quick tip, showing how you can automatically enforce an Init Script during Azure Machine Learning Compute Instance provisioning.","title":"Enforcing Init Scripts on Azure Machine Learning Compute Instances"},{"content":"Introduction # This post will explain deploying Azure Machine Learning models to Azure App Service. This allows for easier model deployments, especially for those users who do not want to deploy to e.g., Azure Kubernetes Service. In this post, we will follow the proposed approach from the official documentation to a certain degree, but we will also add model telemetry/metrics logging and Model Data Collection. This allows for a \u0026lsquo;more complete\u0026rsquo; approach of model deployment with model monitoring over just running a plain Docker container on App Service.\nOverall, this example can be reused to deploy the Docker images generated by Azure Machine Learning to any platform that is capable of running Docker images.\nDeployment Architecture If we look at the architecture diagram above, we\u0026rsquo;ll focus on the following steps in this post:\nPackaging the model as a Docker image Deploying the image to App Service Adding model telemetry logging to Application Insights Adding model data collection to Azure Blob Consuming the model using its exposed API Getting Started # To get started, we assume that we already have:\na registered model in Azure Machine Learning a score.py scoring script with model data collection enabled (example: score.py) a conda.yml with your model dependencies (example: conda.yml) In short, you already have taken the steps to train and deploy a model. Packaging our model for deployment # First, we want to package our existing model using the registered model, our scoring script and our Conda environment:\nfrom azureml.core import Workspace, Model from azureml.core.model import InferenceConfig from azureml.core.environment import Environment from azureml.core.conda_dependencies import CondaDependencies ws = Workspace.from_config() env = Environment(\u0026#34;inference-env\u0026#34;) env.docker.enabled = True # Replace with your conda enviroment file env.python.conda_dependencies = CondaDependencies(\u0026#34;./conda.yml\u0026#34;) # Replace with your score.py inference_config = InferenceConfig(entry_script=\u0026#34;score.py\u0026#34;, environment=env) # Replace with your model model = Model(ws, \u0026#39;my-model\u0026#39;) package = Model.package(ws, [model], inference_config) package.wait_for_creation(show_output=True) print(f\u0026#34;Packaged model image: {package.location}\u0026#34;) The code will return the URL to the new Docker image:\nPackaged model image: amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx Now that we have the model image built, we can start deploying it to a Docker runtime. If you need a more customized Docker image (e.g., maybe you are required to add a few more complex dependencies), you can follow the tutorial from the last post on building custom Docker images.\nRunning the model locally # Next, we can try running the model locally on our laptop, Compute Instance or where ever you have Docker running. For this, we log in to the Azure Container Registry where our model image has been stored:\ndocker login amldemowexxxxxx.azurecr.io You can easily retrieve the login credentials for the Container Registry through the Azure Portal:\nContainer Registry details From here, we can run the image via Docker by forwarding web service port 5001 to the host:\ndocker run -it --rm \\ -p 5001:5001 \\ amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx And then quickly test if we can call the model successfully:\nimport json, requests test_sample = json.dumps({ \u0026#39;data\u0026#39;: [{ \u0026#34;Age\u0026#34;: 20, \u0026#34;Sex\u0026#34;: \u0026#34;male\u0026#34;, \u0026#34;Job\u0026#34;: 0, \u0026#34;Housing\u0026#34;: \u0026#34;own\u0026#34;, \u0026#34;Saving accounts\u0026#34;: \u0026#34;little\u0026#34;, \u0026#34;Checking account\u0026#34;: \u0026#34;little\u0026#34;, \u0026#34;Credit amount\u0026#34;: 100, \u0026#34;Duration\u0026#34;: 48, \u0026#34;Purpose\u0026#34;: \u0026#34;radio/TV\u0026#34; }] }) url = \u0026#34;http://localhost:5001/score\u0026#34; headers = {\u0026#39;Content-Type\u0026#39;:\u0026#39;application/json\u0026#39;} response = requests.post(url, test_sample, headers=headers) print(response.status_code) print(response.text) Which in our case here, returns the HTTP response code and the mode\u0026rsquo;s predictions:\n200 {\u0026#34;predict_proba\u0026#34;: [[0.6900664207902661, 0.30993357920973386]]} Perfect, our model is up and running, next we\u0026rsquo;ll add some telemetry logging to Application Insights.\nAdding model telemetry logging # Next, we can add model telemetry logging to Application Insights. We can achieve this by setting the appropriate logging-related environment variables in our Docker command:\nAML_APP_INSIGHTS_ENABLED=true AML_APP_INSIGHTS_KEY=\u0026lt;Instrumentation key\u0026gt; WORKSPACE_NAME=\u0026lt;Name of your Workspace\u0026gt; SERVICE_NAME=\u0026lt;arbitrary service name, e.g. deployment name or build Id\u0026gt; You can retrieve the Instrumentation key for Application Insights from the Azure Portal:\nRetrieving our Application Insights key Adding those to our Docker command should look like this:\ndocker run -it --rm \\ -e WORKSPACE_NAME=aml-demo-we \\ -e SERVICE_NAME=build12345 \\ -e AML_APP_INSIGHTS_ENABLED=true \\ -e AML_APP_INSIGHTS_KEY=1f224928-xxxx-xxxx-xxxx-xxxxxxxxx \\ -p 5001:5001 \\ amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx Now, once we run the model and send some data to it, we should see it popping up in Application Insights by going to \u0026ldquo;Log Analytics\u0026rdquo; and querying for requests:\nOur requests in Application Insights Alternatively, we can also query by traces, which will show us STDOUT/STDERR of our model\u0026rsquo;s code that is running the Docker container:\nOur traces in Application Insights Great, now we have our model running and it is reporting back its STDOUT/STDERR and its predictions to Application Insights. Next, we will also add model data collection, to push model input and prediction data back to Azure Blob Storage.\nAdding model data collection # Lastly, we can add model data collection to our model. For this, we first need to have a storage account with a container called modeldata. In this case, we can just create the container using the Azure Portal:\nCreating a new Blob container for our data collection Next, we need to set the Model Data Collection related environment variables:\nAML_MODEL_DC_STORAGE_ENABLED=true AML_MODEL_DC_STORAGE=\u0026lt;Storage Connection String\u0026gt; In this case, AML_MODEL_DC_STORAGE refers to the connection string to your Storage Account. With this, we can re-run our Docker container with the appropriate parameters set:\ndocker run -it --rm \\ -e WORKSPACE_NAME=aml-demo-we \\ -e SERVICE_NAME=build2542 \\ -e AML_APP_INSIGHTS_ENABLED=true \\ -e AML_APP_INSIGHTS_KEY=123445-1234-1234-1234-12345667889 \\ -e AML_MODEL_DC_STORAGE_ENABLED=true \\ -e AML_MODEL_DC_STORAGE=\u0026#34;DefaultEndpointsProtocol=https;AccountName=xxxxxx;AccountKey=xxxxxxxxx;EndpointSuffix=core.windows.net\u0026#34; \\ -p 5001:5001 \\ amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx After a while (it is currently unclear to me how long this takes), our model input and prediction data will show up in our modeldata container in Blob:\nOur data collection is working From here, we can finally start deploying the model to App Service.\nDeployment to Azure App Service # First, let\u0026rsquo;s create an new App Service:\naz group create --name app-service-deployment --location \u0026#34;West Europe\u0026#34; az appservice plan create --name models --resource-group app-service-deployment --sku B1 --is-linux Next, let\u0026rsquo;s deploy our container to the App Service (it won\u0026rsquo;t pull the image yet as we first need to add authentication to our Container Registry):\naz webapp create --resource-group app-service-deployment --plan models --name model1-blog-demo --deployment-container-image-name amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx Next, let\u0026rsquo;s add the Managed Identity of our new app to the Container Registry, so it can pull the image:\n# Assign Managed Identity to our Web App az webapp identity assign --resource-group app-service-deployment --name model1-blog-demo --query principalId --output tsv # Query the resource id of our Container Registry az acr show -g aml-demo-we -n amldemowexxxxxx --query id --output tsv # Assign Pull permission of our Web App to our Container Registry az role assignment create --assignee \u0026lt;id from first command\u0026gt; --scope \u0026lt;output from second command\u0026gt; --role \u0026#34;AcrPull\u0026#34; Next, we can add the port mapping and our environment variables:\naz webapp config appsettings set --resource-group app-service-deployment --name model1-blog-demo --settings WEBSITES_PORT=5001 az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings WORKSPACE_NAME=\u0026#34;aml-demo-we\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings SERVICE_NAME=\u0026#34;build12345\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings AML_APP_INSIGHTS_ENABLED=\u0026#34;true\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings AML_APP_INSIGHTS_KEY=\u0026#34;123445-1234-1234-1234-12345667889\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings AML_MODEL_DC_STORAGE_ENABLED=\u0026#34;true\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings AML_MODEL_DC_STORAGE=\u0026#34;DefaultEndpointsProtocol=https;AccountName=xxxxx;AccountKey=xxxxxxxx;EndpointSuffix=core.windows.net\u0026#34; Lastly, let\u0026rsquo;s restart our app so it pulls the new settings:\naz webapp restart --resource-group app-service-deployment --name model1-blog-demo From here, we can finally call our endpoint (same code as above) using https://model1-blog-demo.azurewebsites.net. After a few minutes, our telemetry metrics and data collection should start to kick in show our model\u0026rsquo;s telemetry in Azure.\nNext Steps # As a last step, we should consider two open points: authentication and automation.\nFirstly, we will need to enable authentication via an identity provider for our deployed model. The web app is per default publicly exposed and by using e.g., Azure Active Directory, we can force the model consumers to authenticate. Furthermore, we can use the rich networking settings on App Service to further lock down the service if desired.\nSecondly, we obviously should use some form of Continuous Deployment to automate the deployment steps. This can be fairly easily done by using the commands provided in this post and putting them into a CI/CD pipeline of your choice, e.g. in Azure DevOps or GitHub Actions.\nSummary # This post gave a short overview how to packages models to Docker images in Azure Machine Learning. From there, we discussed options how to capture model telemetry and also enable model data collection. With this, we can now deploy models easily to various platform, such as Azure App Service, while still receiving logs in Application Insights and data in Azure Blob.\n","date":"5 May 2021","permalink":"/posts/deploying-azure-machine-learning-models-azure-app-service/","section":"Posts","summary":"Introduction # This post will explain deploying Azure Machine Learning models to Azure App Service.","title":"Deploying Azure Machine Learning Models to Azure App Service"},{"content":"Introduction # This posts shows how to use custom images in Azure Machine Learning for model training. \u0026ldquo;Why would I need that?\u0026rdquo; – you might ask – \u0026ldquo;Doesn\u0026rsquo;t AzureML handle all of this?\u0026rdquo; – and yes, you\u0026rsquo;re right, Azure Machine Learning does all of that. However, there are a few scenarios where building your own custom images enables a few new scenarios:\nInclusion of libraries not available via pip/conda (odbc drivers, etc.) Stricter security processes (image creation, scanning, consumption) Enable scenarios, where Azure Machine Learning is not allowed to talk the non-Azure resources on the internet (e.g., PyPI, conda-forge, etc.) These might not be your typical everyday use cases, but especially in more lock-down, secured environments, these scenarios are quite common. So let\u0026rsquo;s get started!\nBuilding Custom Images # The idea of custom images for Azure Machine Learning is that you take one of the base AML images and add your additional Docker image layers on top of them. You can refer to this repo for getting a list of all the base images available for Azure Machine Learning. I typically end up using one of these:\nmcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04 mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu\u0026lt;x\u0026gt;.04 (x=16/18) mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.y-cudnn7-ubuntu\u0026lt;x\u0026gt;.04 (x=16/18, y=0/1/2) But wait, can\u0026rsquo;t I build a fully-customized image from scratch? Yes, you can, but I would not recommend it as using the AML base images will ensure that your resulting image will (very likely) stay compatible with AML.\nFor this post, I\u0026rsquo;ve created this example repo as a starting point: https://github.com/csiebler/aml-custom-image – feel free to clone it and get started.\nTo keep it simple, I\u0026rsquo;ve just put a simple requirements.txt that gets installed into the image (see Dockerfile):\nFROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04 COPY requirements.txt /tmp/requirements.txt RUN pip install -r /tmp/requirements.txt From here, add all the stuff you want to package into your custom image.\nOnce done, let\u0026rsquo;s build this image and put it to work! I assume you already have a AML workspace up again running. So first, let\u0026rsquo;s log in to your Azure Container Registry via CLI:\naz acr login --name amldemowe12345 docker login amldemowe12345.azurecr.io You can easily retrieve the login credentials for the Container Registry through the Azure Portal:\nACR credentials in Azure Portal Next, you can build the image and push it to the registry:\ndocker build . -t amldemowe12345.azurecr.io/azureml-images/scikit-learn:0.23.2 docker push amldemowe12345.azurecr.io/azureml-images/scikit-learn:0.23.2 Great, now we\u0026rsquo;ve pushed the image, we can get started to run some training jobs with it!\nTraining using Custom Images # Let\u0026rsquo;s look how we can use the new image and run a training job with it, either with the CLI or the Python SDK.\nUsing the az ml CLI # You can run a training job in AML by editing the training.runconfig from the repo and update the docker -\u0026gt; baseImage section to point to your newly created image:\n... docker: enabled: true baseImage: amldemowe12345.azurecr.io/azureml-images/scikit-learn:0.23.2 ... Next, we can use the az ml CLI to attach to the workspace. This will authenticate us and set the resource group/workspace as default for subsequent CLI commands. From there, we can kick off the training using the training.runconfig:\naz ml folder attach -g aml-demo -w aml-demo az ml run submit-script -c training -e custom-image-training If you look at the training run in the ML Studio, you can see that the image has been pulled correctly, without AML building its own image:\nAzureML pulling our custom image Using the Python SDK # Alternatively, you can run this example via the Python SDK:\nfrom azureml.core import Workspace, Environment from azureml.core import ScriptRunConfig from azureml.core.compute import ComputeTarget from azureml.core import Experiment ws = Workspace.from_config() custom_env = Environment(\u0026#34;custom_env\u0026#34;) custom_env.docker.enabled = True custom_env.python.user_managed_dependencies = True custom_env.docker.base_image = \u0026#34;amldemowe12345.azurecr.io/azureml-images/scikit-learn:0.23.2\u0026#34; cluster_name = \u0026#34;cpu-cluster\u0026#34; compute_target = ComputeTarget(workspace=ws, name=cluster_name) src = ScriptRunConfig(source_directory=\u0026#39;./train-example\u0026#39;, script=\u0026#39;train.py\u0026#39;, compute_target=compute_target, environment=custom_env) run = Experiment(ws,\u0026#39;custom-image-training\u0026#39;).submit(src) run.wait_for_completion(show_output=True) Again, use the log output from the training run in the Studio UI to confirm, that AML used your custom image!\nNext Steps # From here, I would recommend to automate building your custom Docker images through e.g., Azure DevOps or GitHub Actions. This will also allow you to further harden the image by e.g., integrating with third-party image scanning capabilities and others.\nSummary # In conclusion, using custom images with Azure Machine Learning is a great way to control which packages are being during model training. Typically, AML does this automatically for us, but in more restrictive environments, it is often desired to do this outside in a CI/CD pipeline. Furthermore, custom images help us to enable environments where internet access for AML can be safely disabled.\nI hope this post was helpful – feel free to reuse the code from https://github.com/csiebler/aml-custom-image.\n","date":"3 May 2021","permalink":"/posts/azure-machine-learning-custom-images/","section":"Posts","summary":"Introduction # This posts shows how to use custom images in Azure Machine Learning for model training.","title":"Azure Machine Learning Custom Images (for Advanced Scenarios)"},{"content":"Introduction # This post outlines how you can mount a Dataset to a Compute Instance in Azure Machine. This can help exploring file-based datasets in Jupyter, especially for large datasets where download to the disk of the Compute Instance is impractical. Furthermore, this method can also help during exploration phase, where you probably want to read only a subset of the data.\nPrerequisites # All we need to get started is a Workspace with a file-based Dataset, as well as a Compute Instance. Im testing this on a STANDARD_DS3_V2 instance in West Europe.\nDataset in Azure Machine Learning Mounting a Dataset to a Compute Instance # Lets start up Jupyter or JupyterLab on the Compute Instance. You can execute the following code to mount the dataset to the machine, access the data, and then later unmount it:\nimport os import pandas as pd from azureml.core import Workspace, Dataset # Connect to Workspace and reference Dataset ws = Workspace.from_config() dataset = ws.datasets[\u0026#34;german-credit-train-tutorial\u0026#34;] # Create mountcontext and mount the dataset mount_ctx = dataset.mount() mount_ctx.start() # Get the mount point dataset_mount_folder = mount_ctx.mount_point print(dataset_mount_folder) # List the files in the mount point files = os.listdir(dataset_mount_folder) print(files) # Read some data df = pd.read_csv(os.path.join(dataset_mount_folder, \u0026#39;german_credit_data.csv\u0026#39;)) # Do some more stuff with the data.... # Unmount the dataset from the instance mount_ctx.stop() If you want to mount the dataset to a specific folder, you can also specify the mount path, e.g., mount(mount_point='/mnt/dataset1'). In this case, the path already needs to exist. See the API documentation for more details.\nMounting a Dataset in Juypter In case you forget to stop the mount context (i.e., unmounting it), no worries! You can also do it from the command line later:\nazureuser@clemens-vm:/$ mount | grep /tmp _DPrepFuse on /tmp/tmp89tgbd31 type fuse (rw,nosuid,nodev,relatime,user_id=1001,group_id=1002) azureuser@clemens-vm:/$ sudo umount /tmp/tmp89tgbd31 azureuser@clemens-vm:/$ mount | grep /tmp Restarting the Compute Instance will also remove the mount. Azure Machine Learning uses fuse to mount the Storage Account. Therefore, there is no drawback on the Storage Account in case you forget to unmount it. The dataset is mounted as read-only, there you cannot cause any inconsistencies anyway.\nPerformance # In my quick test, I was using a 10 GB file in Blob Storage and just read it into the Compute Instance:\n$ dd if=test_10gb.tmp of=/dev/null bs=64k 163840+0 records in 163840+0 records out 10737418240 bytes (11 GB, 10 GiB) copied, 89.567 s, 120 MB/s The results are very much in line with the expected performance for reading a single file in Blob (Standard tier). Typically, we should see around ~80-120MB/s per file (sometimes even a bit more). Reading files in parallel, leveraging Premium Blob, ADLSg2, or using a larger Compute Instance size, etc. could obviously improve performance even more.\nSummary # Mounting a Dataset to a Compute Instance in Azure Machine Learning is easy and can dramatically help during data exploration and when dealing with large datasets.\nStay safe and let me know if you have any questions!\n","date":"7 December 2020","permalink":"/posts/mount-datasets-compute-instance-azure-machine-learning/","section":"Posts","summary":"Introduction # This post outlines how you can mount a Dataset to a Compute Instance in Azure Machine.","title":"Mounting Datasets to a Compute Instance in Azure Machine Learning"},{"content":"I\u0026rsquo;m Clemens – I studied Computer Science at the Karlsruhe Institute of Technology, Germany and spend a year researching at Carnegie Mellon University in Pittsburgh, PA, USA. In my professional life I worked in several roles as a Software Engineer, Cloud Solution Architect, and also as a manager of a team of Solution Architects. Over the past couple of years, I\u0026rsquo;ve published multiple patents and research papers. I am currently working as a Specialist for AI \u0026amp; ML at Microsoft.\nIn my spare time I enjoy traveling and keeping up with the civil aviation industry, hiking up the Bavarian Alps, and last but not least, spend as much time as possible cooking in the kitchen.\n","date":null,"permalink":"/about/","section":"Clemens Siebler's Blog","summary":"I\u0026rsquo;m Clemens – I studied Computer Science at the Karlsruhe Institute of Technology, Germany and spend a year researching at Carnegie Mellon University in Pittsburgh, PA, USA.","title":"About"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" Speaking in front of small and large audiences has been my passion for many years – if you want to see me on stage, check out my upcoming talks. If you want to host me for a talk, feel free to reach out to me!\nPast events # Medientage München #MTM22 – Munich, Germany (October 2022) # Synthetische Stimmen bei ntv​ - Sprachtechnologie als strategisches Instrument für Reichweiten- und Produktentwicklung​ (together with Tatjana Anisimov) Microsoft Business Summit 2019 – Leipzig, Germany (October 2019) # Künstliche Intelligenz anhand von Praxisbeispielen besser verstehen Künstliche Intelligenz für alle – wie der Einstieg in KI durch die Azure Plattform immer einfacher wird Intelligent ERP: Vom Systemintegrator (SI) zum SaaS-Anbieter Global AI Nights 2019 – Munich, Germany (September 2019) # Automated Machine Learning on Azure Microsoft #DPK18 – Leipzig, Germany (October 2018) # AI \u0026amp; ML: Das benötigen Sie, um technische Projekte erfolgreich umzusetzen AI \u0026amp; ML: Eindrucksvolle Demos, mit denen Sie neue Projekte generieren Microsoft Tech Briefing – Munich, Germany (May 2018) # DevOps, Containers \u0026amp; Serverless Computing NetApp Insight 2018 – Barcelona, Spain (April 2018) # Microsoft: Migrate to Azure with Confidence: Start Moving your Data, Applications, and Infrastructure Today Microsoft Tech Briefing – Munich, Germany (April 2018) # Artificial Intelligence \u0026amp; Machine Learning on Azure NetApp Insight 2017 – Berlin, Germany (November 2017) # Building Scalable Web Applications with NetApp StorageGRID Webscale Everything You Need To Know About the S3 Protocol in NetApp StorageGRID Webscale Red Hat Forum Austria 2017 – Vienna, Austria (October 2017) # Running Stateful Container Workloads with NetApp (Slides) NetApp Insight 2017 – Las Vegas, USA (October 2017) # Building Scalable Web Applications with NetApp StorageGRID Webscale Everything You Need To Know About the S3 Protocol in NetApp StorageGRID Webscale Large-scale StorageGRID Deployments – Best Practices from the Field NetApp Insight 2016 – Berlin, Germany (November 2016) # Building Scalable Web Applications with NetApp StorageGRID Webscale Everything You Need To Know About the S3 Protocol in NetApp StorageGRID Webscale NetApp Insight 2016 – Las Vegas, USA (October 2016) # Building Scalable Web Applications with NetApp StorageGRID Webscale Everything You Need To Know About the S3 Protocol in NetApp StorageGRID Webscale Docker User Group Stuttgart #4 (August 2016) # Persistent Storage for Containers NetApp Insight 2015 – Berlin, Germany (November 2015) # Building Scalable Web Applications with NetApp StorageGRID Webscale How to Deploy NetApp Private Storage for Microsoft Azure NetApp Insight 2015 – Las Vegas, USA (October 2015) # Building Scalable Web Applications with NetApp StorageGRID Webscale ","date":null,"permalink":"/talks/","section":"Clemens Siebler's Blog","summary":"Speaking in front of small and large audiences has been my passion for many years – if you want to see me on stage, check out my upcoming talks.","title":"Talks"}]