[{"content":"","date":null,"permalink":"/","section":"Clemens Siebler's Blog","summary":"","title":"Clemens Siebler's Blog"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"Introduction #In this post, we\u0026rsquo;ll explain how to deploy Deepseek R1 via vLLMs using Azure Machine Learning\u0026rsquo;s Managed Online Endpoints for efficient, scalable, and secure real-time inference. This has the benefit that the model is running within your own Azure subscription and you\u0026rsquo;re in full control of what is happening to your data. For example, this allows to deploy R1 within a region of the US or EU, e.g., eastus2, swedencentral or others.\nIn detail, we\u0026rsquo;ll be deploying DeepSeek-R1-Distill-Llama-8B, which is based off Llama-3.1-8B.\nTools used #To deploy R1, we\u0026rsquo;ll be using:\nvLLM Managed Online Endpoints in Azure Machine Learning Here\u0026rsquo;s a short summary of what those two components do:\nIntroduction to vLLM #vLLM is a high-throughput and memory-efficient inference and serving engine designed for large language models (LLMs). It optimizes the serving and execution of LLMs by utilizing advanced memory management techniques, such as PagedAttention, which efficiently manages attention key and value memory. This allows for continuous batching of incoming requests and fast model execution, making vLLM a powerful tool for deploying and serving LLMs at scale.\nvLLM supports seamless integration with popular Hugging Face models and offers various decoding algorithms, including parallel sampling and beam search. It also supports tensor parallelism and pipeline parallelism for distributed inference, making it a flexible and easy-to-use solution for LLM inference (see full docs).\nManaged Online Endpoints in Azure Machine Learning #Managed Online Endpoints in Azure Machine Learning provide a streamlined and scalable way to deploy machine learning models for real-time inference. These endpoints handle the complexities of serving, scaling, securing, and monitoring models, allowing us to focus on building and improving your models without worrying about infrastructure management.\nDeepseek R1 model deployment #Let\u0026rsquo;s go through deploying R1 on Azure Machine Learning\u0026rsquo;s Managed Online Endpoints. For this, we\u0026rsquo;ll use a custom Dockerfile and configuration files to set up the deployment. As a model, we\u0026rsquo;ll be using deepseek-ai/DeepSeek-R1-Distill-Llama-8B on a single Standard_NC24ads_A100_v4 instance.\nStep 1: Create a custom Environment for vLLM on AzureML #First, we create a Dockerfile to define the environment for our model. For this, we\u0026rsquo;ll be using vllm\u0026rsquo;s base container that has all the dependencies and drivers included:\nFROM vllm/vllm-openai:latest ENV MODEL_NAME deepseek-ai/DeepSeek-R1-Distill-Llama-8B ENTRYPOINT python3 -m vllm.entrypoints.openai.api_server --model $MODEL_NAME $VLLM_ARGS The idea here is that we can pass a model name via an ENV variable, so that we can easily define which model we want to deploy during deployment time.\nNext, we log into our Azure Machine Learning workspace:\naz account set --subscription \u0026lt;subscription ID\u0026gt; az configure --defaults workspace=\u0026lt;Azure Machine Learning workspace name\u0026gt; group=\u0026lt;resource group\u0026gt; Now, we create an environment.yml file to specify the environment settings:\n$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json name: r1 build: path: . dockerfile_path: Dockerfile Then let\u0026rsquo;s build the environment:\naz ml environment create -f environment.yml Step 2: Deploy the AzureML Managed Online Endpoint #Time for deployment, so let\u0026rsquo;s first create an endpoint.yml file to define the Managed Online Endpoint:\n$schema: https://azuremlsdk2.blob.core.windows.net/latest/managedOnlineEndpoint.schema.json name: r1-prod auth_mode: key Let\u0026rsquo;s create it:\naz ml online-endpoint create -f endpoint.yml For the next step, we\u0026rsquo;ll need the address of the Docker image address we created. We can quickly get it from AzureML Studio -\u0026gt; Environments -\u0026gt; r1:\nDocker Image address Finally, we create a deployment.yml file to configure the deployment settings and deploy our desired model from HuggingFace via vLLM:\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json name: current endpoint_name: r1-prod environment_variables: MODEL_NAME: deepseek-ai/DeepSeek-R1-Distill-Llama-8B VLLM_ARGS: \u0026#34;\u0026#34; # optional args for vLLM runtime environment: image: xxxxxx.azurecr.io/azureml/azureml_xxxxxxxx # paste Docker image address here inference_config: liveness_route: port: 8000 path: /ping readiness_route: port: 8000 path: /health scoring_route: port: 8000 path: / instance_type: Standard_NC24ads_A100_v4 instance_count: 1 request_settings: # This section is optional, yet important for optimizing throughput max_concurrent_requests_per_instance: 32 request_timeout_ms: 60000 liveness_probe: initial_delay: 10 period: 10 timeout: 2 success_threshold: 1 failure_threshold: 30 readiness_probe: initial_delay: 120 # wait for 120s before we start probing, so the model can load peacefully period: 10 timeout: 2 success_threshold: 1 failure_threshold: 30 Some notes on the parameter choices that we should consider when deploying this:\ninstance_count - defines how many nodes of Standard_NC24ads_A100_v4 we want to spin up. max_concurrent_requests_per_instance - defines how many concurrent requests we\u0026rsquo;ll let pass through the endpoint into the queue before returning a HTTP 429. request_timeout_ms - amount of milliseconds that can pass, until the endpoint closes the connection. In our case here, the client will receive a HTTP 408 after waiting for 60 seconds. Changing these parameters will have the following impact:\nIncreasing max_concurrent_requests_per_instance will increase overall throughput and TPMs (tokens per minute), but will also increase total latency for a given call. Increasing request_timeout_ms will allow clients to wait longer for a response, given that max_concurrent_requests_per_instance has not been exhausted yet. Increasing instance_count will linearly scale throughput and cost. A full explanation of the parameters can be found in my prior post Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints .\nLastly, we can deploy the r1 model:\naz ml online-deployment create -f deployment.yml --all-traffic By following these steps, we have deployed a HuggingFace model on Azure Machine Learningâ€™s Managed Online Endpoints, ensuring efficient and scalable real-time inference. Time to test it!\nStep 2: Testing the deployment #First, let\u0026rsquo;s get the endpoint\u0026rsquo;s scoring uri and the api keys:\naz ml online-endpoint show -n r1-prod az ml online-endpoint get-credentials -n r1-prod For completion models, we can then call the endpoint using this Python code snippet:\nimport requests url = \u0026#34;https://r1-prod.polandcentral.inference.ml.azure.com/v1/chat/completions\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer xxxxxxxxxxxx\u0026#34; } data = { \u0026#34;model\u0026#34;: \u0026#34;deepseek-ai/DeepSeek-R1-Distill-Llama-8B\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is better, summer or winter?\u0026#34; }, ] } response = requests.post(url, headers=headers, json=data) print(response.json()) Answer looks good, we can see the \u0026lt;think\u0026gt; tags where the model does its reasoning:\n{ \u0026#34;id\u0026#34;:\u0026#34;chatcmpl-ccf51218-30a0-4200-bfa4-5d90ac1fdd98\u0026#34;, \u0026#34;object\u0026#34;:\u0026#34;chat.completion\u0026#34;, \u0026#34;created\u0026#34;:1738058980, \u0026#34;model\u0026#34;:\u0026#34;deepseek-ai/DeepSeek-R1-Distill-Llama-8B\u0026#34;, \u0026#34;choices\u0026#34;:[ { \u0026#34;index\u0026#34;:0, \u0026#34;message\u0026#34;:{ \u0026#34;role\u0026#34;:\u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;:\u0026#34;\u0026lt;think\u0026gt;\\nHmm, the user is asking whether summer or winter is better. I need to consider various aspects to make the answer... that both seasons have their pros and cons, and it\u0026#39;s about what you value more.\\n\u0026lt;/think\u0026gt;\\n\\nThe debate between summer and winter is subjective and depends on personal preferences. Here\u0026#39;s a breakdown of each season\u0026#39;s characteristics:\\n\\n### Summer:\\n- **Weather**: Generally hotter and longer days, ideal for beach activities, hiking, and outdoor sports.\\n- **Nature**: Blooms with flowers, greenery, and fruit ripening; many animals are active.\\n- **Events**: Festivals, concerts, and seasonal events like Independence Day.\\n- **Challenges**: Hotter temperatures can be uncomfortable indoors; some people find it harder to sleep in the heat.\\n\\n### Winter:\\n- **Weather**: Cooler temperatures, shorter days, and potentially snowy conditions.\\n- **Nature**: Fewer bugs, quieter wildlife; plants slow down growth; fewer daylight hours can affect mood.\\n- **Events**: Holidays like Christmas, winter festivals, and activities like skiing or snowboarding.\\n- **Challenges**: Cold weather can be inconvenient; Infrastructure issues like slippery roads and closed schools.\\n\\n### Neutral Considerations:\\n- **Light and Mood**: Summer\u0026#39;s extended daylight can boost energy, while winter\u0026#39;s earlier sunset may lead to a more restful environment.\\n- **Comfort**: Summer can feel stuffy indoors; winter may require more layers but can feel cozy at home.\\n\\n### Conclusion:\\nBoth seasons have their highs and lows, and the choice between summer and winter is largely a matter of personal preference. If you enjoy outdoor activities, parties, and vibrant social scenes, summer might be your pick. If you prefer a quieter, more introspective time, winter can be more appealing. Ultimately, the better season depends on what you\u0026#39;re searching for.\u0026#34;, \u0026#34;tool_calls\u0026#34;:[ ] }, \u0026#34;logprobs\u0026#34;:\u0026#34;None\u0026#34;, \u0026#34;finish_reason\u0026#34;:\u0026#34;stop\u0026#34;, \u0026#34;stop_reason\u0026#34;:\u0026#34;None\u0026#34; } ], \u0026#34;usage\u0026#34;:{ \u0026#34;prompt_tokens\u0026#34;:11, \u0026#34;total_tokens\u0026#34;:553, \u0026#34;completion_tokens\u0026#34;:542, \u0026#34;prompt_tokens_details\u0026#34;:\u0026#34;None\u0026#34; }, \u0026#34;prompt_logprobs\u0026#34;:\u0026#34;None\u0026#34; } Works! Lastly, we can also use the OpenAI SDK to perform streaming:\nfrom openai import OpenAI url = \u0026#34;https://r1-prod.polandcentral.inference.ml.azure.com/v1\u0026#34; client = OpenAI(base_url=url, api_key=\u0026#34;xxxxxxxx\u0026#34;) response = client.chat.completions.create( model=\u0026#34;deepseek-ai/DeepSeek-R1-Distill-Llama-8B\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is better, summer or winter?\u0026#34;}, ], stream=True ) for chunk in response: delta = chunk.choices[0].delta if hasattr(delta, \u0026#34;content\u0026#34;): print(delta.content, end=\u0026#34;\u0026#34;, flush=True) Monitoring our deployment #We can also monitor the resource utilization, most importantly obviously the GPUs via Azure Monitor, as outline here. When putting constant load on the model, we see that vLLM eats up 90% of the GPU memory (default setting) and we see GPU utilization close to 100%:\nMetrics in Azure Monitor Autoscaling our Deepseek R1 deployment #Autoscaling Managed Online Endpoint deployments in Azure Machine Learning allows us to dynamically adjust the number of instances allocated to our endpoints based on real-time metrics and schedules. This ensures that our application can handle varying loads efficiently without manual intervention. By integrating with Azure Monitor, you can set up rules to scale out when the CPU or GPU utilization exceeds a certain threshold or scale in during off-peak hours. For detailed guidance on configuring autoscaling, you can refer to the official documentation.\nSummary #In this post, we\u0026rsquo;ve discussed how to deploy the 8bn parameter version of Deepseek\u0026rsquo;s R1 model to Azure Machine Learning\u0026rsquo;s Managed Online Endpoints for efficient real-time inference. This allowed us to use R1 in a secure, private environment where we have full control over the data and full model ownership. Furthermore, this allowed us to run the model in an Azure region of our choice, e.g., in a US- or EU-based Azure data center.\n","date":"28 January 2025","permalink":"/posts/deploying-deepseek-r1-azure-machine-learning/","section":"Posts","summary":"","title":"Securely deploying Deepseek R1 on Azure Machine Learning"},{"content":"Introduction #A few days ago, Microsoft published the GPT-4o-Audio-Preview API on Azure OpenAI. This new, powerful API marks a significant leap forward in the AI landscape, particularly for businesses exploring audio-driven solutions. The gpt4o-audio model enables audio prompts, generates spoken responses, and delivers advanced audio analysis capabilities, offering developers the tools to create immersive, voice-first applications. While the technological potential is impressive, understanding the cost implications of using this API is crucial for businesses aiming to deploy it efficiently.\nHence, we\u0026rsquo;ll use this post to investigate how much this API costs across different languages and see how tokens and words will map to $/hour of generated audio.\nCalling the API #First, let\u0026rsquo;s look a quick example for calling the API and generating some audio:\nimport base64 from openai import AzureOpenAI from azure.identity import DefaultAzureCredential, get_bearer_token_provider token_provider=get_bearer_token_provider(DefaultAzureCredential(), \u0026#34;https://cognitiveservices.azure.com/.default\u0026#34;) endpoint = \u0026#34;https://your-endpoint.openai.azure.com/\u0026#34; # Keyless authentication client=AzureOpenAI( azure_ad_token_provider=token_provider, azure_endpoint=endpoint, api_version=\u0026#34;2025-01-01-preview\u0026#34; ) # Make the audio chat completions request completion=client.chat.completions.create( model=\u0026#34;gpt-4o-audio-preview\u0026#34;, modalities=[\u0026#34;text\u0026#34;, \u0026#34;audio\u0026#34;], audio={\u0026#34;voice\u0026#34;: \u0026#34;alloy\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;wav\u0026#34;}, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Read out this message in English: We are thrilled to announce the release of audio support accessible via Chat Completions API featuring the new GPT-4o-Audio preview Model, now available in preview. Building on to our recent launch of GPT-4o-Realtime-Preview, this groundbreaking addition to the GPT-4o family introduces support for audio prompts and the ability to generate spoken audio responses.\u0026#34; } ] ) print(completion.choices[0].message.audio.transcript) print(completion.usage) # Write the output audio data to a file wav_bytes=base64.b64decode(completion.choices[0].message.audio.data) with open(\u0026#34;test.wav\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(wav_bytes) The console should print:\nWe are thrilled to announce the release of audio support accessible via Chat Completions API, featuring the new GPT-4o-Audio preview Model, now available in preview. Building on to our recent launch of GPT-4o-Realtime-Preview, this groundbreaking addition to the GPT-4o family introduces support for audio prompts and the ability to generate spoken audio responses. CompletionUsage(completion_tokens=696, prompt_tokens=88, total_tokens=784, completion_tokens_details=CompletionTokensDetails(audio_tokens=589, reasoning_tokens=0, accepted_prediction_tokens=0, rejected_prediction_tokens=0, text_tokens=107), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0, image_tokens=0, text_tokens=88)) With all this information, let\u0026rsquo;s figure out the cost of this call.\nPricing #We can find the pricing for GPT-4o-Audio-Preview-2024-12-17-Global on the Azure OpenAI pricing page - All information in this post is as of 01/30/2025 and is likely to change in the future:\nType Input Price (per 1M Tokens) Output Price (per 1M Tokens) Text $2.50 $10 Audio $40 $80 So for our example above, we consumed:\nInput tokens:\ntext_tokens: 88 \u0026ndash;\u0026gt; $2.50/1M * 88 = $0.00022 Output tokens:\ntext_tokens: 107 \u0026ndash;\u0026gt; $10/1M * 107 = $0.00107 audio_tokens: 589 \u0026ndash;\u0026gt; $80/1M * 589 = $0.04712 Total: $0.04841\nCost analysis: Audio Generation #So let\u0026rsquo;s looking into how audio generation scales across different length of text and different languages. For this, let\u0026rsquo;s first calculate the cost programmatically:\n# get the length of the audio file import wave with wave.open(\u0026#34;test.wav\u0026#34;, \u0026#34;rb\u0026#34;) as f: audio_length_in_seconds = f.getnframes()/f.getframerate() cost_text_input = completion.usage.prompt_tokens_details.text_tokens * 2.5/1_000_000 cost_text_output = completion.usage.completion_tokens_details.text_tokens * 10/1_000_000 cost_audio_input = completion.usage.prompt_tokens_details.audio_tokens * 40/1_000_000 cost_audio_output = completion.usage.completion_tokens_details.audio_tokens * 80/1_000_000 cost_total = cost_text_input + cost_text_output + cost_audio_input + cost_audio_output cost_per_hour = cost_total / audio_length_in_seconds * 3600 print(f\u0026#34;Cost for generating {audio_length_in_seconds} seconds audio file was ${cost_total}\u0026#34;) print(f\u0026#34;This equates to ${cost_per_hour} per hour of audio\u0026#34;) Let\u0026rsquo;s check the output:\nCost for generating 29.45 seconds audio file was $0.04841 This equates to $5.9176910017 per hour of audio So this means for synthesizing audio, we\u0026rsquo;re looking at roughly $6 per hour. But okay, this was just a small example, so let\u0026rsquo;s run some experiments, scale it and evaluate across a few more languages.\nSynthesis cost per hour across different languages #The question we want to answer is: even though we pay for audio tokens, does this imply the same audio costs per hour across languages?\nSo for sake of testing, we\u0026rsquo;ll use Arabic, Chinese, English, French, German, Korean, and Spanish. For this, I\u0026rsquo;ve created 10 random news articles with gpt4o-mini and used each article for audio input synthesis, then calculated the average cost. The results are as follows:\nLanguage Cost per hour of Audio Arabic $5.94 Chinese $5.93 English $5.91 French $5.94 German $5.92 Korean $5.96 Spanish $5.92 Looking at this data, we can derive our first insight:\nLearning #1: Synthesizing one hour audio costs around $5.93. This is more or less independent of the language.\nNow let\u0026rsquo;s break it down by cost per 1000 words:\nLanguage Price per 1000 words synthesized Arabic $0.90 Chinese $0.55* English $0.82 French $0.72 German $0.94 Korean $1.06 Spanish $0.76 * take this with a grain of salt, as word counting goes map easily\nJust as gpt4o requires different token amounts to process different languages (in some cases 1.5x or even more), gpt4o-audio also results in different costs per word when generating audio. This was expected, as for example German has fairly long words compared to English.\nLearning #2: As expected, cost per word varies depending on the language and averages at $0.87 per 1000 words.\nNow to be fair, this test was at fairly small scale and did not contain a large variety of text input data. Further evaluation is needed with more diverse and longer datasets.\nCost analysis: Audio Input (Transcription) #So let\u0026rsquo;s it the other way around: Take our synthesized news articles (audio) and feed them into GPT-4o-Audio-Preview API for transcription to text. Again, with a bit of math we get to:\nLanguage Price per hour of Audio Arabic $1.55 Chinese $1.54 English $1.49 French $1.56 German $1.54 Korean $1.55 Spanish $1.54 Learning #3: Transcribing audio costs around $1.55 per hour of audio.\nSummary #In this post, we\u0026rsquo;ve explored the cost implications of using the GPT-4o-Audio-Preview API, highlighting how it processes both text and audio tokens. By analyzing a small example and scaling across different languages, we found that generating one hour of audio costs approximately $5.93, regardless of the language. In summary, the cost per 1,000 words synthesized can vary depending on the language, due to differences in token requirements and word lengths. However, on average the cost per 1000 words sits at around $0.87. For transcribing audio to text, the cost is approximately $1.55 per hour of audio, again, independent of the language. This insight provides a clear understanding of the API\u0026rsquo;s pricing, making it easier for developers and business to plan its integration into their projects.\n","date":"24 January 2025","permalink":"/posts/azure-openai-gpt4o-audio-api-cost-analysis/","section":"Posts","summary":"","title":"The new gpt-4o-audio-preview in Azure OpenAI is awesome! But how much will it actually cost me?"},{"content":"Introduction #In this post, we\u0026rsquo;ll explain how to deploy an embedding model like BGE-M3 on Azure Machine Learning\u0026rsquo;s Managed Online Endpoints for efficient, scalable, and secure real-time embedding vector creation. Many embedding models, including BGE-M3 are available on \u0026ldquo;Models-as-a-Platform\u0026rdquo; on Azure AI Studio and can be deployed with a few clicks. However, in this guide we\u0026rsquo;ll do it the \u0026ldquo;manual\u0026rdquo; way, which allows us to easier automate the deployment (LLMOps) and gives us more choice when it comes to deploying the model on smaller VM/GPU types, e.g. a T4 GPU, instead of an A100.\nManaged Online Endpoints in Azure Machine Learning #Managed Online Endpoints in Azure Machine Learning provide a streamlined and scalable way to deploy machine learning models for real-time inference. These endpoints handle the complexities of serving, scaling, securing, and monitoring models, allowing us to focus on building and improving your models without worrying about infrastructure management.\nDeployment Steps #Authoring our scoring script for inference #Firstly, let\u0026rsquo;s write a short score.py script that we\u0026rsquo;ll run on our inference server on Managed Online Endpoints:\nimport json import logging from FlagEmbedding import BGEM3FlagModel def init(): global model logging.info(\u0026#34;Loading model from web...\u0026#34;) model = BGEM3FlagModel(\u0026#39;BAAI/bge-m3\u0026#39;, use_fp16=True) logging.info(\u0026#34;Init complete\u0026#34;) def run(raw_data): data = json.loads(raw_data)[\u0026#34;input\u0026#34;] embeddings = model.encode(data, batch_size=12, max_length=8192) logging.info(f\u0026#34;Embeddings: {embeddings}\u0026#34;) return embeddings[\u0026#39;dense_vecs\u0026#39;].tolist() The code is quite simple:\ninit() - loads the model using the FlagEmbedding library run() - takes in a JSON document with an array called input, which contains a list of strings that need to be embedded. Now, to run this code, we\u0026rsquo;ll need to define our dependencies in conda.yml:\nname: model-env channels: - conda-forge dependencies: - python=3.10 - numpy - pip=23.0.1 - scikit-learn - scipy - pip: - azureml-defaults - inference-schema[numpy-support] - joblib - FlagEmbedding - peft Managed Online Endpoint deployment steps #Time to get started with our deployment, so let\u0026rsquo;s log into our Azure Machine Learning workspace:\naz account set --subscription \u0026lt;subscription ID\u0026gt; az configure --defaults workspace=\u0026lt;Azure Machine Learning workspace name\u0026gt; group=\u0026lt;resource group\u0026gt; Next, we can create our Managed Online Endpoint endpoint.yml definition:\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json name: bge auth_mode: key Then let\u0026rsquo;s do the same for the deployment deployment.yml, which will be running inside our endpoint:\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json name: current endpoint_name: bge code_configuration: code: . scoring_script: score.py environment: conda_file: conda.yml image: mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04:latest instance_type: Standard_NC4as_T4_v3 instance_count: 1 request_settings: max_concurrent_requests_per_instance: 16 request_timeout_ms: 10000 Here, we\u0026rsquo;re using mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04:latest as our base inference image. This image contains cuda, so we can direclty leverage the T4 GPUs of our Standard_NC4as_T4_v3 instance. The full list of base images can be found under Azure/AzureML-Containers.\nFor handling concurrency, we will need to find a good value for max_concurrent_requests_per_instance. In our case, we\u0026rsquo;ve chosen 16, which means API consumers will get a HTTP 429 in case more than 16 requests are being queued at a given point in time. By tuning this, we can optimize the throughput/latency curve. However, this value heavily depends on the SKU instance we\u0026rsquo;re using for the deployment. We\u0026rsquo;ll discuss this more in the testing phase below.\nFinally, let\u0026rsquo;s create the deployment using:\naz ml online-endpoint create -f endpoint.yml az ml online-deployment create -f deployment.yml --all-traffic This will:\nCreate the \u0026ldquo;empty\u0026rdquo; Managed Online Endpoint Automatically bake the conda environment into the azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04:latest base image Deploy the newly created image to an managed Standard_NC4as_T4_v3 instance with our score.py script inside of it Testing the deployment #So let\u0026rsquo;s test the endpoint. First, we\u0026rsquo;ll need to get the endpoint\u0026rsquo;s scoring uri and the API keys:\naz ml online-endpoint show -n bge az ml online-endpoint get-credentials -n beg For our embedding model, we can then call the endpoint using this Python code snippet:\nimport json import requests url = \u0026#34;https://bge.polandcentral.inference.ml.azure.com/score\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; } data = { \u0026#34;input\u0026#34;: [\u0026#34;Hello World\u0026#34;, \u0026#34;This is just a test for another text\u0026#34;] } response = requests.post(url, headers=headers, data=json.dumps(data)) print(response.status_code) print(response.json()) Response:\n[[-0.05633544921875, ..., 0.01433563232421875], [-0.0546875, ..., 0.00677490234375]] Works!\nSo, let\u0026rsquo;s finally put some load on the model and see how it performs. In our case, I\u0026rsquo;m running the following workload profile:\nmax_concurrent_requests_per_instance set to 16 Single Standard_NC4as_T4_v3 instance ~2000 input tokens during the test 16 threads in parallel 10k inference calls Once we run this sustained for a few minutes, we can see the following curve in the Azure Portal (under our endpoint in the Metrics tab):\nGPU utilization benchmark Not fully utilized, but we\u0026rsquo;re getting close to a 100% GPU utilization. From here, we can tune our endpoint for more throughput.\nAutoscaling our embedding endpoint #Autoscaling Managed Online Endpoint deployments in Azure Machine Learning allows us to dynamically adjust the number of instances allocated to our endpoints based on real-time metrics and schedules. This ensures that our application can handle varying loads efficiently without manual intervention. By integrating with Azure Monitor, you can set up rules to scale out when the CPU or GPU utilization exceeds a certain threshold or scale in during off-peak hours. For detailed guidance on configuring autoscaling, you can refer to the official documentation.\nSummary #In this post, we\u0026rsquo;ve discussed how to deploy embedding models like BGE-M3 using Azure Machine Learning\u0026rsquo;s Managed Online Endpoints for efficient real-time inference. The guide outlined the steps for creating the required score.py and conda.yml assets, then defining the endpoint, and deploying our model using Azure CLI commands. We also looked at examples for testing the deployed model.\n","date":"22 October 2024","permalink":"/posts/deploying-bge-m3-embedding-models-azure-machine-learning-managed-online-endpoints/","section":"Posts","summary":"","title":"Deploying BGE-M3 and other embedding models on Azure Machine Learning with Managed Online Endpoints"},{"content":"Introduction #In this post, we\u0026rsquo;ll explain how to deploy LLMs on vLLM using Azure Machine Learning\u0026rsquo;s Managed Online Endpoints for efficient, scalable, and secure real-time inference. To get started, let\u0026rsquo;s briefly look into what vLLM and Managed Online Endpoints are.\nYou can find the full code examples on csiebler/vllm-on-azure-machine-learning.\nIntroduction to vLLM #vLLM is a high-throughput and memory-efficient inference and serving engine designed for large language models (LLMs). It optimizes the serving and execution of LLMs by utilizing advanced memory management techniques, such as PagedAttention, which efficiently manages attention key and value memory. This allows for continuous batching of incoming requests and fast model execution, making vLLM a powerful tool for deploying and serving LLMs at scale.\nvLLM supports seamless integration with popular Hugging Face models and offers various decoding algorithms, including parallel sampling and beam search. It also supports tensor parallelism and pipeline parallelism for distributed inference, making it a flexible and easy-to-use solution for LLM inference (see full docs).\nManaged Online Endpoints in Azure Machine Learning #Managed Online Endpoints in Azure Machine Learning provide a streamlined and scalable way to deploy machine learning models for real-time inference. These endpoints handle the complexities of serving, scaling, securing, and monitoring models, allowing us to focus on building and improving your models without worrying about infrastructure management.\nHuggingFace Model Deployment #Let\u0026rsquo;s go through deploying a HuggingFace model on Azure Machine Learning\u0026rsquo;s Managed Online Endpoints. For this, we\u0026rsquo;ll use a custom Dockerfile and configuration files to set up the deployment. As a model, we\u0026rsquo;ll be using meta-llama/Llama-3.2-11B-Vision on a single Standard_NC24ads_A100_v4 instance.\nStep 1: Create a custom Environment for vLLM on AzureML #First, we create a Dockerfile to define the environment for our model. For this, we\u0026rsquo;ll be using vllm\u0026rsquo;s base container that has all the dependencies and drivers included:\nFROM vllm/vllm-openai:latest ENV MODEL_NAME facebook/opt-125m ENTRYPOINT python3 -m vllm.entrypoints.openai.api_server --model $MODEL_NAME $VLLM_ARGS The idea here is that we can pass a model name via an ENV variable, so that we can easily define which model we want to deploy during deployment time.\nNext, we log into our Azure Machine Learning workspace:\naz account set --subscription \u0026lt;subscription ID\u0026gt; az configure --defaults workspace=\u0026lt;Azure Machine Learning workspace name\u0026gt; group=\u0026lt;resource group\u0026gt; Now, we create an environment.yml file to specify the environment settings:\n$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json name: vllm build: path: . dockerfile_path: Dockerfile Then let\u0026rsquo;s build the environment:\naz ml environment create -f environment.yml Step 2: Deploy the AzureML Managed Online Endpoint #Time for deployment, so let\u0026rsquo;s first create an endpoint.yml file to define the Managed Online Endpoint:\n$schema: https://azuremlsdk2.blob.core.windows.net/latest/managedOnlineEndpoint.schema.json name: vllm-hf auth_mode: key Let\u0026rsquo;s create it:\naz ml online-endpoint create -f endpoint.yml For the next step, we\u0026rsquo;ll need the address of the Docker image address we created. We can quickly get it from AzureML Studio -\u0026gt; Environments -\u0026gt; vllm:\nDocker Image address Finally, we create a deployment.yml file to configure the deployment settings and deploy our desired model from HuggingFace via vLLM:\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json name: current endpoint_name: vllm-hf environment_variables: MODEL_NAME: meta-llama/Llama-3.2-11B-Vision # define the model name using the identifier from HG VLLM_ARGS: \u0026#34;--max-num-seqs 16 --enforce-eager\u0026#34; # optional args for vLLM runtime HUGGING_FACE_HUB_TOKEN: \u0026lt;Your HF token\u0026gt; # use this, if you want to authenticate to HF environment: image: xxxxxx.azurecr.io/azureml/azureml_xxxxxxxx # paste Docker image address here inference_config: liveness_route: port: 8000 path: /ping readiness_route: port: 8000 path: /health scoring_route: port: 8000 path: / instance_type: Standard_NC24ads_A100_v4 instance_count: 1 request_settings: # This section is optional, yet important for optimizing throughput max_concurrent_requests_per_instance: 1 request_timeout_ms: 10000 liveness_probe: initial_delay: 10 period: 10 timeout: 2 success_threshold: 1 failure_threshold: 30 readiness_probe: initial_delay: 120 # wait for 120s before we start probing, so the model can load peacefully period: 10 timeout: 2 success_threshold: 1 failure_threshold: 30 Since vLLM does not support separate probes for readiness and liveness, we\u0026rsquo;ll need to make sure that the model has fully loaded before the fire the first probe. This is why we increased readiness_probe.initial_delay to 120s. For larger models, we should also follow vLLM\u0026rsquo;s documentation for using tensor parallel inference (model on single node but spanning multiple GPUs) by adding --tensor-parallel-size \u0026lt;NUM_OF_GPUs\u0026gt; to VLLM_ARGS. Since we\u0026rsquo;re using a single A100 GPU in our example (Standard_NC24ads_A100_v4), this is not required though.\nThe request_settings depend a bit on our instance type/size and might require some manual tuning to get the model run properly and efficiently. Goal is to find a good tradeoff between concurrency (max_concurrent_requests_per_instance) and queue time in order to avoid either hitting request_timeout_ms from the endpoint side, or any HTTP-timeouts on the client side. Both these scenarios result in HTTP 429, and the client would need to implement exponential backoff (e.g. via tenacity library).\nLastly, we can deploy the model:\naz ml online-deployment create -f deployment.yml --all-traffic By following these steps, we have deployed a HuggingFace model on Azure Machine Learningâ€™s Managed Online Endpoints, ensuring efficient and scalable real-time inference. Time to test it!\nStep 3: Testing the deployment #First, let\u0026rsquo;s get the endpoint\u0026rsquo;s scoring uri and the api keys:\naz ml online-endpoint show -n vllm-hf az ml online-endpoint get-credentials -n vllm-hf For completion models, we can then call the endpoint using this Python code snippet:\nimport requests url = \u0026#34;https://vllm-hf.polandcentral.inference.ml.azure.com/v1/completions\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer xxxxxxxxxxxx\u0026#34; } data = { \u0026#34;model\u0026#34;: \u0026#34;meta-llama/Llama-3.2-11B-Vision\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;San Francisco is a\u0026#34;, \u0026#34;max_tokens\u0026#34;: 200, \u0026#34;temperature\u0026#34;: 0.7 } response = requests.post(url, headers=headers, json=data) print(response.json()) Response:\n{ \u0026#34;id\u0026#34;:\u0026#34;cmpl-74bd153fff5740b3ac070e324f99494c\u0026#34;, \u0026#34;object\u0026#34;:\u0026#34;text_completion\u0026#34;, \u0026#34;created\u0026#34;:1728460457, \u0026#34;model\u0026#34;:\u0026#34;meta-llama/Llama-3.2-11B-Vision\u0026#34;, \u0026#34;choices\u0026#34;:[ { \u0026#34;index\u0026#34;:0, \u0026#34;text\u0026#34;:\u0026#34; top tourist destination known for its iconic landmarks, vibrant neighborhoods, and cultural attractions. Whether you\u0026#39;re interested in history, art, music, or food, there\u0026#39;s something for everyone in this amazing city. Here are some of the top things to do in San Francisco:...,\u0026#34;, \u0026#34;logprobs\u0026#34;:\u0026#34;None\u0026#34;, \u0026#34;finish_reason\u0026#34;:\u0026#34;length\u0026#34;, \u0026#34;stop_reason\u0026#34;:\u0026#34;None\u0026#34;, \u0026#34;prompt_logprobs\u0026#34;:\u0026#34;None\u0026#34; } ], \u0026#34;usage\u0026#34;:{ \u0026#34;prompt_tokens\u0026#34;:5, \u0026#34;total_tokens\u0026#34;:205, \u0026#34;completion_tokens\u0026#34;:200 } } Works!\nCustom Model Deployment #So let\u0026rsquo;s shift gears and deploy our own fine-tuned or own even pre-trained model. How can we use vLLM to deploy this? In short, there are two options:\nIn case of being able to upload the model to HuggingFace, we can follow the deployment steps from above or If we want to keep the model fully private, we can directly deploy it via AzureML In this section, we\u0026rsquo;ll discuss the second option. In order to do so, we\u0026rsquo;ll perform the following steps:\nRegister our custom model in Azure Machine Learning\u0026rsquo;s Model Registry Create a custom vLLM container that supports local model loading Deploy the model to Managed Online Endpoints Step 1: Create a custom Environment for vLLM on AzureML #First, let\u0026rsquo;s create a custom vLLM Dockerfile that takes a MODEL_PATH as input. This path will be used by AzureML to mount our custom model.\nFROM vllm/vllm-openai:latest ENV MODEL_PATH \u0026#34;/models/opt-125m\u0026#34; ENTRYPOINT python3 -m vllm.entrypoints.openai.api_server --model $MODEL_PATH $VLLM_ARGS Then, let\u0026rsquo;s log into our Azure Machine Learning workspace:\naz account set --subscription \u0026lt;subscription ID\u0026gt; az configure --defaults workspace=\u0026lt;Azure Machine Learning workspace name\u0026gt; group=\u0026lt;resource group\u0026gt; Next, we create an environment.yml file to specify the environment settings:\n$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json name: vllm-custom-model build: path: . dockerfile_path: Dockerfile Then let\u0026rsquo;s build the environment:\naz ml environment create -f environment.yml Step 2: Register custom model in Model Registry #Before we continue, we need to register our model. For this, we can go to AzureML Studio, select Models, then select Register. There, we can register our model as Unspecified type and reference the whole folder, which contains all our model\u0026rsquo;s artifacts:\nUpload model folder Next, let\u0026rsquo;s name our model:\nName our custom model And check the final summary before uploading it:\nModel registration summary The folder name will later determine the model\u0026rsquo;s name during inference API calls. In our case, this will be demo-model-125m, derived from the base folder name of the model.\nStep 3: Deploy the AzureML Managed Online Endpoint #It\u0026rsquo;s deployment time! First, we create our endpoint.yml file to define the Managed Online Endpoint:\n$schema: https://azuremlsdk2.blob.core.windows.net/latest/managedOnlineEndpoint.schema.json name: vllm-hf auth_mode: key Then, we create it:\naz ml online-endpoint create -f endpoint.yml For the next step, we\u0026rsquo;ll need the Docker image address, which we can quickly get from AzureML Studio -\u0026gt; Environments -\u0026gt; vllm-custom-model:\nDocker Image address Finally, we create a deployment.yml file to configure the deployment settings and deploy our desired model from HuggingFace via vLLM:\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json name: current endpoint_name: vllm-hf model: azureml:demo-model-125m:1 # specify our registered model model_mount_path: /models # mount to /models path, so model will show up /models/demo-model-125m environment_variables: MODEL_PATH: /models/demo-model-125m # this will need to be set, so vLLM knows where to find the model VLLM_ARGS: \u0026#34;\u0026#34; environment: image: xxxxxx.azurecr.io/azureml/azureml_xxxxxxxx # paste Docker image address here inference_config: liveness_route: port: 8000 path: /health readiness_route: port: 8000 path: /health scoring_route: port: 8000 path: / instance_type: Standard_NC24ads_A100_v4 instance_count: 1 request_settings: max_concurrent_requests_per_instance: 1 request_timeout_ms: 10000 liveness_probe: initial_delay: 10 period: 10 timeout: 2 success_threshold: 1 failure_threshold: 30 readiness_probe: initial_delay: 120 period: 10 timeout: 2 success_threshold: 1 failure_threshold: 30 Here, our focus should be on top the top section:\nmodel: azureml:demo-model-125m:1 - This is the identifier under which our model was registered (azureml:\u0026lt;name\u0026gt;:\u0026lt;version\u0026gt;) model_mount_path: /models - This is to tell our Managed Online Endpoint, under which mount point it should mount the model environment_variables \u0026ndash;\u0026gt; MODEL_PATH: /models/demo-model-125m - This is the path were our vLLM Docker container will look for the model\u0026rsquo;s files environment_variables \u0026ndash;\u0026gt; VLLM_ARGS: \u0026quot;\u0026quot; - Any additional args for vLLM (see section above) For configuring the request_settings section properly, see the steps above.\nLastly, we can deploy the model:\naz ml online-deployment create -f deployment.yml --all-traffic Step 4 - Testing the deployment #Again, let\u0026rsquo;s get the endpoints scoring uri and the api keys:\naz ml online-endpoint show -n vllm-hf az ml online-endpoint get-credentials -n vllm-hf We can then call the endpoint using this Python code snippet:\nimport requests url = \u0026#34;https://vllm-hf.polandcentral.inference.ml.azure.com/v1/completions\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer xxxxxxxxxxxx\u0026#34; } data = { \u0026#34;model\u0026#34;: \u0026#34;/models/demo-model-125m\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;San Francisco is a\u0026#34;, \u0026#34;max_tokens\u0026#34;: 200, \u0026#34;temperature\u0026#34;: 0.7 } response = requests.post(url, headers=headers, json=data) print(response.json()) { \u0026#34;id\u0026#34;:\u0026#34;cmpl-50b8b30f820b418689576bc23ece3d16\u0026#34;, \u0026#34;object\u0026#34;:\u0026#34;text_completion\u0026#34;, \u0026#34;created\u0026#34;:1728471381, \u0026#34;model\u0026#34;:\u0026#34;/models/demo-model-125m\u0026#34;, \u0026#34;choices\u0026#34;:[ { \u0026#34;index\u0026#34;:0, \u0026#34;text\u0026#34;:\u0026#34; great place to live.\\nI\u0026#39;ve heard of San Francisco, but I\u0026#39;ve never been.\u0026#34;, \u0026#34;logprobs\u0026#34;:\u0026#34;None\u0026#34;, \u0026#34;finish_reason\u0026#34;:\u0026#34;stop\u0026#34;, \u0026#34;stop_reason\u0026#34;:\u0026#34;None\u0026#34;, \u0026#34;prompt_logprobs\u0026#34;:\u0026#34;None\u0026#34; } ], \u0026#34;usage\u0026#34;:{ \u0026#34;prompt_tokens\u0026#34;:5, \u0026#34;total_tokens\u0026#34;:25, \u0026#34;completion_tokens\u0026#34;:20 } } Autoscaling our vLLM endpoint #Autoscaling Managed Online Endpoint deployments in Azure Machine Learning allows us to dynamically adjust the number of instances allocated to our endpoints based on real-time metrics and schedules. This ensures that our application can handle varying loads efficiently without manual intervention. By integrating with Azure Monitor, you can set up rules to scale out when the CPU or GPU utilization exceeds a certain threshold or scale in during off-peak hours. For detailed guidance on configuring autoscaling, you can refer to the official documentation.\nSummary #In this post, we\u0026rsquo;ve discussed how to deploy vLLM models using Azure Machine Learning\u0026rsquo;s Managed Online Endpoints for efficient real-time inference. We introduced vLLM as a high-throughput, memory-efficient inference engine for LLMs, with the focus of deploying models from HuggingFace. The guide outlined the steps for creating a custom environment, defining the endpoint, and deploying a model using Azure CLI commands. We also looked at examples for testing the deployed model. Additionally, we explored how to deploy custom models while keeping them private.\n","date":"9 October 2024","permalink":"/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/","section":"Posts","summary":"","title":"Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints"},{"content":"Introduction #In this post we\u0026rsquo;ll discuss how we can use Azure API Management (APIM) to host multiple versions of the Azure OpenAI API. Over the years, Azure OpenAI has introduced several API versions, labeled stable or preview, all offering different features.\nHosting multiple versions can be useful when we deploy APIM as a smart load balancer to route requests between Provisioned Throughput (PTU) and/or multiple PAYGO endpoints. This post builds upon the learnings shared in Smart Load-Balancing for Azure OpenAI with Azure API Management.\nSolutions #To support more than one API version of Azure OpenAI in APIM we can choose between two options:\nForward the full API call to the backend - in this case, APIM just acts as an proxy Import each API version separately, thus supporting each one individually Each solution has different pros and cons and we\u0026rsquo;ll discuss these in the following sections.\nPlain call forwarding #In this scenario, we\u0026rsquo;ll use APIM as a simple proxy that forwards our full requests to the Azure OpenAI backends. All calls just get routed through, no matter if they are correct or if their requested api-version even exists.\nThis has the following advantages:\nEasy to implement Automatically supports all new API versions But it also comes with some disadvantages:\nAPIM Developer Portal will only show one single API path and does not provide details about each supported operation Developers have to look up the API spec directly in the documentation (which they probably would do anyway\u0026hellip;) To set this up, we start by creating an empty, HTTP based API and route it to /openai:\nCreate a new, empty API Before we forget, let\u0026rsquo;s directly update our authentication header name to api-key under Settings:\nauthentication header name Then we add three catch all routes for GET, POST, and DELETE (as of writing of this post, these are the only verbs that the Azure OpenAI API uses):\nAdd \u0026lsquo;catch all\u0026rsquo; routes Each of these is configured exactly the same, except for the HTTP verb.\nLastly, we deploy our APIM policy as always, a full example can be found here.\nNow we can test it by making an API call:\nfrom openai import AzureOpenAI client = AzureOpenAI( azure_endpoint=\u0026#34;https://\u0026lt;something\u0026gt;.azure-api.net/\u0026#34;, api_key=\u0026#34;\u0026lt;secret\u0026gt;\u0026#34;, api_version=\u0026#34;2024-02-01\u0026#34; ) response = client.chat.completions.create( model=\u0026#34;gpt-35-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Does Azure OpenAI support customer managed keys?\u0026#34;} ] ) print(response) Looks good:\nChatCompletion(id=\u0026#39;chatcmpl-......\u0026#39;, choices=[Choice(finish_reason=\u0026#39;stop\u0026#39;, index=0, logprobs=None, message=ChatCompletionMessage(content=\u0026#39;Yes, Azure OpenAI service does support customer managed keys. This allows customers to have more control over the encryption keys used to protect their data within the service.\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None), content_filter_results={\u0026#39;hate\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;self_harm\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;sexual\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;violence\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}})], created=1719478908, model=\u0026#39;gpt-35-turbo\u0026#39;, object=\u0026#39;chat.completion\u0026#39;, system_fingerprint=\u0026#39;fp_811936bd4f\u0026#39;, usage=CompletionUsage(completion_tokens=32, prompt_tokens=26, total_tokens=58), prompt_filter_results=[{\u0026#39;prompt_index\u0026#39;: 0, \u0026#39;content_filter_results\u0026#39;: {\u0026#39;hate\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;self_harm\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;sexual\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;violence\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}}}]) We can also try out different API version and they all should work fine - as long as they exist obviously.\nStrongly-defined API versions #In this scenario, we\u0026rsquo;re taking the Azure OpenAI Specs from Azure/azure-rest-api-specs and properly version them within APIM. This is similar to what has been discussed in Smart Load-Balancing for Azure OpenAI with Azure API Management, except here we\u0026rsquo;ll be hosting multiple versions.\nThis has the following advantages:\nStrongly-defined and full control over which versions we want to expose to developers APIM Developer Portal will be supported and will give all the details about the API operations But it also comes with some disadvantages:\nSignificantly more effort to configure (manual or for creating the automation) Need to add updated API version proactively after each release Nevertheless, let\u0026rsquo;s play through this scenario with the latest stable and preview versions (links go to the inference.json of each):\ninference/stable/2024-02-01 inference/preview/2024-05-01-preview The challenge is that the API spec has a required query parameter for api-version on each operation:\n{ \u0026#34;in\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;api-version\u0026#34;, \u0026#34;required\u0026#34;: true, \u0026#34;schema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;example\u0026#34;: \u0026#34;2024-05-01-preview\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;api version\u0026#34; } As we will use APIM to do the routing of the API version, we will need to edit all specs we want to import and make this an optional parameter throughout the whole spec:\n{ \u0026#34;in\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;api-version\u0026#34;, \u0026#34;required\u0026#34;: false, \u0026#34;schema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;example\u0026#34;: \u0026#34;2024-05-01-preview\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;api version\u0026#34; } After we\u0026rsquo;ve replaced all occurrences within both files, we\u0026rsquo;ll create a new API in APIM by importing the first OpenAPI spec - this time, in full mode so we can configure versioning:\nCreating a versioned endpoint Version identifier: 2024-02-01 (named as the Azure OpenAI version name) Versioning scheme: Query string Version query parameter: api-version Next, let\u0026rsquo;s directly update our authentication header name to api-key:\nauthentication header name So for our policy, we can re-use the same policy, but we need to re-add the api-version header to it. This is because APIM strips out the header once it routed the API request to the correct API definition.\n\u0026lt;!-- Re-add the api-version query parameter, so the backends know which version we want --\u0026gt; \u0026lt;set-query-parameter name=\u0026#34;api-version\u0026#34; exists-action=\u0026#34;override\u0026#34;\u0026gt; \u0026lt;value\u0026gt;@(context.Request.OriginalUrl.Query.GetValueOrDefault(\u0026#34;api-version\u0026#34;))\u0026lt;/value\u0026gt; \u0026lt;/set-query-parameter\u0026gt; The full policy can be found here.\nLet\u0026rsquo;s see if it works:\nfrom openai import AzureOpenAI client = AzureOpenAI( azure_endpoint=\u0026#34;https://\u0026lt;something\u0026gt;.azure-api.net/\u0026#34;, api_key=\u0026#34;\u0026lt;secret\u0026gt;\u0026#34;, api_version=\u0026#34;2024-02-01\u0026#34; ) response = client.chat.completions.create( model=\u0026#34;gpt-35-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Does Azure OpenAI support customer managed keys?\u0026#34;} ] ) print(response) Looks good:\nChatCompletion(id=\u0026#39;chatcmpl-......\u0026#39;, choices=[Choice(finish_reason=\u0026#39;stop\u0026#39;, index=0, logprobs=None, message=ChatCompletionMessage(content=\u0026#39;Yes, Azure OpenAI service does support customer managed keys. This allows customers to have more control over the encryption keys used to protect their data within the service.\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None), content_filter_results={\u0026#39;hate\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;self_harm\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;sexual\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;violence\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}})], created=1719478908, model=\u0026#39;gpt-35-turbo\u0026#39;, object=\u0026#39;chat.completion\u0026#39;, system_fingerprint=\u0026#39;fp_811936bd4f\u0026#39;, usage=CompletionUsage(completion_tokens=32, prompt_tokens=26, total_tokens=58), prompt_filter_results=[{\u0026#39;prompt_index\u0026#39;: 0, \u0026#39;content_filter_results\u0026#39;: {\u0026#39;hate\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;self_harm\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;sexual\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;violence\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}}}]) However, once we change it to a different API Version that we have yet to import, e.g., 2024-05-01-preview, we get:\nError code: 404 - {\u0026#39;statusCode\u0026#39;: 404, \u0026#39;message\u0026#39;: \u0026#39;Resource not found\u0026#39;} This is expected as our API versioning is now handled by APIM, which allows us to firmly control the versions we want to expose.\nNow, since we want to add more than one API version, e.g., also the latest preview version, we need to firstly add a new version under the API:\nSelect adding a new version Add the new version tag This version will be a clone of the original version, so we will need to overwrite its API spec by importing the new one:\nUpdate/Overwrite our existing version It is important to select Update which will fully overwrite the existing API definition.\nSince the APIM policy will be cloned (we now have two independent policies), the API should directly work. It is important that we import the edited API spec where we made the api-version query parameter optional, otherwise the APIM will throw a 404. This is because APIM won\u0026rsquo;t be able to find any matching operation that can be called without api-version as a query parameter. Recall, APIM removes the api-version query parameter once the call got routed to the correct API definition!\nLastly, we want to start using Fragments in APIM. Otherwise, whenever we need to update our policy, we would need to update the policy within each version - way too cumbersome! It\u0026rsquo;s better to have a single fragment, that is used within each policy. That allows us to just update the fragment, for e.g., adding or changing the backend endpoints or logging configuration.\nSummary #Hosting multiple API versions of Azure OpenAI behind Azure API Management is quite straight forward. Which out of the two solution we want to choose mostly depends on how much effort we want to spend on configuring and maintaining it. Using the strongly-defined approach is more work, but offers granular control over which versions a developer can use, but comes at the cost of having to update it for each new API version release. Use the simple proxy approach will cover all APIs as they get launched in Azure OpenAI, but does not give the user useful access to the APIM Developer Portal, nor does it give us control over which API versions a developer may use.\n","date":"27 June 2024","permalink":"/posts/versioning-azure-openai-endpoints-behind-api-management/","section":"Posts","summary":"","title":"Versioning Azure OpenAI Endpoints behind Azure API Management"},{"content":"Introduction #This post explains how multiple use cases can safely co-exist on Azure OpenAI\u0026rsquo;s Provisioned Throughput (PTU) offering so that we can:\nGet higher utilization of the Provisioned Throughput resource (instead of having to buy one PTU per use case) Track token consumption per use case Ensure use cases don\u0026rsquo;t step on each others\u0026rsquo; toes Prioritize certain use cases by Limiting their throughput Limiting their access to PTUs during certain times of the day or days of the week And obviously, our goal is to make this work for both streaming and non-streaming chat completion calls.\nThis post is a continuation of my previous post Smart Load-Balancing for Azure OpenAI with Azure API Management and builds upon the discussion there.\nTypical scenarios #So when planning to run multiple use cases on one PTU deployment, our main motivation is typically to save cost. At the same time, we need to make sure that the benefits of the PTU (low latency responses, high throughput) are not lost due to use cases aggressively \u0026ldquo;eating up\u0026rdquo; the whole throughput capacity (noisy neighbors).\nHence, when sharing one PTU deployment across multiple use cases we should differentiate between two scenarios:\nAll use cases have the same priority Some use cases have lower priority than others (therefore could potentially become noisy neighbors) For example, if we have two customer-facing chatbots or copilots, we want the same, quick, real-time response behavior for both of them. Both use cases have likely the same priority and therefore could run on the same PTU. In case we run out of throughput capacity, we\u0026rsquo;d surely need scale up, or in case we\u0026rsquo;re underutilized, we could add more similar use cases or scale down. Easy!\nHowever, let\u0026rsquo;s say we have one batch use case that processes a lot of documents and one customer-facing chatbot. The batch processing workload could easily eat up the whole PTU throughput capacity during main business hours, which would obviously defeat most of the value prop of the PTU. Hence, we need to be a bit more careful in this scenario.\nSo what we\u0026rsquo;ll do now is to discuss those two scenarios as they pose different architectural challenges.\nScenario 1 - Multiple use cases with the same priority #This scenario assumes that all use cases sharing the PTU have similar priority. In most cases, this means all use cases have the same quick, real-time response requirements, such as e.g., end-user facing chatbots or copilots and therefore want to use the PTU for low-latency responses. Azure OpenAI PAYGO resources should only be used to economically handle occasional peaks.\nOne way to address this scenario is through the following architecture:\nMultiple use cases with the same priority We use a single instance of API Management (APIM) with a single API endpoint. All use cases connect to that one endpoint using their own APIM subscription keys. We do this so we can later track tokens per subscription key, and therefore per use case. This can be useful for showback or chargeback within the organization. To achieve this technically, APIM is configured to emit token metrics per use case to Application Insights, where they are stored for later analysis. With a simple Log Analytics query, we can then get the consumed tokens per subscription key for any desired time frame.\nBehind APIM, we leverage multiple Azure OpenAI resources. In our example we configure:\nAn AOAI resource that hosts our PTU (priority 1) - This means all traffic should go to the PTU first, as long as there are no 429 errors. This makes sure that as many requests as possible are processed by the PTU, which enables fast responses and drives up utilization, hence saves money. An AOAI resource with PAYGO (priority 2) - This is a PAYGO deployment within the same region as the PTU, but in a separate AOAI resource. This allows us to have all model deployments be called the same, e.g. gpt-4o. In case the PTU is out of throughput, we can fall back to this PAYGO instance first. Surely, this means slower response times, but gives us a safety net and also allows us to deal with occasional peaks for which the PTU might not be economically feasible. An AOAI resource with PAYGO (priority 3) - This is another PAYGO deployment, but within a different region as the prior two resources. This is to cover the case that our PTU and the PAYGO resource (either only our resource or the whole data center) are at capacity. This is our double safety net and ideally, this resource should never get called. Lastly, we track the PTU utilization in Azure Monitor. This allows us to see if we still have good headroom or if it is potentially time to scale up our PTU deployment (i.e., buy more capacity). At the same time, this also gives us an indicator if we have too many PTUs deployed and could potentially scale down or deploy more use cases on them.\nScenario 2 - Multiple use cases with different priorities #In this scenario we assume that our use cases have different latency requirements:\nSome might require fast responses (often customer-facing) Some use cases maybe might not care too much (but won\u0026rsquo;t complain if they are fast too), and Some purely would use PTU to save money (e.g., batch workloads) Here, the core idea is to utilize the PTU as much as possible (save cost) but without creating a noisy neighbor problem. This could easily arise if a large batch workload is starting to send many API requests to the PTU, thus using up all its capacity. A latency sensitive use case then might need to revert back to PAYGO, thus suffering from slower response times. This scenario is hard to solve, as we typically do not know when the low-priority workload is starting (surely, in same cases we are in control of it).\nSolution 1 - Two PTU deployments #As said, solving this scenario perfectly is not easy. However, we can make some tradeoffs and get to a reasonably good solution using this simple architecture:\nServing multiple use cases with two PTUs Again, we use a single APIM instance but we deploy two API endpoints:\nOne endpoint for high-priority workloads One endpoint for low-priority workloads (Alternatively we could also use one single endpoint that routes based on the APIM subscription keys, but this requires a bit more coding and configuration!)\nSame as before, all use cases connect to their respective endpoints using their own APIM subscription key. Within APIM, we can limit API access for the low-priority use case to just be allowed to access the low-priority endpoint.\nNow the main idea here is that instead of using one shared PTU resource, we split it up into two AOAI resources, each holding half of the PTUs. This obviously requires twice the capacity, so for very small workloads this might not be economically feasible. However, with models becoming cheaper and faster, it is more feasible than ever. Now to architect this, we define the following access pattern:\nOur high-priority endpoint gets access both PTUs Our low-priority endpoint gets access only one PTU This effectively means that:\nOur high-priority workloads can leverage up to 100% of the overall PTU capacity, but are guaranteed at least 50% Our low-priority workloads can leverage up to 50% of the overall PTU capacity Obviously, these percentage numbers can be tweaked, as long as each PTU has the minimum amounts of PTUs required for a given model.\nBehind APIM, we configure the following Azure OpenAI resources:\nHigh-priority endpoint\nAn AOAI resource that hosts the first PTU (priority 1) An AOAI resource that hosts the second PTU (priority 1) - This should be in the same region, but requires an additional AOAI resource, so that both PTU resources can name their model deployments the same (e.g., gpt-4o) An AOAI resource with PAYGO (priority 2) - For fallback to PAYGO within the same region Low-priority endpoint\nPoints to the AOAI resource that hosts the second PTU (priority 1) Points to the AOAI resource with PAYGO (priority 2) Both endpoints should also have a secondary PAYGO resource in a different region as a secondary fall back (as discussed earlier), but for sake of simplicity this is not shown here.\nYes, this architecture is by no means perfect (requires at least 2 PTU deployments to get started), but it is easy and quick to implement, and yet flexible enough to handle growth over time.\nSolution 2 - Using a time-based approach #Often enough, our use cases show different usage patterns during the course of a day. For example, a chatbot might get used heavily from 8am to 10pm, but does not receive much traffic during the night. At the same time, batch workloads might be happy to primarily run at night, especially when they are not time-critical. We can leverage these day/night patterns to our advantage and limit access to PTUs based on time.\nTo safely enable this scenario, we can deploy the following architecture:\nServing multiple use cases with a time-based routing Again, we have two endpoints:\nOur high-priority endpoint that can access all AOAI resources at any time Our low-priority endpoint that can only access the PTU during certain times of day or days of the week With this, we ensure safety for our high-priority workload(s) if a low-priority use case needs to get a workload done during the day (will be routed to PAYGO), but at night it may use the PTU resource as much as needed. Surely, it could still fully hog it, but at least it wouldn\u0026rsquo;t happen during core business hours.\nSolution 3 - Token Limiting #One additional solution is to limit the amount of tokens a single use case can consume per minute (measured in Tokens per Minute - TPM).\nFor example, we could say:\nHigh-priority use cases: No limitations Low-priority use cases: 10k TPMs/use case Using the Azure OpenAI Capacity calculator, we can roughly estimate how much throughput in terms of TPMs we get for a given model per PTU. With this, we can make an educated decision how many TPMs we want to grand to the low-priority use cases. The architecture looks very simple:\nToken limits per subscription key However, this token limit is implemented on the APIM layer, thus it is evaluated as the requests come. This is before APIM routes them to the PTU or PAYGO backends. This means that the token limit is independent of the backends and does not exclusively restrict PTU access, but also potentially PAYGO access.\nSolution 4 - A healthy combination #Lastly, we may combine multiple approaches into one. For example we can combine a time-based approach with the multiple PTU approach:\nCombining both approaches This ensures that during business hours our PTUs can only be used by high-priority use cases. Outside of the business hours, we nevertheless offer some part of them to the low-priority use cases, thus saving cost, while still ensuring smooth operations for occasional requests that are high priority.\nFurthermore, we also could combine this with token limiting, but for sake of simplicity this is not shown here.\nImplementation #Let\u0026rsquo;s discuss how these scenarios can be implemented with APIM.\nEnable Token Tracking via Application Insights #To start tracking token consumption per inference call, we need to first add Application Insights to our APIM instance:\nAdd Application Insights to our APIM We can disable both settings, as we do not need them:\nUse as default and enable basic logging for all APIs Add availability monitor Next, we want to enable Custom Metrics in Application Insights, otherwise Application Insights won\u0026rsquo;t accept the incoming token consumption feed from APIM:\nEnable Custom Metrics in Application Insights We need to enable With Dimension so we get all the data.\nNow, we need to go to All APIs in APIM, select Settings and enable Application Insights for all requests:\nConfigure APIM to log all requests It is important to set:\nSampling to 100% (otherwise we\u0026rsquo;ll only track a fraction of the tokens) Verbosity to Information (if we only log errors, we won\u0026rsquo;t see tokens for the successfully finished calls) Next, we want to update our APIM policy to emit the token metric (paste it at the end of the \u0026lt;inbound\u0026gt; section):\n\u0026lt;azure-openai-emit-token-metric\u0026gt; \u0026lt;dimension name=\u0026#34;SubscriptionId\u0026#34; value=\u0026#34;@(context.Subscription.Id)\u0026#34; /\u0026gt; \u0026lt;/azure-openai-emit-token-metric\u0026gt; A complete policy example can be found here.\nThen, we can configure multiple subscription keys. In this case, the name will be used during logging of the tokens:\nConfiguration of multiple subscription keys Lastly, after firing a few API calls, we can go Log Analytics and query our customMetrics table:\ncustomMetrics | extend subId = tostring(parse_json(customDimensions).SubscriptionId) | summarize totalTokens = sum(valueSum), totalCalls = sum(valueCount) by name, subId If everything worked out, we should see the token consumption per subscription name:\nTokens per APIM Configure time-based routing #To configure time-based routing, we need to add a new attribute in our endpoint configuration to indicate that a certain resource (e.g., PTU) is not allowed during certain time-windows:\nbackends.Add(new JObject() { { \u0026#34;url\u0026#34;, \u0026#34;https://abcdefg.openai.azure.com/\u0026#34; }, { \u0026#34;priority\u0026#34;, 1}, { \u0026#34;isThrottling\u0026#34;, false }, { \u0026#34;retryAfter\u0026#34;, DateTime.MinValue } , { \u0026#34;hasTimeRestriction\u0026#34;, true } }); And then in our backend selection code, we check if the current time falls outside the defined window:\n... \u0026lt;set-variable name=\u0026#34;backendIndex\u0026#34; value=\u0026#34;@{ // define during which times we want the PTU to be accessible bool IsWithinAllowedHours() { TimeZoneInfo timeZoneInfo = TimeZoneInfo.FindSystemTimeZoneById(\u0026#34;W. Europe Standard Time\u0026#34;); DateTime currentTime = TimeZoneInfo.ConvertTimeFromUtc(DateTime.UtcNow, timeZoneInfo); TimeSpan startTime = new TimeSpan(8, 0, 0); TimeSpan endTime = new TimeSpan(22, 0, 0); return !(currentTime.TimeOfDay \u0026gt;= startTime \u0026amp;\u0026amp; currentTime.TimeOfDay \u0026lt;= endTime); } JArray backends = (JArray)context.Variables[\u0026#34;listBackends\u0026#34;]; bool isWithinAllowedHours = IsWithinAllowedHours(); int selectedPriority = Int32.MaxValue; List\u0026lt;int\u0026gt; availableBackends = new List\u0026lt;int\u0026gt;(); for (int i = 0; i \u0026lt; backends.Count; i++) { JObject backend = (JObject)backends[i]; if (!backend.Value\u0026lt;bool\u0026gt;(\u0026#34;isThrottling\u0026#34;)) { int backendPriority = backend.Value\u0026lt;int\u0026gt;(\u0026#34;priority\u0026#34;); bool hasTimeRestriction = backend.Value\u0026lt;bool?\u0026gt;(\u0026#34;hasTimeRestriction\u0026#34;) ?? false; // Check the time condition for the backend with time restriction if (hasTimeRestriction \u0026amp;\u0026amp; !isWithinAllowedHours) { continue; // Skip this backend if it\u0026#39;s restricted by time and we\u0026#39;re not within the allowed hours } ... A full policy example can be found here.\nConfigure token limiting #To limit the amount of tokens a use case can consume per minute, we can leverage the new azure-openai-token-limit statement in APIM. So in our \u0026lt;inbound\u0026gt; policy, we can use an choose statement to define an azure-openai-token-limit per subscription id:\n\u0026lt;choose\u0026gt; \u0026lt;when condition=\u0026#34;@(context.Subscription.Id.Equals(\u0026#34;UseCase1\u0026#34;))\u0026#34;\u0026gt; \u0026lt;azure-openai-token-limit counter-key=\u0026#34;@(context.Subscription.Id)\u0026#34; tokens-per-minute=\u0026#34;50000\u0026#34; estimate-prompt-tokens=\u0026#34;false\u0026#34; remaining-tokens-header-name=\u0026#34;x-tokens-remaining\u0026#34; tokens-consumed-header-name=\u0026#34;x-tokens-consumed\u0026#34; remaining-tokens-variable-name=\u0026#34;remainingTokens\u0026#34; /\u0026gt; \u0026lt;/when\u0026gt; \u0026lt;when condition=\u0026#34;@(context.Subscription.Id.Equals(\u0026#34;UseCase2\u0026#34;))\u0026#34;\u0026gt; \u0026lt;azure-openai-token-limit counter-key=\u0026#34;@(context.Subscription.Id)\u0026#34; tokens-per-minute=\u0026#34;10000\u0026#34; estimate-prompt-tokens=\u0026#34;false\u0026#34; remaining-tokens-header-name=\u0026#34;x-tokens-remaining\u0026#34; tokens-consumed-header-name=\u0026#34;x-tokens-consumed\u0026#34; remaining-tokens-variable-name=\u0026#34;remainingTokens\u0026#34; /\u0026gt; \u0026lt;/when\u0026gt; \u0026lt;/choose\u0026gt; In this example, use case 1 receives 50k TPMs, while use case 2 only receives 10k TPMs. If we do not specify a limit for a given subscription, it will just give it unlimited throughput. And yes, this code heavily should be made much less repetitive!\nSummary #Sharing multiple use cases on one PTU deployment is definitively possible, but requires good planning and also making a few tradeoffs. In this post we\u0026rsquo;ve went through several approaches, that together, can get us to a (hopefully) well functioning solution.\n","date":"25 June 2024","permalink":"/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/","section":"Posts","summary":"","title":"Sharing Azure OpenAI Provisioned Throughput (PTU) for multiple use cases with Azure API Management"},{"content":"Introduction #Azure OpenAI has (silently) introduced two new headers for token and request tracking (x-ratelimit-remaining-tokens and x-ratelimit-remaining-requests), which help to get a feeling of how many requests and tokens an API caller is still allowed to perform. This post explains how we can interpret them, as they are not as straight forward to understand as one might think or hope.\nNew Headers #The two new headers we\u0026rsquo;ll discuss here are returned for each API call (at least for the models that support the headers) and are the following:\nx-ratelimit-remaining-tokens - Returns how many tokens the caller can still consume without getting a 429 back x-ratelimit-remaining-requests - Returns how many inference requests the caller can still perform without getting a 429 back So let\u0026rsquo;s do a quick example:\nModel deployment:\nModel: gpt-4-turbo-2024-04-09 Configured token quota: 20k TPM (Tokens/min) Request quota: 120 RPM (Req/min) - Azure OpenAI gives 6 RPM per 1k TPM Dynamic quota: Disabled So let\u0026rsquo;s run a quick test with:\nPrompt tokens: 100 max_tokens: 25 Once the API request comes back, the usage is reported correctly (same results as when counting it beforehand using tiktoken):\n\u0026#34;usage\u0026#34;:{ \u0026#34;completion_tokens\u0026#34;: 25, \u0026#34;prompt_tokens\u0026#34;: 100, \u0026#34;total_tokens\u0026#34;: 125 } and if we look at our headers, we see the following:\nx-ratelimit-remaining-tokens: 19913 x-ratelimit-remaining-requests: 19 Tokens look kind of right (even though it should be 19875), but why is the request limit only 19? Shouldn\u0026rsquo;t we have 119 left? Let\u0026rsquo;s dive into it!\nDeep dive into x-ratelimit-remaining-tokens #To figure out how tokens are measured, let\u0026rsquo;s run a more sophisticated test with the following properties:\nModel deployment:\nModel: gpt-35-turbo-1106 Configured token quota: 10k TPM Request quota: 60 RPM Dynamic quota: Disabled Call properties:\n100 Input tokens 2000 max_tokens (the generation will stop well before, but we need to \u0026ldquo;eat\u0026rdquo; up token quota to hit the limit) In this case, we calculate the actual token cost that was deducted from our quota by subtracting it from our known quota (10k) or the last reported remaining tokens from the API response. This means we run single-threaded, no calls in parallel, and wait until each call is done. Let\u0026rsquo;s run it for a bit over a minute:\nTime 0.0s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining tokens: 7939 / Tokens used: 100+73=173 / Actual token cost: 2061 Time 1.9s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining tokens: 5878 / Tokens used: 100+74=174 / Actual token cost: 2061 Time 3.6s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining tokens: 3815 / Tokens used: 100+71=171 / Actual token cost: 2063 Time 5.4s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining tokens: 1753 / Tokens used: 100+74=174 / Actual token cost: 2062 Time 7.8s / Model gpt-35-turbo-1106 / HTTP 429 ...many more 429 errors... Time 59.6s / Model gpt-35-turbo-1106 / HTTP 429 Time 60.2s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining tokens: 1752 / Tokens used: 100+74=174 / Actual token cost: 1 Time 62.2s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining tokens: 1752 / Tokens used: 100+73=173 / Actual token cost: 0 Time 64.5s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining tokens: 1752 / Tokens used: 100+74=174 / Actual token cost: 0 Time 66.7s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining tokens: 1753 / Tokens used: 100+67=167 / Actual token cost: -1 Time 68.6s / Model gpt-35-turbo-1106 / HTTP 429 ...many more 429 errors... Firstly, we can see that we can get four initial calls through, which was expected. Each call requires 100+2000=2100 tokens in total and we need to stay below 10k tokens per minute. Furthermore we see the remaining tokens decrease until we have less available than our requested payload cost. We also clearly see that the high max_token settings removes valuable tokens from our quota, despite the response only being ~75 tokens (as shared in earlier posts, always keep max_tokens as low as possible).\nIn addition, we see that tokens start to get \u0026ldquo;refilled\u0026rdquo; exactly 60 seconds after they were consumed. They do not get refilled at once, but rather on a rolling basis (otherwise the first call after the 60 second mark would have had a cost of ~-8400). This means Azure OpenAI uses a rolling window of 60 seconds to manage the TPM quota. As an example, if we right now place a call that uses 5000 tokens, we\u0026rsquo;d get back 5000 tokens of quota 60 seconds later. This aligns with the documentation (Understanding rate limits), where it is stated that the quota management happens on 10 or 60 seconds windows (depends on the model). This approach makes sense, as it allows for large API calls to pass through.\nAzure OpenAI\u0026rsquo;s token quota refill logic However, it is unclear to me why the actual token cost is ~2061 and not 2100 and I could not figure out why this is happening. However, it always seems to be below the actual used token amount.\nDeep dive into x-ratelimit-remaining-requests #Let\u0026rsquo;s repeat the same test to understand how the remaining requests header works, but let\u0026rsquo;s reduce the TPM so we can test the behavior with fewer API calls:\nModel deployment:\nModel: gpt-35-turbo-1106 Configured token quota: 2k TPM Request quota: 12 RPM Dynamic quota: Disabled Call properties:\n100 Input tokens 10 max_tokens (to not hit any TPM quota limit) Time 0.0s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining requests: 1 / Remaining tokens: 1838 Time 2.1s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining requests: 0 / Remaining tokens: 1674 Time 3.9s / Model gpt-35-turbo-1106 / HTTP 429 ...more 429 errors... Time 10.0s / Model gpt-35-turbo-1106 / HTTP 429 Time 10.6s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining requests: 0 / Remaining tokens: 1511 Time 12.6s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining requests: 0 / Remaining tokens: 1349 Time 14.3s / Model gpt-35-turbo-1106 / HTTP 429 ...more 429 errors... Time 20.5s / Model gpt-35-turbo-1106 / HTTP 429 Time 21.1s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining requests: 0 / Remaining tokens: 1187 Time 22.9s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining requests: 0 / Remaining tokens: 1024 Time 26.4s / Model gpt-35-turbo-1106 / HTTP 429 ...more 429 errors... Time 30.7s / Model gpt-35-turbo-1106 / HTTP 429 Time 31.4s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining requests: 0 / Remaining tokens: 861 Time 33.1s / Model gpt-35-turbo-1106 / HTTP 200 / Remaining requests: 0 / Remaining tokens: 699 Time 34.8s / Model gpt-35-turbo-1106 / HTTP 429 ...more 429 errors... This looks very different from what we were expecting after the first test! Only two calls pass through - how can this be? Well, if we look at the full log, it becomes clear: the RPM limit is not enforced on a minute window, but rather on a 10 second window. This means, if we made a call right now, we\u0026rsquo;d get back a +1 \u0026ldquo;call refill\u0026rdquo; 10 seconds later. This also explains why we only get 1/6th of the full minute quota that the UI and documentation shows us. Initially, this caused a lot of confusion for me personally (\u0026ldquo;Why can\u0026rsquo;t I push more more calls through!!!\u0026rdquo;), but made sense once I understood how it works.\nAzure OpenAI\u0026rsquo;s request quota refill logic Summary #Using the x-ratelimit-remaining-tokens and x-ratelimit-remaining-requests headers in Azure OpenAI can be a useful tool to estimate how many calls we can still make before e.g., needing to switch to a different model deployment or resource. However, it is crucial to understand on which time-window those metrics operate and how they get \u0026ldquo;refilled\u0026rdquo;. Hence, in summary:\nToken quota gets refilled 60 seconds after a call has been accepted (by the amount of tokens deducted) Request quota gets refilled 10 seconds after a call has been accepted With this, it is easy to optimize throughput or run batch jobs more efficiently.\n","date":"5 June 2024","permalink":"/posts/understanding-azure-openai-x-ratelimit-remaining-tokens-x-ratelimit-remaining-requests-headers/","section":"Posts","summary":"","title":"Understanding Azure OpenAI's x-ratelimit-remaining-tokens and x-ratelimit-remaining-requests headers"},{"content":"Introduction #This post was updated on June 13, 2024 to reflect the latest Azure OpenAI updates.\nIn this post, we will guide you through the process of consolidating various Azure OpenAI resources behind a unified API endpoint using Azure API Management (APIM). While there have been numerous discussions on this topic previously, this article focuses on a more sophisticated approach. This method enables us to address the following use cases:\nAggregating multiple Azure OpenAI resources, potentially across various regions, with a preference for prioritizing in-region resources. Handling spillover from Provisioned Throughput Units in Azure OpenAI (\u0026ldquo;committed capacity\u0026rdquo;) to Pay-as-you-go in case the limits of your committed capacity are reached. A combination of both scenarios. The original idea comes from my colleague, Andre Dewes, and his initial implementation. You can find the latest code from him and a few others under: Azure-Samples/openai-apim-lb.\nTo recap, in Azure OpenAI you have the following throughput constraints:\nPay-as-you-go\nWithin each region and subscription, there are specific Tokens-per-Minute (TPM) and Requests-per-Minute (RPM) limits per model.\nThese limits can be increased by using the \u0026ldquo;Request Quota\u0026rdquo; link within the Azure OpenAI Studio.\nIf we exhaust TPMs or RPMs in a particular region and/or subscription, we will encounter 429 errors.\nAs a mitigation, we can either redirect our API calls to another region or utilize a different subscription.\nProvisioned Throughput Units (PTUs)\nIn the committed capacity scenario, we provision a set of PTUs that offer a designated throughput (dependent on our workload pattern).\nUpon reaching 100% utilization of our allocated throughput, we will encounter 429 errors.\nIn this situation, we can either provision additional PTUs or revert to Pay-as-you-go.\nTo streamline this process, we can leverage APIM and \u0026ldquo;hide\u0026rdquo; multiple Azure OpenAI resources behind it, as illustrated in this diagram:\nAPIM Load Balancing Architecture The groups of resources can contain one or more Azure OpenAI endpoints, with the key being to assign a priority to each group. This approach allows us to first utilize committed capacity, subsequently transitioning to Pay-as-you-go resources if needed. Alternatively, it offers the flexibility to prioritize resources within the region before doing cross-region API calls. In the event of all resources being exhausted, we will revert back to the initial resource and relay its response to the API caller.\nTutorial #Provision Azure API Management #Firstly, we want to provision an Azure API Management instance and ensure that we enable Managed Identity during provisioning. It\u0026rsquo;ll make most sense to put our APIM instance in the same region where our primary Azure OpenAI instance lives in.\nTo get started, use the Azure Portal to provision an API Management instance: Provision APIM instance For the pricing tier, use the tier that suits our needs, I\u0026rsquo;ve tested with Developer and Basic tiers.\nCreate your APIM instance Lastly, we shouldn\u0026rsquo;t forget to enable Managed Identity during provisioning:\nEnable Managed Identity during provisioning Provision Azure OpenAI and assign Managed Identity #Next, ensure that your Azure OpenAI resources are prepared. Since we will be directing API calls through APIM to multiple resources, it is essential to confirm that each Azure OpenAI resource has the same models (type and version) deployed, and, crucially, with identical names. For instance:\nAzure OpenAI resource 1 -\u0026gt; Model deployment with name gpt-4-8k-0613 (model type: gpt-4-8k, version: 0613) Azure OpenAI resource 2 -\u0026gt; Model deployment with name gpt-4-8k-0613 (model type: gpt-4-8k, version: 0613) I recommend adopting a uniform naming scheme for each deployment by encoding the model type and version in its name (e.g., gpt-4-turbo-1106).\nTo complete this step, we must provide API Management with access to our Azure OpenAI resources. However, for security considerations, let\u0026rsquo;s refrain from using API keys. Instead, we should add the Managed Identity of the APIM to each Azure OpenAI resource, granting it the Cognitive Services OpenAI User permission. We may perform this step while the APIM instance is still in the provisioning stage.\nSo for each Azure OpenAI Service instance, we need to add the Managed Identity of the API Management. For this, goto each Azure OpenAI instance in the Azure Portal, click Access control (IAM), click + Add, click Add role assignment, select the role Cognitive Services OpenAI User:\nSelect Cognitive Services OpenAI User role Click Next, select Managed Identity under Assign access to, then + Select Members, and select the Managed Identity of your API Management instance.\nSelect your APIM Managed Identity Import API schema into API Management #Next, we need to add Azure OpenAI\u0026rsquo;s API schema into our APIM instance. For this, download the desired API schema for Azure OpenAI Service for the schema repo. In this post, we\u0026rsquo;ll be using version 2024-05-01-preview.\nOnce downloaded, open inference.json in the editor of your choice and update the servers section:\n\u0026#34;servers\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;https://microsoft.com/openai\u0026#34;, \u0026#34;variables\u0026#34;: { \u0026#34;endpoint\u0026#34;: { \u0026#34;default\u0026#34;: \u0026#34;itdoesntmatter.openai.azure.com\u0026#34; } } } ], We won\u0026rsquo;t use this, but in order to import the file into API Management, we need to a correct URL there.\nNow, let\u0026rsquo;s go to our API Management instance in the Azure Portal, then select API on the left side, click + Add API and select OpenAPI. In the dialog, load our inference.json and make sure to set API URL suffix to openai. Then click Create.\nCreate API definition Configure API Management #Next, let\u0026rsquo;s fully configure API Management. So select the new API, goto Settings, then go to Subscription and ensure Subscription required is checked and Header name is set to api-key. This is important to ensure compatibility with the OpenAI SDK.\nSet Header name to api-key Also validate that API URL suffix is set to openai:\nValidate that API Url suffix is set to openai Now, download the apim-policy.xml from Azure-Samples/openai-apim-lb and edit the backends section as needed:\nbackends.Add(new JObject() { { \u0026#34;url\u0026#34;, \u0026#34;https://resource-sweden-1.openai.azure.com/\u0026#34; }, { \u0026#34;priority\u0026#34;, 1}, { \u0026#34;isThrottling\u0026#34;, false }, { \u0026#34;retryAfter\u0026#34;, DateTime.MinValue } }); ... Make sure you add all the Azure OpenAI instances you want to use and assign them the desired priority.\nTo put this policy into effect, go back to API Management, select Design, select All operations and click the \u0026lt;/\u0026gt; icon in inbound processing.\nSelect inbound processing Replace the code with the contents of your apim-policy.xml, then hit Save:\nSelect APIM policies Lastly, goto Subscriptions in API Management, select + Add Subscription, give it a name and scope it API and select your Azure OpenAI Service API, click Create.\nCreate new subscription keys in APIM Then get the primary subscription key via the ... on the right side, we need this for the next step:\nGet new subscription keys Lastly, we also need the endpoint URL, which we can find on the overview page:\nGet the APIM endpoint Test it #Finally, we can test if everything works by running some code of your choice, e.g., this code with OpenAI Python SDK (v1.x):\nfrom openai import AzureOpenAI client = AzureOpenAI( azure_endpoint=\u0026#34;https://\u0026lt;your APIM endpoint\u0026gt;.azure-api.net/\u0026#34;, api_key=\u0026#34;\u0026lt;your APIM subscription key\u0026gt;\u0026#34;, api_version=\u0026#34;2024-05-01-preview\u0026#34; ) response = client.chat.completions.create( model=\u0026#34;gpt-35-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Does Azure OpenAI support customer managed keys?\u0026#34;} ] ) print(response) Our response looks good:\nChatCompletion(id=\u0026#39;chatcmpl-8XnvnKVt0KFORZw5Z0T3I9Z45fpnP\u0026#39;, choices=[Choice(finish_reason=\u0026#39;stop\u0026#39;, index=0, message=ChatCompletionMessage(content=\u0026#34;Yes, Azure OpenAI offers support for customer managed keys. With customer managed keys, you can maintain control and ownership of your encryption keys used to protect your OpenAI resources and data. By managing your own keys, you have the ability to control access to your data and ensure compliance with your organization\u0026#39;s security policies.\u0026#34;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None), content_filter_results={\u0026#39;hate\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;self_harm\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;sexual\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;violence\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}})], created=1703067451, model=\u0026#39;gpt-35-turbo\u0026#39;, object=\u0026#39;chat.completion\u0026#39;, system_fingerprint=None, usage=CompletionUsage(completion_tokens=63, prompt_tokens=26, total_tokens=89), prompt_filter_results=[{\u0026#39;prompt_index\u0026#39;: 0, \u0026#39;content_filter_results\u0026#39;: {\u0026#39;hate\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;self_harm\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;sexual\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}, \u0026#39;violence\u0026#39;: {\u0026#39;filtered\u0026#39;: False, \u0026#39;severity\u0026#39;: \u0026#39;safe\u0026#39;}}}]) Summary #In this post, we\u0026rsquo;ve demonstrated a straightforward method for bundling multiple Azure OpenAI resources behind Azure API Management. This approach seamlessly facilitates spillover in scenarios where we encounter the TPM or RPM limit of a resource, or when reaching the overall throughput in Azure OpenAI\u0026rsquo;s committed capacity model. Once again, a special acknowledgment to Andrew Dewes for his original implementation: andredewes/apim-aoai-smart-loadbalancing\n","date":"20 December 2023","permalink":"/posts/smart-loadbalancing-for-azure-openai-with-api-management/","section":"Posts","summary":"","title":"Smart Load-Balancing for Azure OpenAI with Azure API Management"},{"content":"Introduction #In this article we\u0026rsquo;ll be looking into two simple and quick ways to implement a \u0026ldquo;kind-of load balancing\u0026rdquo; for Azure OpenAI using LangChain. In my last poste A Guide to Azure OpenAI Service\u0026rsquo;s Rate Limits and Monitoring we discussed how rate limits can negatively affect throughput and latency when running completions, so in this post we\u0026rsquo;ll dive deeper into two practical solutions.\nBoth approaches outlined in this post achieve similar results as using an external load balancer, but are significantly easier to implement when using LangChain. However, neither approach splits up the load between multiple instances equally (e.g., using round-robin). Instead, they shift the load to a subsequent model once the current one hits its limit.\nSolutions #To run the code examples, make sure you have the latest versions of openai and langchain installed:\npip install openai --upgrade pip install langchain --upgrade In this post, we\u0026rsquo;ll be using openai==0.27.9 and langchain==0.0.271.\nFalling back to a larger model #One simple way to deal with the token or rate limit is to use a different or larger model when this happens. The rational behind it is that larger models often give larger token limits per minute (so for example does Azure). So instead of retrying and waiting for the same model to become \u0026ldquo;useable\u0026rdquo; again, we just switch to a larger model.\nThis approach is especially useful when using gpt4 on Azure OpenAI as the default TPM (token per minute) limit for gpt4-8k is only 20k TPM. However, since we get an additional 60k TPM for gpt4-32k, why not use them? Surely, there is some additional, higher cost associated with this (the 32k model is more expensive for both prompt and completion tokens), but if throughput is the limiting factor, this is an easy fix.\nTo get this going, let\u0026rsquo;s first make sure we have both models deployed: Then create an .env with the credentials to the endpoint:\nAOAI_ENDPOINT_01=https://xxxxxxx.openai.azure.com/ AOAI_KEY_01=xxxxxx And lastly run the following Python code:\nimport os from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.prompts.chat import ( ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ) load_dotenv() kwargs = { \u0026#34;openai_api_type\u0026#34;: \u0026#34;azure\u0026#34;, \u0026#34;openai_api_base\u0026#34;: os.getenv(\u0026#34;AOAI_ENDPOINT_01\u0026#34;), \u0026#34;openai_api_key\u0026#34;: os.getenv(\u0026#34;AOAI_KEY_01\u0026#34;), \u0026#34;openai_api_version\u0026#34;: \u0026#34;2023-05-15\u0026#34;, \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: 100 } # Create a connection to gpt4-8k and gpt4-32k models llm_8k = AzureChatOpenAI(deployment_name=\u0026#34;gpt4-8k\u0026#34;, max_retries=0, **kwargs) llm_32k = AzureChatOpenAI(deployment_name=\u0026#34;gpt4-32k\u0026#34;, **kwargs) # This is where the magic happens llm = llm_8k.with_fallbacks([llm_32k]) system_message_prompt = SystemMessagePromptTemplate.from_template(\u0026#34;You are an AI assistant that tells jokes.\u0026#34;) human_message_prompt = HumanMessagePromptTemplate.from_template(\u0026#34;{text}\u0026#34;) chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt]) chain = chat_prompt | llm for i in range (1,10): result = chain.invoke({\u0026#34;text\u0026#34;: \u0026#34;Tell me a dad joke\u0026#34;}) print(result) Easy! This code will run happily and choose the 8k model, but in case it fails (we set max_retries=0), it will fall back to the 32k model and will keep retrying until it gives up (per default 6 tries). All we needed to do was create an AzureChatOpenAI for each model, and then configure the fallback. The remainder of the LangChain code stayed the same, so adding this to an existing projects is pretty easy. The other benefit is that it allows to use higher level concepts like ConversationalRetrievalChain in LangChain.\nIn summary, this approach has the following benefits:\nOnly requires one Azure OpenAI resource Great for gpt4, where the quota is lower and quota increases are harder to get Its downsides are:\nTypically more costly Limited scalability (mostly due to the fact that getting access to gpt4 can still take some time) Does not solve the noisy neighbor problem Falling back to multiple, but the same models #If we do not want to use a larger model, we can try an equally simple approach: falling back to the same model, but deployed in a different region and/or subscription.\nFor example, with gpt-35-turbo we\u0026rsquo;ll get 240k TPM per subscription and region, so we if leverage four regions at once, we get close to 1m TPM (at least theoretically)! All we need to do is create an Azure OpenAI resource in each region and deploy our gpt-35-turbo model.\nFor this, create an .env pointing to our Azure OpenAI endpoints in the different regions (could be all in the same or different subscriptions):\nAOAI_ENDPOINT_01=https://xxxxxxx.openai.azure.com/ AOAI_KEY_01=xxxxxx AOAI_ENDPOINT_02=https://xxxxxxx.openai.azure.com/ AOAI_KEY_02=xxxxxx AOAI_ENDPOINT_03=https://xxxxxxx.openai.azure.com/ AOAI_KEY_03=xxxxxx Next, let\u0026rsquo;s fire up Python:\nimport os from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.prompts.chat import ( ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ) load_dotenv() # get all credentials for connections from .env keys = [] for key, value in os.environ.items(): if key.startswith(\u0026#34;AOAI_ENDPOINT_\u0026#34;) and value is not None: num = key.split(\u0026#34;_\u0026#34;)[-1] keys.append((value, os.environ[f\u0026#34;AOAI_KEY_{num}\u0026#34;])) # Init connections to LLMs llms = [AzureChatOpenAI(openai_api_type=\u0026#34;azure\u0026#34;, openai_api_base=x[0], openai_api_key=x[1], openai_api_version=\u0026#34;2023-05-15\u0026#34;, deployment_name=\u0026#34;gpt-35-turbo\u0026#34;, temperature=0.7, max_tokens=100, max_retries=0) for x in keys] # Let\u0026#39;s keep the last LLM as the backup with retries, in case all other LLMs failed llms[-1].max_retries = 6 # Create our main LLM object that can fall back to all other LLMs llm = llms[0].with_fallbacks(llms[1:]) # Perform prompting as normal system_message_prompt = SystemMessagePromptTemplate.from_template(\u0026#34;You are an AI assistant that tells jokes.\u0026#34;) human_message_prompt = HumanMessagePromptTemplate.from_template(\u0026#34;{text}\u0026#34;) chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt]) chain = chat_prompt | llm for i in range (1,10): result = chain.invoke({\u0026#34;text\u0026#34;: \u0026#34;Tell me a dad joke\u0026#34;}) print(result) Same as before, we use the with_fallbacks() option from LangChain to fall back to the next model, then the next model, and so on. For the last model, we keep the max_retries at 6 in order to have a last resort for retries.\nIn summary, this approach has the following benefits:\nVery large scalability Might solve the noisy neighbor problem, in case other regions are less crowded (requires to set a low request_timeout) Its downsides are:\nRequires multiple Azure OpenAI resources, potentially multiple subscriptions Considerations #When using either strategy, it\u0026rsquo;s vital to determine the specific scenarios where the with_fallbacks() method comes into play. For instance, in our provided example, we default to a fallback during any error. However, in some scenarios, you might want to activate this only during token or rate limiting incidents. This customization can be done seamlessly by incorporating the exceptions_to_handle parameter.\nAnother aspect to keep an eye on is defining the request_timeout for the model. To prevent prolonged API interactions, consider an immediate transition to an alternative model after a stipulated timeout, say 10 seconds. This can be realized by initializing the AzureChatOpenAI() connections using the request_timeout=10 parameter.\nLastly, it\u0026rsquo;s always prudent to surround our Azure OpenAI interactions with a try/catch mechanism, ensuring a safe exit strategy should any unexpected issues arise.\nKey Takeaways #In this article, we\u0026rsquo;ve explored two straightforward methods to establish fallback strategies with LangChain, effectively circumventing token or rate limitations in Azure OpenAI. While these methods excel in addressing token or rate restrictions, if your goal is to delve into genuine load-balancing at the http level, I recommend reading this detailed post.\n","date":"24 August 2023","permalink":"/posts/azure_openai_load_balancing_langchain_with_fallbacks/","section":"Posts","summary":"","title":"Using LangChain with Azure OpenAI to implement load balancing via fallbacks"},{"content":"Introduction #In this post we\u0026rsquo;re looking into how Azure OpenAI Service performs rate limiting, as well as monitoring. For this, we\u0026rsquo;ll be looking at different scenarios for using gpt-35-turbo and discuss how usage can be optimized.\nRate Limiting Mechanisms #There are two principal rate limiting strategies within Azure OpenAI Service which we need to understand:\nTokens per minute (TPM) Requests per minute (RPM) Let\u0026rsquo;s delve into the details of these:\nTPMs are allocated to a model deployment (like gpt-35-turbo), defining the maximum number of tokens that can be processed per minute in an ideal scenario. TPM is measured over one-minute windows. RPMs are derived from the TPM settings and calculated as follows: 1000 TPM = 6 RPM When either the TPM or RPM limit is reached, the API begins to return 429 errors, indicating the rate limit has been exceeded.\n{ \u0026#34;error\u0026#34;:{ \u0026#34;code\u0026#34;:\u0026#34;429\u0026#34;, \u0026#34;message\u0026#34;:\u0026#34;Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 5 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\u0026#34; } } Complexities #Upon inspecting the documentation (Understanding rate limits), we quickly realize that it\u0026rsquo;s not as simple as it might seem. For instance, the explanation of the TPM rate limit might be slightly confusing:\nTPM rate limits are based on the maximum number of tokens that are estimated to be processed by a request at the time the request is received. It isn\u0026rsquo;t the same as the token count used for billing, which is computed after all processing is completed.\nSo, what does this imply? The TPM limit is estimated by Prompt text and count, max_tokens parameter setting, and best_of parameter setting. This is logical since the API needs to anticipate the total tokens of a request prior to execution. But as it can\u0026rsquo;t predict the completion length, it uses the max_tokens provided by the user. best_of acts as a multiplier if the user asks for additional completions. So, once the token limit based on these estimations is hit within a minute, the API starts returning 429.\nNow let\u0026rsquo;s focus on RPMs:\nRPM rate limits are based on the number of requests received over time. The rate limit expects that requests be evenly distributed over a one-minute period\u0026hellip;To implement this behavior, Azure OpenAI Service evaluates the rate of incoming requests over a small period of time, typically 1 or 10 seconds.\nConsider an example where we have 1000 TPMs, which equates to 6 RPM. This implies we should have one request in a 10-second window. If another request is sent within this 10-second window, a 429 error should be received.\nValidation #To validate our understanding, let\u0026rsquo;s put it to the test. I have deployed gpt-35-turbo with a limit of 1k TPM, translating to 6 RPM. I\u0026rsquo;ve chosen a low limit so that it\u0026rsquo;s easier to hit the limits (TPM or RPM) from my local machine.\nAzure OpenAI Service Model Deployment with 1k TPM Also, remember to save your ENDPOINT and KEY in a .env file:\nENDPOINT=https://xxxxx.api.cognitive.microsoft.com/ KEY=xxxxx Requests per Minute Rate Limit Test #We\u0026rsquo;ll use a basic script for calling the Azure OpenAI API, keeping the max_tokens at 1 and using a very short prompt (each call should cost less than 20 tokens).\nimport os import requests from dotenv import load_dotenv max_tokens = 1 load_dotenv() url = f\u0026#34;{os.getenv(\u0026#39;ENDPOINT\u0026#39;)}/openai/deployments/gpt-35-turbo/chat/completions?api-version=2023-05-15\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;api-key\u0026#34;: os.getenv(\u0026#34;KEY\u0026#34;) } for i in range(1000): data = {\u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;messages\u0026#34;:[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;},{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi\u0026#34;}]} try: response = requests.post(url, headers=headers, json=data) status = response.status_code if (status != 200): print(response.json()) print(f\u0026#34;Finished call with status: {status}\u0026#34;) except Exception as e: print(f\u0026#34;Call failed with exception: {e}\u0026#34;) The log output should display:\nAn API call is processed approximately every 10 seconds Roughly 6 calls are successfully completed per minute (this could be 5, depending on the exact timing) The resource\u0026rsquo;s monitoring tab in the Azure Portal confirms this behavior (\u0026ldquo;Successful calls\u0026rdquo; and \u0026ldquo;Blocked calls\u0026rdquo;):\nAzure OpenAI request per minute rate limit test Tokens per Minute Rate Limit Test #Let\u0026rsquo;s repeat the process, this time aiming to breach the TPM limit. We simply change max_tokens to 1000.\nmax_tokens = 1000 The log output will show:\nOnly one API call will be processed per minute All other calls will result in 429 errors The monitoring tab validates this:\nAzure OpenAI tokens per minute rate limit test Thus, everything operates as anticipated!\nKey Takeaways #Now that we understand how the rate limiting works in detail, what can we learn from it:\nMaintain the max_tokens parameter at the smallest feasible value while ensuring it is large enough for your requirements. Increase the quota assigned to your model or distribute the load across multiple subscriptions or regions to optimize performance (also refer to Optimizing latency for Azure OpenAI Service). Consider fallback options such as turbo-16k or gpt-4-32k when reaching the quota limits of turbo or gpt-4-8k. These alternatives have independent quota buckets within the Azure OpenAI Service. Implement retry logic into your code. This strategy is particularly beneficial when you encounter request rate limits, as these limits reset after each 10-second window. Depending on your quota, the reset time might be even faster. If you need practical advise on how to implemented the recommendation outlined here in LangChain, have a look at this post: Using LangChain with Azure OpenAI to implement load balancing via fallbacks.\nHappy \u0026ldquo;rate-limit-avoiding\u0026rdquo;!\n","date":"1 August 2023","permalink":"/posts/understanding-azure-openai-rate-limits-monitoring/","section":"Posts","summary":"","title":"A Guide to Azure OpenAI Service's Rate Limits and Monitoring"},{"content":"Introduction #In this post we\u0026rsquo;ll be looking into measuring and optimizing Azure OpenAI Service response latency by evaluating the deployed endpoints Azure OpenAI endpoints on a global scale. By optimizing latency, we can enable more real-time use cases, as well as maximize throughput for batch workloads. Our main goal in this exercise to reduce latency peaks that might show up here and there if any of the regions experiences significant load (noisy neighbors) or if we\u0026rsquo;re running into API rate limits.\nThis can be used to optimize latency for gpt-35-turbo, but can also be applied to gpt-4 model series.\nA word of caution, the solution discussed here won\u0026rsquo;t be perfect and won\u0026rsquo;t be able to avoid latency peaks completely. If you want to run latency sensitive use cases on Azure OpenAI where you can\u0026rsquo;t tolerate any peaks, I\u0026rsquo;d suggest to talk to your Microsoft Sales person regarding the Provisioned Throughput Model, which offers dedicated Azure OpenAI throughput capacity.\nIdeas for optimizing latency #If we want to build a \u0026ldquo;latency-optimized\u0026rdquo; app using Azure OpenAI, we could do the following approach:\nMeasure latency against a range of worldwide regions using a short test prompt Based on the call\u0026rsquo;s status code, latency, and rolling average latency (for instance, a decay rate of 0.8), select the fastest regions for the actual API call Execute the API calls Repeat this check at intervals between 10 and 60 seconds But what about the latency added by using an Azure region far from our application? Yes, this can cause additional latency. However, the main goal here is to prevent abrupt latency spikes. To give you some idea, here are a few quick tests:\nLatency from central Germany to canadaeast: \u0026lt;110ms Latency from central Germany to uksouth: \u0026lt;20ms Latency from central Germany to japaneast: \u0026lt;250ms Even considering a long distance, such as from the East coast to Singapore, the worst-case scenario is ~300ms of latency. However, if your app runs on Azure, you should experience significantly lower latency due to the use of the Microsoft backbone, as opposed to the public internet.\nIn context, running a prompt with 1000 input tokens and 200 completion tokens likely takes between half a second and two seconds to complete, so adding 100ms, 200ms, or 300ms doesn\u0026rsquo;t significantly impact our aim to prevent spikes.\nAccess configuration #First, let\u0026rsquo;s create an accounts.json that holds the endpoints and access keys for all the regions we want to test. In this case, I\u0026rsquo;ve just created Azure OpenAI resources in all regions where I still had capacity left:\n[ { \u0026#34;endpoint\u0026#34;: \u0026#34;https://canadaeast.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://eastus2.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://francecentral.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://japaneast.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://northcentralus.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; }, { \u0026#34;endpoint\u0026#34;: \u0026#34;https://uksouth.api.cognitive.microsoft.com/\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;...\u0026#34; } ] Testing latency via Python #To begin, install requests:\npip install requests We opt for simple HTTP requests over the openai SDK due to easier management of call timeouts and status codes.\nHere\u0026rsquo;s a sample script for the job:\nimport json import time import requests decay_rate = 0.8 http_timeout = 10 test_interval = 15 with open(\u0026#39;accounts.json\u0026#39;, \u0026#39;r\u0026#39;) as f: accounts = json.load(f) def get_latency_for_endpoint(endpoint, key): url = f\u0026#34;{endpoint}/openai/deployments/gpt-35-turbo/chat/completions?api-version=2023-05-15\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;api-key\u0026#34;: key } data = {\u0026#34;max_tokens\u0026#34;: 1, \u0026#34;messages\u0026#34;:[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;},{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi\u0026#34;}]} try: t_start = time.time() response = requests.post(url, headers=headers, json=data, timeout=http_timeout) latency = (time.time() - t_start) status = response.status_code except Exception as e: status = 500 latency = http_timeout # print(response.json()) print(f\u0026#34;Endpoint: {endpoint}, Status: {status}, Latency: {latency}s\u0026#34;) return { \u0026#34;ts\u0026#34;: time.strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, time.localtime()), \u0026#34;status\u0026#34;: status, \u0026#34;latency\u0026#34;: latency, } stat = {} for account in accounts: stat[account[\u0026#39;endpoint\u0026#39;]] = { \u0026#39;last_updated\u0026#39;: None, \u0026#39;status\u0026#39;: None, \u0026#39;latency\u0026#39;: None, \u0026#39;latency_ra\u0026#39;: 0 } while(True): for account in accounts: endpoint = account[\u0026#39;endpoint\u0026#39;] key = account[\u0026#39;key\u0026#39;] result = get_latency_for_endpoint(endpoint, key) stat[endpoint][\u0026#39;last_updated\u0026#39;] = result[\u0026#39;ts\u0026#39;] stat[endpoint][\u0026#39;status\u0026#39;] = result[\u0026#39;status\u0026#39;] stat[endpoint][\u0026#39;latency\u0026#39;] = result[\u0026#39;latency\u0026#39;] stat[endpoint][\u0026#39;latency_ra\u0026#39;] = decay_rate * result[\u0026#39;latency\u0026#39;] + (1-decay_rate) * stat[endpoint][\u0026#39;latency_ra\u0026#39;] print(json.dumps(stat,indent=4)) time.sleep(test_interval) In this script, endpoints are checked every 15 seconds, with timeouts set at 10 seconds. A rolling average with a decay rate of 0.8 is calculated.\nA response from a single prompt will look like this:\n{ \u0026#34;id\u0026#34;:\u0026#34;chatcmpl-.....\u0026#34;, \u0026#34;object\u0026#34;:\u0026#34;chat.completion\u0026#34;, \u0026#34;created\u0026#34;:1690872556, \u0026#34;model\u0026#34;:\u0026#34;gpt-35-turbo\u0026#34;, \u0026#34;choices\u0026#34;:[ { \u0026#34;index\u0026#34;:0, \u0026#34;finish_reason\u0026#34;:\u0026#34;length\u0026#34;, \u0026#34;message\u0026#34;:{ \u0026#34;role\u0026#34;:\u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;:\u0026#34;Hello\u0026#34; } } ], \u0026#34;usage\u0026#34;:{ \u0026#34;completion_tokens\u0026#34;:1, \u0026#34;prompt_tokens\u0026#34;:14, \u0026#34;total_tokens\u0026#34;:15 } } Overall cost per call is 15 tokens, which would cost us 30 days * 24 hours * 60 minutes * 4 requests/minute * 15 tokens * $0.002 / 1000 tokens = $5.2 / month per region in a month. Not sure if we need to test every every 15 seconds, or if every minute is sufficient. In terms of requests/minute, Azure OpenAI gives 1440 requests/minute per region and subscription, so sacrificing 4 calls is less than 0.3%.\nRunning the script over a period yields data such as:\n{ \u0026#34;https://canadaeast.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:25\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.5866355895996094, \u0026#34;latency_ra\u0026#34;: 0.5867746781616211 }, \u0026#34;https://eastus2.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:25\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.5309584140777588, \u0026#34;latency_ra\u0026#34;: 0.5271010751342773 }, \u0026#34;https://francecentral.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:26\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.725212812423706, \u0026#34;latency_ra\u0026#34;: 0.6279167041015624 }, \u0026#34;https://japaneast.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:27\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 1.0203375816345215, \u0026#34;latency_ra\u0026#34;: 1.0150870689697267 }, \u0026#34;https://northcentralus.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:28\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.7335877418518066, \u0026#34;latency_ra\u0026#34;: 0.7090948748168945 }, \u0026#34;https://uksouth.api.cognitive.microsoft.com/\u0026#34;: { \u0026#34;last_updated\u0026#34;: \u0026#34;2023-08-01 09:36:28\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;latency\u0026#34;: 0.2238612174987793, \u0026#34;latency_ra\u0026#34;: 0.22408719714355468 } } We can clearly see that japaneast is the slowest, but as discussed before, latency from my machine to this region is already ~250ms, which probably explains it.\nMoving forward #While the above script is functional, a practical application should account for:\nExecution: The script could be executed with a timer in an Azure Function, persisting results into Azure Blob or Azure CosmosDB. The app would then query the current status periodically, caching responses and making regional choices based on current latency and the rolling average. Rate-limiting: Azure OpenAI Service defaults to 240k tokens per minute (TPMs) for gpt-35-turbo per region and subscription (as of 08/01/2023). If the test prompt encounters a limit for a region, it will be marked with status 429. Consequently, the app should then pick the next best option. Fallback measures: In case limits are reached across most regions, ensure it\u0026rsquo;s not because of the http timeout set for the POST request. In such unlikely scenarios, temporarily increase the http timeout value to identify regions still responding. Summary #This post has presented an easy approach to measure Azure OpenAI response latency across the globe. By sending a tiny prompt, waiting for its completion, and then choosing the best-performing region, we can optimize our actual API calls and hopefully minimize latency spikes. While this will likely reduce the latency spikes you\u0026rsquo;ll see, it won\u0026rsquo;t fully eliminate them. If your workload can\u0026rsquo;t tolerate any any spikes, I\u0026rsquo;d suggest you to talk to your Microsoft Sales person regarding the Provisioned Throughput Model.\n","date":"1 August 2023","permalink":"/posts/optimizing-latency-azure-openai/","section":"Posts","summary":"","title":"Optimizing latency for Azure OpenAI Service"},{"content":"Introduction # Recently, Azure Cognitive Search introduced vector search for indexing, storing, and retrieving vector embeddings from a search index. In this post, we\u0026rsquo;ll look into how we can use this to chat with your private data, similar to ChatGPT. So besides Azure Cognitive Search we\u0026rsquo;ll be using LangChain and Azure OpenAI Service. As the underlying Large Language Model, we\u0026rsquo;ll be using gpt-3.5-turbo (the \u0026ldquo;ChatGPT\u0026rdquo; model).\nTutorial #First, create a .env and add your Azure OpenAI Service details and Azure Cognitive Search details:\nOPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_KEY=xxxxxx OPENAI_API_VERSION=2023-05-15 AZURE_COGNITIVE_SEARCH_SERVICE_NAME=https://xxxxxxx.search.windows.net AZURE_COGNITIVE_SEARCH_API_KEY=xxxxxx AZURE_COGNITIVE_SEARCH_INDEX_NAME=xxxxxx Next, make sure that you have gpt-35-turbo and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service Model Deployments Let\u0026rsquo;s install the latest versions of openai, langchain, and azure-search-documents (which is used under the hood by LangChain) via pip:\npip install openai --upgrade pip install langchain --upgrade pip install azure-search-documents --pre --upgrade For azure-search-documents, we need the preview version, as only this one includes vector search capabilities. In this post, we\u0026rsquo;re using openai==0.27.8, langchain==0.0.245, and azure-search-documents==11.4.0b6.\nOk, let\u0026rsquo;s start writing some code. First, let\u0026rsquo;s initialize our Azure OpenAI Service connection, create the LangChain objects, and create our Azure Search connection:\nimport os import openai from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import AzureSearch from langchain.document_loaders import DirectoryLoader from langchain.document_loaders import TextLoader from langchain.text_splitter import TokenTextSplitter from langchain.chains import ConversationalRetrievalChain from langchain.prompts import PromptTemplate # Load environment variables load_dotenv() # Configure OpenAI API openai.api_type = \u0026#34;azure\u0026#34; openai.api_base = os.getenv(\u0026#39;OPENAI_API_BASE\u0026#39;) openai.api_key = os.getenv(\u0026#39;OPENAI_API_KEY\u0026#39;) openai.api_version = os.getenv(\u0026#39;OPENAI_API_VERSION\u0026#39;) # Initialize gpt-35-turbo and our embedding model llm = AzureChatOpenAI(deployment_name=\u0026#34;gpt-35-turbo\u0026#34;) embeddings = OpenAIEmbeddings(deployment_id=\u0026#34;text-embedding-ada-002\u0026#34;, chunk_size=1) # Connect to Azure Cognitive Search acs = AzureSearch(azure_search_endpoint=os.getenv(\u0026#39;AZURE_COGNITIVE_SEARCH_SERVICE_NAME\u0026#39;), azure_search_key=os.getenv(\u0026#39;AZURE_COGNITIVE_SEARCH_API_KEY\u0026#39;), index_name=os.getenv(\u0026#39;AZURE_COGNITIVE_SEARCH_INDEX_NAME\u0026#39;), embedding_function=embeddings.embed_query) If the Azure Search index does not exist yet, it will be created automatically.\nNext, we can load up a bunch of text files, chunk them up and embed them. LangChain supports a lot of different document loaders, which makes it easy to adapt to other data sources and file formats. You can download the sample data here.\nloader = DirectoryLoader(\u0026#39;data/qna/\u0026#39;, glob=\u0026#34;*.txt\u0026#34;, loader_cls=TextLoader, loader_kwargs={\u0026#39;autodetect_encoding\u0026#39;: True}) documents = loader.load() text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0) docs = text_splitter.split_documents(documents) # Add documents to Azure Search acs.add_documents(documents=docs) Lastly, we can create our document question-answering chat chain. In this case, we specify the condense question prompt, which converts the user\u0026rsquo;s question to a standalone question (using the chat history), in case the user asked a follow-up question:\n# Adapt if needed CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\u0026#34;\u0026#34;\u0026#34;Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\u0026#34;\u0026#34;\u0026#34;) qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=acs.as_retriever(), condense_question_prompt=CONDENSE_QUESTION_PROMPT, return_source_documents=True, verbose=False) Let\u0026rsquo;s ask a question:\nchat_history = [] query = \u0026#34;what is Azure OpenAI Service?\u0026#34; result = qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: chat_history}) print(\u0026#34;Question:\u0026#34;, query) print(\u0026#34;Answer:\u0026#34;, result[\u0026#34;answer\u0026#34;]) From where, we can also ask follow up questions:\nchat_history = [(query, result[\u0026#34;answer\u0026#34;])] query = \u0026#34;Which regions does the service support?\u0026#34; result = qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: chat_history}) print(\u0026#34;Question:\u0026#34;, query) print(\u0026#34;Answer:\u0026#34;, result[\u0026#34;answer\u0026#34;]) This should yield the following (or similar) output (btw, this information is outdated!):\nQuestion: what is Azure OpenAI Service? Answer: Azure OpenAI Service is a service provided by Microsoft that gives users access to OpenAI\u0026#39;s language models such as GPT-3, Codex and Embeddings series. Users can access the service through REST APIs, Python SDK, or our web-based interface in the Azure OpenAI Studio. The service can be used for content generation, summarization, semantic search, and natural language to code translation. Azure OpenAI offers private networking, regional availability, and responsible AI content filtering. Access to the service is currently limited due to high demand and upcoming product improvements. Question: Which regions does the service support? Answer: Azure OpenAI Service is currently available in the East US, South Central US, and West Europe regions. Looks good! Since we use the follow-up question prompt, LangChain converts the latest question to a follow up question by resolving it via the context.\nSummary #In this blog post, we discussed how we can use Azure Cognitive Search, LangChain, and Azure OpenAI Service to build a ChatGPT-like experience, but over private data. We used embeddings and Azure Cognitive Search to enable the document retrieval step and then used the gpt-3.5-turbo model to generate an answer from the retrieved documents.\nSpecial thanks goes to Fabrizio Ruocco who pushed the PR for Azure Cognitive Search integration to LangChain!\n","date":"27 July 2023","permalink":"/posts/azure-search-vector-search-openai-langchain/","section":"Posts","summary":"","title":"Using Azure Search for vector search with Azure OpenAI and LangChain"},{"content":"Introduction #In this post we briefly discuss how LangChain and gpt-35-turbo (the model behind ChatGPT) can be used with Azure OpenAI Service.\nTutorial #First, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_VERSION=2023-05-15 Next, make sure that you have gpt-35-turbo deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service ChatGPT Model Deployment Let\u0026rsquo;s install/upgrade to the latest versions of openai and langchain via pip:\npip install openai --upgrade pip install langchain --upgrade Here, we\u0026rsquo;re using openai==0.27.8 and langchain==0.0.240. This is super important, as older versions of the openai Python SDK do not support the API version needed to access gpt-35-turbo.\nFinally, we can run our sample code:\nBy setting the openai configuration, we force LangChain (which uses the OpenAI Python SDK under the hood) to talk to Azure OpenAI instead of OpenAI directly.\nSummary #In this blog post, we discussed how we can use the ChatGPT API (gpt-35-turbo model) with Azure OpenAI Service and LangChain.\n","date":"4 May 2023","permalink":"/posts/using-langchain-and-chatgpt-turbo-with-azure-openai-service/","section":"Posts","summary":"","title":"Using LangChain and gpt-3.5-turbo (ChatGPT API) with Azure OpenAI Service"},{"content":"Introduction #In this post we briefly discuss how LlamaIndex ðŸ¦™ (GPT Index) and gpt-35-turbo (the model behind ChatGPT) can be used with Azure OpenAI Service.\nIf you want a short into to using Azure OpenAI Service with Llama-Index, have a look at this post: posts/using-gpt-index-llamaindex-with-azure-openai-service/\nTutorial #First, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_VERSION=2023-05-15 Next, make sure that you have gpt-35-turbo and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service ChatGPT Model Deployment Let\u0026rsquo;s install/upgrade to the latest versions of openai, langchain, and llama-index via pip:\npip install openai --upgrade pip install langchain --upgrade pip install llama-index --upgrade Here, we\u0026rsquo;re using openai==0.27.8, langchain==0.0.240, and llama-index==0.7.11.post1.\nSummary #In this blog post, we discussed how we can use the ChatGPT API (gpt-35-turbo model) with Azure OpenAI Service and Llama-Index.\n","date":"4 May 2023","permalink":"/posts/using-llamaindex-with-turbo-and-azure-openai-service/","section":"Posts","summary":"","title":"Using LlamaIndex and gpt-3.5-turbo (ChatGPT API) with Azure OpenAI Service"},{"content":"Introduction video #This video post is a short introduction OpenAI\u0026rsquo;s embeddings using the Azure OpenAI Service:\nCode samples #Initial Embedding Testing #Initialize text-embedding-ada-002 on Azure OpenAI Service using LangChain:\nimport os import openai from dotenv import load_dotenv from langchain.embeddings import OpenAIEmbeddings from openai.embeddings_utils import cosine_similarity # Load environment variables (contains OPENAI_API_BASE and OPENAI_API_KEY) load_dotenv() # Configure Azure OpenAI Service API openai.api_type = \u0026#34;azure\u0026#34; openai.api_version = \u0026#34;2022-12-01\u0026#34; openai.api_base = os.getenv(\u0026#39;OPENAI_API_BASE\u0026#39;) # looks like https://********.openai.azure.com/ openai.api_key = os.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) # Initialize embedding model embeddings = OpenAIEmbeddings(model=\u0026#34;text-embedding-ada-002\u0026#34;, chunk_size=1) Do some quick testing with embeddings:\ntext = \u0026#34;This is just a test\u0026#34; # Embed a single document e = embeddings.embed_query(text) print(e) print(len(e)) Try out a few more examples:\ndef get_similarity_from_text(text1, text2): e1 = embeddings.embed_query(text1) e2 = embeddings.embed_query(text2) s = cosine_similarity(e1, e2) print(f\u0026#34;Similarity between \u0026#39;{text1}\u0026#39; and \u0026#39;{text2}\u0026#39;: {s}\u0026#34;) get_similarity_from_text(\u0026#34;boy\u0026#34;, \u0026#34;girl\u0026#34;) get_similarity_from_text(\u0026#34;boy\u0026#34;, \u0026#34;man\u0026#34;) get_similarity_from_text(\u0026#34;boy\u0026#34;, \u0026#34;woman\u0026#34;) get_similarity_from_text(\u0026#34;Germany\u0026#34;, \u0026#34;Berlin\u0026#34;) get_similarity_from_text(\u0026#34;France\u0026#34;, \u0026#34;Paris\u0026#34;) get_similarity_from_text(\u0026#34;Germany\u0026#34;, \u0026#34;Paris\u0026#34;) And with longer text:\nstory1 = \u0026#34;Once upon a time, there was a little girl named Sarah. She lived with her family in a small village near the woods. Every morning Sarah would wake up early, get dressed, and go outside to play.\u0026#34; story2 = \u0026#34;One day, while Sarah was playing in the woods, she noticed a small rabbit hopping around in the grass. She decided to follow it, hoping to see where it would go. The rabbit kept hopping until it reached the entrance of a small cave.\u0026#34; insurance_clause = \u0026#34;In the event of any losses or damages incurred by either party due to unforeseen circumstances, both parties agree to be liable for their respective liabilities and hold the other harmless for any and all damages and losses sustained.\u0026#34; get_similarity_from_text(story1, story2) get_similarity_from_text(story1, insurance_clause) Movie Description Embedding - Recommendations #Load movie csv:\nimport pandas as pd df = pd.read_csv(\u0026#39;../data/movies/movies.csv\u0026#39;) # only keep original_title and overview column df = df[[\u0026#39;original_title\u0026#39;, \u0026#39;overview\u0026#39;]] # add embedding column with embedding df[\u0026#39;embedding\u0026#39;] = df[\u0026#39;overview\u0026#39;].apply(lambda x: embeddings.embed_query(x)) df.head() Calculate most similar movies (according to their description embedding):\n# Let\u0026#39;s pick a movie that exists in df, keeping in mind we only have 500 movies in it! movie = \u0026#34;Frozen\u0026#34; # get embedding for movie e = df[df[\u0026#39;original_title\u0026#39;] == movie][\u0026#39;embedding\u0026#39;].values[0] # get cosine similarity between movie and all other movies and sort ascending similarities = df[\u0026#39;embedding\u0026#39;].apply(lambda x: cosine_similarity(x, e)) # combine original_title from df and similiaries and sort ascending by similarity recommendations = pd.concat([df[\u0026#39;original_title\u0026#39;], similarities], axis=1).sort_values(by=\u0026#39;embedding\u0026#39;, ascending=False) recommendations.head() City Name Embeddings Example #Take a bunch of city names and create embeddings from them:\nimport pandas as pd # generate an array with different city names cities = [\u0026#39;Jakarta\u0026#39;, \u0026#39;Hong Kong\u0026#39;, \u0026#39;Tokyo\u0026#39;, \u0026#39;Bangkok\u0026#39;, \u0026#39;Shanghai\u0026#39;, \u0026#39;Ho Chi Minh City\u0026#39;, \u0026#39;Beijing\u0026#39;, \u0026#39;New York\u0026#39;, \u0026#39;Los Angeles\u0026#39;, \u0026#39;San Francisco\u0026#39;, \u0026#39;Chicago\u0026#39;, \u0026#39;Paris\u0026#39;, \u0026#39;Rome\u0026#39;, \u0026#39;Barcelona\u0026#39;, \u0026#39;Madrid\u0026#39;, \u0026#39;Amsterdam\u0026#39;, \u0026#39;Berlin\u0026#39;] df = pd.DataFrame({\u0026#34;city\u0026#34;: cities}) # generate embeddings for all cities df[\u0026#39;cities_embeddings\u0026#39;] = [embeddings.embed_query(city) for city in cities] Use PCA (or tSNE) to reduce the dimensionality for visualization:\n# use PCA to reduce dimensionality to 3 from sklearn.decomposition import PCA pca = PCA(n_components=3) t = pca.fit_transform(df[\u0026#39;cities_embeddings\u0026#39;].tolist()) Plot to 3d scatter plot:\n# draw t in a scatter plot and put names on each point import matplotlib.pyplot as plt fig = plt.figure(figsize=(10,10)) ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.scatter(t[:,0], t[:,1], t[:,2]) for i, txt in enumerate(cities): ax.text(t[i,0], t[i,1], t[i,2], txt) plt.show() Results:\nCities embeddings visualized ","date":"18 April 2023","permalink":"/posts/azure-openai-service-embeddings-tutorial/","section":"Posts","summary":"","title":"Embeddings Tutorial using Azure OpenAI Service"},{"content":"Introduction #In this post we discuss how we can build a system that allows you to chat with your private data, similar to ChatGPT. For this, we\u0026rsquo;ll be using LangChain, Azure OpenAI Service, and Faiss as our vector store. As the underlying Large Language Model, we\u0026rsquo;ll be using gpt-3.5-turbo (the \u0026ldquo;ChatGPT\u0026rdquo; model).\nTutorial #First, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_VERSION=2023-05-15 Next, make sure that you have gpt-35-turbo and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service Model Deployments Let\u0026rsquo;s install the latest versions of openai and langchain via pip:\npip install openai --upgrade pip install langchain --upgrade In this post, we\u0026rsquo;re using openai==0.27.8 and langchain==0.0.240.\nOk, let\u0026rsquo;s start writing some code. First, let\u0026rsquo;s initialize our Azure OpenAI Service connection and create the LangChain objects:\nimport os import openai from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.embeddings import OpenAIEmbeddings # Load environment variables (set OPENAI_API_KEY, OPENAI_API_BASE, and OPENAI_API_VERSION in .env) load_dotenv() # Configure OpenAI API openai.api_type = \u0026#34;azure\u0026#34; openai.api_base = os.getenv(\u0026#39;OPENAI_API_BASE\u0026#39;) openai.api_key = os.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) openai.api_version = os.getenv(\u0026#39;OPENAI_API_VERSION\u0026#39;) # Initialize gpt-35-turbo and our embedding model llm = AzureChatOpenAI(deployment_name=\u0026#34;gpt-35-turbo\u0026#34;, temperature=0) embeddings = OpenAIEmbeddings(deployment_id=\u0026#34;text-embedding-ada-002\u0026#34;, chunk_size=1) Next, we can load up a bunch of text files, chunk them up and embed them. LangChain supports a lot of different document loaders, which makes it easy to adapt to other data sources and file formats. You can download the sample data here.\nfrom langchain.document_loaders import DirectoryLoader from langchain.document_loaders import TextLoader from langchain.text_splitter import TokenTextSplitter loader = DirectoryLoader(\u0026#39;data/qna/\u0026#39;, glob=\u0026#34;*.txt\u0026#34;, loader_cls=TextLoader, loader_kwargs={\u0026#39;autodetect_encoding\u0026#39;: True}) documents = loader.load() text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0) docs = text_splitter.split_documents(documents) Next, let\u0026rsquo;s ingest documents into Faiss so we can efficiently query our embeddings:\nfrom langchain.vectorstores import FAISS db = FAISS.from_documents(documents=docs, embedding=embeddings) Lastly, we can create our document question-answering chat chain. In this case, we specify the question prompt, which converts the user\u0026rsquo;s question to a standalone question, in case the user asked a follow-up question:\nfrom langchain.chains import ConversationalRetrievalChain from langchain.prompts import PromptTemplate # Adapt if needed CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\u0026#34;\u0026#34;\u0026#34;Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\u0026#34;\u0026#34;\u0026#34;) qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), condense_question_prompt=CONDENSE_QUESTION_PROMPT, return_source_documents=True, verbose=False) Let\u0026rsquo;s ask a question:\nchat_history = [] query = \u0026#34;what is Azure OpenAI Service?\u0026#34; result = qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: chat_history}) print(\u0026#34;Question:\u0026#34;, query) print(\u0026#34;Answer:\u0026#34;, result[\u0026#34;answer\u0026#34;]) From where, we can also ask follow up questions:\nchat_history = [(query, result[\u0026#34;answer\u0026#34;])] query = \u0026#34;Which regions does the service support?\u0026#34; result = qa({\u0026#34;question\u0026#34;: query, \u0026#34;chat_history\u0026#34;: chat_history}) print(\u0026#34;Question:\u0026#34;, query) print(\u0026#34;Answer:\u0026#34;, result[\u0026#34;answer\u0026#34;]) This should yield the following (or similar) output:\nQuestion: what is Azure OpenAI Service? Answer: Azure OpenAI is a service that provides REST API access to OpenAI\u0026#39;s language models, including GPT-3, Codex, and Embeddings. These models can be used for content generation, summarization, semantic search, and natural language to code translation. The service can be accessed through REST APIs, Python SDK, or a web-based interface in the Azure OpenAI Studio. Azure OpenAI offers virtual network support, managed identity, and responsible AI content filtering. However, access to the service is currently limited due to high demand and Microsoft\u0026#39;s commitment to responsible AI. Question: Which regions does the service support? Answer: Azure OpenAI Service is currently available in the following regions: East US, South Central US, and West Europe. Looks good! Since we use the follow-up question prompt, LangChain converts the latest question to a follow up question, hence resolving it via the context.\nSummary #In this blog post, we discussed how we can use LangChain, Azure OpenAI Service, and Faiss to build a ChatGPT-like experience, but over private data. We used embeddings and Faiss to enable the document retrieval step and then used the gpt-3.5-turbo model to generate an answer from the retrieved documents.\n","date":"3 April 2023","permalink":"/posts/chatting-private-data-langchain-azure-openai-service/","section":"Posts","summary":"","title":"Chatting with your private data using LangChain with Azure OpenAI Service"},{"content":"Introduction #In this post we briefly discuss how LangChain can be used with Azure OpenAI Service.\nLangChain is a powerful tool for building language models that can be used for a variety of applications, from personal assistants to question answering and chatbots. Its modules provide support for different model types, prompt management, memory, indexes, chains, and agents, making it easy to customize and create unique language models. LangChain also offers guidance and assistance for use cases such as interacting with APIs, extracting structured information from text, summarization, and evaluation.\nAzure OpenAI Service is a cloud-based platform that enables you to access and use OpenAI\u0026rsquo;s powerful LLMs, such as GPT-4, GPT-3 and Codex. It offers a simple and secure way to integrate these models into your applications, with features such as authentication, encryption, scaling, monitoring, etc.\nTutorial #First, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ OPENAI_API_VERSION=2023-05-15 Next, make sure that you have text-davinci-003 and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service Model Deployments Let\u0026rsquo;s install the latest versions of openai and langchain via pip:\npip install openai --upgrade pip install langchain --upgrade Here, we\u0026rsquo;re using openai==0.27.8 and langchain==0.0.240.\nFinally, we can run our test code:\nBy setting the openai configuration, we force LangChain (which uses the OpenAI Python SDK under the hood) to talk to Azure instead of OpenAI directly. From here, we can initialize our LLM and use it. For embeddings, we need to make sure to set the chunk_size to 1, as Azure OpenAI Service API does not support embedding multiple pieces of text in one API call at once.\nSummary #In this blog post, we discussed how to use LangChain and Azure OpenAI Service together to build complex LLM-based applications with just a few lines of code. In order to use Azure OpenAI Service, we only needed a few lines of configuration for using text-davinci-003 and text-embedding-ada-002, instead of relying on models hosted on openai.com.\n","date":"27 March 2023","permalink":"/posts/using-langchain-with-azure-openai-service/","section":"Posts","summary":"","title":"Using LangChain with Azure OpenAI Service"},{"content":"Introduction #In this post we briefly discuss how LlamaIndex ðŸ¦™ (GPT Index) can be used with Azure OpenAI Service.\nLlamaIndex ðŸ¦™ (GPT Index) is a project that provides a central interface to connect your large language models (LLMs) with external data. It allows you to index your data for various LLM tasks, such as text generation, summarization, question answering, etc., and remove concerns over prompt size limitations. It also supports data connectors to your common data sources and provides cost transparency and tools that reduce cost while increasing performance.\nAzure OpenAI Service is a cloud-based platform that enables you to access and use OpenAI\u0026rsquo;s powerful LLMs, such as GPT-3 and Codex. It offers a simple and secure way to integrate these models into your applications, with features such as authentication, encryption, scaling, monitoring, etc.\nTutorial #You can find the full example in the following notebook qna-quickstart-with-llama-index.ipynb.\nFirst, create a .env and add your Azure OpenAI Service details:\nOPENAI_API_KEY=xxxxxx OPENAI_API_BASE=https://xxxxxxxx.openai.azure.com/ Next, make sure that you have text-davinci-003 and text-embedding-ada-002 deployed and used the same name as the model itself for the deployment.\nAzure OpenAI Service Model Deployments Once we\u0026rsquo;ve installed openai and llama-index via pip, we can run the following code:\nThis will initialize llama-index to use Azure OpenAI Service, by setting a custom LLMPredictor. For this code to work, we\u0026rsquo;ll need to have OPENAI_API_KEY and OPENAI_API_BASE set in our env (in this example we use dotenv). Once we have that, we use the SimpleDirectoryReader to read all text files from the data/ directory. We use GPTSimpleVectorIndex to create our index and lastly query it with a question.\nSummary #In this blog post, we discussed how to use LlamaIndex ðŸ¦™ (GPT Index) and Azure OpenAI Service together to quickly index data and perform queries on it. Luckily, we only needed a few lines of configuration over using text-davinci-003 and text-embedding-ada-002 directly from openai.com.\n","date":"9 March 2023","permalink":"/posts/using-gpt-index-llamaindex-with-azure-openai-service/","section":"Posts","summary":"","title":"Using LlamaIndex (GPT Index) with Azure OpenAI Service"},{"content":"Introduction video #This vide post shares 5 tips you can use to write better and more effective prompts in Azure OpenAI Service, OpenAI Playground and in ChatGPT:\nThe the sections below for copy/pasting the example prompts.\nExamples #Tip 1 #Classify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. Also add the revenue for 2020. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Tip 2 #JSON:\nClassify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. Also add the revenue for 2020. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Answer in JSON using the keys company, vertical, revenue. Make it an array. YAML:\nClassify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Also add the revenue for 2020. Answer as YML with keys company, vertical, revenue. Do proper newline formatting. CSV:\nClassify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Also add the revenue for 2020. Answer as CSV and print the header \u0026#34;company, vertical, revenue\u0026#34;. Markdown as table:\nClassify Microsoft, UBS, FedEx, Clemens Inc. into their verticals. If you don\u0026#39;t know certain piece of information, say \u0026#34;N/A\u0026#34;. Also add the revenue for 2020. Answer in markdown using a table. Tip 3 #From the following NDA, please extract the following information: - When the contract was signed - Who signed the contract - Address of the person signing the contract - Fine for breaching the contract Answer in JSON. Use the keys signing_date, name, address, and fine_amount. Format the signing_date as MM/DD/YYYY. NDA: On the date of August 17th, 2019, as an employee of Contoso Restaurant, I, Mateo Gomez, residing in 1234 Hollywood Boulevard Los Angeles CA, with social security number: 123-45-6789, hereby declare to fully support and promote the top priorities delegated to me at Contoso Restaurant, and vow to never disclose any information including but not limited to trade secrets, finances, delivery schedules, and recipes. If I, Mateo Gomez, accidentally or with intent breach the conditions set forth in this contract, understand fully that I shall receive a written termination to the following email address mateo@contosorestaurant.com as well as a fine of up to $10,000. JSON: Tip 4 #Content: On the date of August 17th, 2019, as an employee of Contoso Restaurant, I, Mateo Gomez, residing in 1234 Hollywood Boulevard Los Angeles CA, with social security number: 123-45-6789, hereby declare to fully support and promote the top priorities delegated to me at Contoso Restaurant, and vow to never disclose any information including but not limited to trade secrets, finances, delivery schedules, and recipes. If I, Mateo Gomez, accidentally or with intent breach the conditions set forth in this contract, understand fully that I shall receive a written termination to the following email address mateo@contosorestaurant.com as well as a fine of up to $10,000. Given the content above, please answer the following question. If you can\u0026#39;t find the answer in the content, then write \u0026#34;not found\u0026#34; Q: Was the signer of the contract married? A: Tip 5 #Extract the following information from the news article below. 1. One sentence summary in German 2. All German States News article: Microsoft hat eine starke PrÃ¤senz im Bereich der KÃ¼nstlichen Intelligenz (KI). In Deutschland hat das Unternehmen BÃ¼ros in Bayern, Baden-WÃ¼rttemberg, Berlin, NRW und anderen BundeslÃ¤ngern. Das Unternehmen hat eine Reihe von Initiativen gestartet, um KI-Technologien zu entwickeln und zu nutzen. Microsoft hat einige der leistungsstÃ¤rksten KI-Tools auf dem Markt, einschlieÃŸlich der Azure Machine Learning-Plattform, die es Entwicklern ermÃ¶glicht, Machine-Learning-Modelle zu erstellen und zu trainieren. Microsoft hat auch ein KI-Forschungszentrum erÃ¶ffnet, das sich auf die Entwicklung von KI-Technologien konzentriert. Microsoft hat auch einige Partnerschaften mit anderen Unternehmen geschlossen, um KI-Technologien zu nutzen. Beispielsweise hat Microsoft eine Partnerschaft mit dem chinesischen Unternehmen Baidu geschlossen, um KI-Technologien fÃ¼r die Entwicklung von Autonomen Fahrzeugen zu nutzen. Microsoft hat auch einige KI-Startups gegrÃ¼ndet, um neue KI-Technologien zu entwickeln und zu nutzen. Beispielsweise hat Microsoft ein Unternehmen namens Maluuba gegrÃ¼ndet, das sich auf die Entwicklung von KI-Technologien zur Verbesserung der Mensch-Maschine-Interaktion konzentriert. Microsoft hat sich zu einem der fÃ¼hrenden Unternehmen im Bereich der KÃ¼nstlichen Intelligenz entwickelt. Mit seinen verschiedenen Initiativen, Partnerschaften und Investitionen in KI-Technologien hat Microsoft gezeigt, dass es entschlossen ist, KI-Technologien voranzutreiben. ","date":"17 February 2023","permalink":"/posts/5-tips-for-writing-better-prompts-in-openai-chatgpt/","section":"Posts","summary":"","title":"5 tips for writing better prompts in Azure OpenAI Studio, OpenAI Playground and ChatGPT"},{"content":"Introduction video #This video post is a short introduction to Azure OpenAI Service and its browser-based Studio capability:\n","date":"16 February 2023","permalink":"/posts/getting-started-with-azure-openai-studio/","section":"Posts","summary":"","title":"Getting started with Azure OpenAI Studio"},{"content":"Introduction #This post explains how you can authenticate to Azure OpenAI Service by using Azure Active Directory. This enables you to use the service without any API key which has the following advantages:\nEasy on- and offboarding of new users via Azure Access Control (IAM) Avoids key leakage or key re-use in other apps Authentication for an (interactive) AAD user #First, give your user the Cognitive Services OpenAI User role in Access Control (IAM) in the Azure Portal. This role allows to use the Azure OpenAI Studio, but does not allow to deploy models and change anything. Furthermore, this role does not have permission to retrieve the access keys.\nNext, install the Azure Identity client library for Python and the OpenAI SDK:\npip install azure-identity pip install openai Then run the following Python script, but replace endpoint and deployment with your Azure OpenAI Service name/deployment. However, for this code to run you will need to be logged into the Azure CLI and selected the correct tenant/subscription.\nimport os import openai from azure.identity import AzureCliCredential, ChainedTokenCredential, ManagedIdentityCredential, EnvironmentCredential # Define strategy which potential authentication methods should be tried to gain an access token credential = ChainedTokenCredential(ManagedIdentityCredential(), EnvironmentCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/.default\u0026#34;) # Configure OpenAI SDK to use the access token openai.api_base = \u0026#34;https://\u0026lt;replace with your name\u0026gt;.openai.azure.com\u0026#34; openai.api_version = \u0026#39;2022-12-01\u0026#39; openai.api_type = \u0026#39;azure_ad\u0026#39; openai.api_key = access_token.token deployment = \u0026#34;text-davinci-003\u0026#34; # Execute completion prompt = \u0026#34;\u0026#34;\u0026#34;Write a tagline for an ice cream brand:\u0026#34;\u0026#34;\u0026#34; response = openai.Completion.create(engine=deployment, prompt=prompt, max_tokens=100) text = response[\u0026#39;choices\u0026#39;][0][\u0026#39;text\u0026#39;] print(f\u0026#34;Response was: {text}\u0026#34;) If all worked, you should see a similar response:\nResponse was: \u0026#34;Taste the Sweetness of Summer with our Creamy Ice Cream!\u0026#34; Authentication using a Service Principal (app registration) #Alternatively, you can use a Service Principal (system-user) to authenticate at the Azure OpenAI Service. In this case, create a new App Registration in your AAD. Next set the following environment variables on your console:\nAZURE_TENANT_ID=xxxx AZURE_CLIENT_ID=xxxx AZURE_CLIENT_SECRET=xxxx Then, run the same code as above - EnvironmentCredential() will pull the environment variables and use it to authenticate at the service.\nSummary #This post showed how we can use Azure Active Directory to authenticate at the Azure OpenAI Service. This allows to onboard or offboard users to the service for testing and also avoids potential key leakage. This is not only useful during development phase, but also when moving use cases to production, like e.g., the use case we discussed in this post: Using Azure OpenAI Service for processing claim calls.\n","date":"3 February 2023","permalink":"/posts/using-aad-for-azure-openai-service-authentication/","section":"Posts","summary":"","title":"Using Azure Active Directory (AAD) to authenticate with Azure OpenAI Service"},{"content":"Introduction #In this post, we\u0026rsquo;ll quickly look into how we can use Azure OpenAI Service to automatically process claim calls. For example, let\u0026rsquo;s assume we have the following call, which we transcribed from the phone conversation using Azure Speech API:\nCaller: Hi, I just had a car accident and wanted to report it. Agent: OK, I hope you\u0026#39;re alright, what happened? Caller: I was driving on the I-18 and I hit another car. Agent: Are you OK? Caller: Yeah, I\u0026#39;m just a little shaken up. Agent: That\u0026#39;s understandable. Can you give me your full name? Caller: Sure, it\u0026#39;s Sarah Standl. Agent: Do you know what caused the accident? Caller: I think I might have hit a pothole. Agent: OK, where did the accident take place? Caller: On the I-18 freeway. Agent: Agent: Was anyone else injured? Caller: I don\u0026#39;t think so. But I\u0026#39;m not sure. Agent: OK, well we\u0026#39;ll need to do an investigation. Can you give me the other drivers information? Caller: Sure, his name is John Radley. Agent: And your insurance number. Caller: OK. Give me a minute. OK, it\u0026#39;s 546452. Agent: OK, what type of damages has the car? Caller: Headlights are broken and the airbags went off. Agent: Are you going to be able to drive it? Caller: I don\u0026#39;t know. I\u0026#39;m going to have to have it towed. Agent: Well, we\u0026#39;ll need to get it inspected. I\u0026#39;ll go ahead and start the claim and we\u0026#39;ll get everything sorted out. Caller: Thank you. From this call, let\u0026rsquo;s try to extract data points like call reason, accident location, involved persons, insurance numbers, damages and a short summary. If we can do this, this would allow to automatically ingest the details into the CRM. Typically, this is often done manually by agents - however, if we can automate it we can save the agent multiple minutes of work per call.\nAzure OpenAI Service Studio Information extraction with Azure OpenAI Service #To extract all the data points, we\u0026rsquo;ll be using OpenAI\u0026rsquo;s most powerful GPT-3 model text-davinci-002 with a temperature of 0.7. We\u0026rsquo;ll give it a generous 1500 tokens (the query typically uses under 1000 tokens). For the prompt, we\u0026rsquo;ll use a zero-shot training approach by very precisely specifying what we want GPT-3 to extract:\nYou must extract the following information from the phone conversation below: 1. Call reason (key: reason) 2. Cause of the incident (key: cause) 3. Names of all drivers as an array (key: driver_names) 4. Insurance number (key: insurance_number) 5. Accident location (key: location) 6. Car damages as an array (key: damages) 7. A short, yet detailed summary (key: summary) Make sure fields 1 to 6 are answered very short, e.g. for location just say the location name. Please answer in JSON machine-readable format, using the keys from above. Pretty print the JSON and make sure that is properly closed at the end. Phone conversation: Hi I just had a car accident and wanted to report it. OK, I hope you\u0026#39;re alright, what happened? I was driving on the I-18 and I hit another car. Are you OK? Yeah, I\u0026#39;m just a little shaken up. That\u0026#39;s understandable. Can you give me your full name? Sure, it\u0026#39;s Sarah standl. Do you know what caused the accident? I think I might have hit a pothole. OK, where did the accident take place? On the I-18 freeway. Was anyone else injured? I don\u0026#39;t think so. But I\u0026#39;m not sure. OK, well we\u0026#39;ll need to do an investigation. Can you give me the other drivers information? Sure, his name is John Radley. And your insurance number. OK. Give me a minute. OK, it\u0026#39;s 546452. OK, what type of damages has the car? Headlights are broken and the airbags went off. Are you going to be able to drive it? I don\u0026#39;t know. I\u0026#39;m going to have to have it towed. Well, we\u0026#39;ll need to get it inspected. I\u0026#39;ll go ahead and start the claim and we\u0026#39;ll get everything sorted out. Thank you. Once we send the example, we will receive the following completion:\n{ \u0026#34;reason\u0026#34;: \u0026#34;To report a car accident\u0026#34;, \u0026#34;cause\u0026#34;: \u0026#34;Hitting a pothole\u0026#34;, \u0026#34;driver_names\u0026#34;: [ \u0026#34;Sarah Standl\u0026#34;, \u0026#34;John Radley\u0026#34; ], \u0026#34;insurance_number\u0026#34;: 546452, \u0026#34;location\u0026#34;: \u0026#34;I-18 freeway\u0026#34;, \u0026#34;damages\u0026#34;: [ \u0026#34;Headlights are broken\u0026#34;, \u0026#34;Airbags went off\u0026#34; ], \u0026#34;summary\u0026#34;: \u0026#34;Sarah Standl was driving on the I-18 when she hit a pothole and caused damage to her car. She exchanged insurance information with the other driver, John Radley.\u0026#34; } This looks pretty good and is easily suitable for some further downstream consumption. But let\u0026rsquo;s discuss a bit, why this actually works.\nPrompt Explanation #Firstly, it is important to tell GPT-3, what we expect it to do:\nYou must extract the following information from the phone conversation below: \u0026lt;more details on what to extract\u0026gt; Phone conversation: \u0026lt;payload\u0026gt; Instructing it to \u0026ldquo;extract information\u0026rdquo; from the \u0026ldquo;phone conversation\u0026rdquo; below helps the model to understand what it is asked to do and where it can actually find the payload on which it should operate. It is important to state where the phone conversation starts by writing \u0026ldquo;Phone conversation:\u0026rdquo;.\nNext, we specify what GPT-3 needs to extract:\n... 1. Call reason (key: reason) 2. Cause of the incident (key: cause) 3. Names of all drivers as an array (key: driver_names) 4. Insurance number (key: insurance_number) 5. Accident location (key: location) 6. Car damages as an array (key: damages) 7. A short, yet detailed summary (key: summary) ... This section is pretty straight forward, but it is important to note that since we want a machine-readable JSON as an output, it helps to specify where we expect an array and what keys we want it to use.\nLastly, we give it even more precise information on what to do:\n... Make sure fields 1 to 6 are answered very short, e.g. for location just say the location name. Please answer in JSON machine-readable format, using the keys from above. Pretty print the JSON and make sure that is properly closed at the end. ... Telling the model to be concise with the answer helps to shorten the responses and avoids that GPT-3 extracts longer chains of words for its answers. For extracting JSON, it helps to not only say that we want JSON, but also to tell it that it should use the \u0026ldquo;keys from above\u0026rdquo; and that the JSON should be properly closed. Otherwise, GPT-3 tends to sometimes forget to close the JSON document.\nNext Steps #While this works well, it might not automatically generalize well across all kinds of call center transcripts, especially for more complex ones. Therefore, two things should be considered before deploying this to production:\nA good validation strategy Using fine-tuning to further enhance extraction accuracy Validation strategy #For validating some of the extracted key-value pairs (e.g., person names, insurance number, etc.), historic data can be used to easily validate the output of the prompt. In this case, a simple string matching approach can be applied to calculate accuracy.\nHowever, for more open-ended key-value pairs (e.g., the summary) it is more challenging to automatically \u0026ldquo;judge\u0026rdquo; how accurate the field is. In this case, it might make sense to calculate the results for maybe a few hundred calls and then have them reviewed by humans. Surely, this is cost and labor-intensive, but allows for a good validation regarding if a human derives value from the summary.\nFine-tuning #One other option is to fine-tune the model. This also requires historic data, so that the prompt and the expected completion form a training dataset. From this, text-davinci-002 is fine-tuned. This helps to further improve the quality of extraction, but also helps to avoid hallucination and other challenges that GPT-3 might introduce. Surely, this is more effort but will ultimately result in a, likely, more robust model.\nSummary #This post showed how we can use Azure OpenAI Service to extract machine-readable information from unstructured call center transcripts. We used a zero-shot learning approach, that can easily extract a variety of data points and even generate a short summary of what happened in the call. This can be used to e.g., automatically populate the CRM after a call, open a ticket, file a claim, etc. For productionizing the prompt, we also discussed how this model can be evaluated and further improved by fine-tuning it.\n","date":"28 November 2022","permalink":"/posts/using-azure-openai-service-for-processing-claim-calls/","section":"Posts","summary":"","title":"Using Azure OpenAI Service for processing claim calls"},{"content":"Introduction #In this post we\u0026rsquo;ll show how you can use Azure Active Directory to authenticate against Azure Cognitive Services. This greatly improves security over using key-based authentication methods. This is because AAD authentication allows us to use Cognitive Services without any passwords or secrets, but rather relies on the identity of the user or Managed Identity (e.g., a VM). Furthermore, it helps with tightening resource access: An access key for Cognitive Services can easily be passed on to somebody else, but passing on a identity is typically not (easily) possible.\nFor this post, we\u0026rsquo;ll demonstrate how this can be done for the following three Cognitive Services:\nSpeech API Read API (via Computer Vision API) Language API However, if you want to authenticate to other Azure Cognitive Services or Azure Search have a look at this repo.\nInstructions #Let\u0026rsquo;s directly get started with provisioning some Cognitive Service resources.\nCreate resources #First, let\u0026rsquo;s create a new Computer Vision resource in the Azure Portal:\nCreate a new Computer Vision Cognitive Services resource Once done, we can repeat the same steps for \u0026ldquo;Speech\u0026rdquo; and \u0026ldquo;Language service\u0026rdquo; resources.\nAssign permissions #Once your resources have been provisioned, go into each of them, select \u0026ldquo;Access Control (IAM)\u0026rdquo; and then click \u0026ldquo;Add\u0026rdquo; to perform a \u0026ldquo;Add Role Assignment\u0026rdquo;:\nAdd new Role Assignment under Access Control In the role selection screen, we need to select Cognitive Services User. This role allows us later to access the APIs using Azure Active Directory authentication.\nSelect the \u0026quot;Cognitive Service User\u0026quot; role Under the role assignment screen, assign the role to yourself, or alternatively to a Managed Identity (e.g. the identity of a VM).\nAssign role to your own user or a Managed Identity Click through the review and assign the permission. Optionally repeat the same for your \u0026ldquo;Speech\u0026rdquo; and \u0026ldquo;Language service\u0026rdquo; resources.\nAuthenticate #We\u0026rsquo;re nearly there, we just need to retrieve the Endpoint URL for our Cognitive Service. Since we\u0026rsquo;re not authenticating with a key, the hostname is used to understand against which resource the API should go. You can find the Endpoint URL in your resource\u0026rsquo;s overview screen in the Azure Portal:\nEndpoint URL That\u0026rsquo;s it, let\u0026rsquo;s try to authenticate at the resources using just our user or a Managed Identity. For this code to run, make sure you do a pip install azure-identity first.\nimport os import time import requests from azure.identity import AzureCliCredential, ChainedTokenCredential, ManagedIdentityCredential # Update this with your computer vision endpoint endpoint = \u0026#39;https://\u0026lt;your_computer_vision_hostname\u0026gt;.cognitiveservices.azure.com\u0026#39;) # Define strategy which potential authentication methods should be tried to gain an access token credential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/\u0026#34;) headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {access_token.token}\u0026#34; } json = { \u0026#34;url\u0026#34;: \u0026#34;https://github.com/Azure-Samples/cognitive-services-python-sdk-samples/raw/master/samples/vision/images/make_things_happen.jpg\u0026#34; } params = { \u0026#34;language\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;pages\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;readingOrder\u0026#34;: \u0026#34;natural\u0026#34; } r = requests.post(url=f\u0026#34;{endpoint}/vision/v3.2/read/analyze\u0026#34;, params=params, headers=headers, json=json) print(f\u0026#34;Status code: {r.status_code}\u0026#34;) if (r.status_code == 202): # Too lazy to query status of the job, so just wait - do not use this in your code :) time.sleep(5) r = requests.get(url=r.headers[\u0026#34;Operation-Location\u0026#34;], headers=headers) print(f\u0026#34;Status code: {r.status_code}\u0026#34;) print(f\u0026#34;Response body: {r.json()}\u0026#34;) So what does this code do?\nFirst of all, our code tries to retrieve an oauth token from AAD, using either a Managed Identity (e.g., when running the code on VM) or an Azure CLI-based credential. For the latter to work, you will need to have the Azure CLI installed and also successfully logged in using az login. In this case, we rely on the new Azure Identity library which does all the heavy lifting for us. This allows us to easily request a token against the scope of Cognitive Services:\ncredential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/\u0026#34;) Now, instead of sending a key using key-based access, we can use the token in the header to authenticate:\nheaders = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {access_token.token}\u0026#34; } Using the same approach with Language Service #This approach will work for most Cognitive Services APIs. For example, we can use the same approach to authenticate to the Language service:\nimport os import time import requests from azure.identity import AzureCliCredential, ChainedTokenCredential, ManagedIdentityCredential # Update this with your language service endpoint endpoint = \u0026#39;https://\u0026lt;your_language_hostname\u0026gt;.cognitiveservices.azure.com\u0026#39; # Define strategy which potential authentication methods should be tried to gain an access token credential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/\u0026#34;) headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {access_token.token}\u0026#34; } json = { \u0026#34;analysisInput\u0026#34;: { \u0026#34;documents\u0026#34;: [ { \u0026#34;language\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Translator is a cloud-based neural machine translation service that is part of the Azure Cognitive Services family of REST APIs. Translator can be used with any operating system and powers many Microsoft products and services used by thousands of businesses worldwide to perform language translation and other language-related operations. In this overview, you\u0026#39;ll learn how Translator can enable you to build intelligent, multi-language solutions for your applications across all supported languages.\u0026#34; }, { \u0026#34;language\u0026#34;: \u0026#34;de\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Der Ãœbersetzer ist ein cloudbasierter Dienst fÃ¼r neuronale maschinelle Ãœbersetzung, der zur Azure Cognitive Services-Familie der REST-APIs gehÃ¶rt. Der Ãœbersetzer kann mit jedem Betriebssystem verwendet werden und unterstÃ¼tzt viele Microsoft-Produkte und -Dienste, die von Tausenden von Unternehmen weltweit verwendet werden, um SprachÃ¼bersetzungen und andere sprachbezogene VorgÃ¤nge durchzufÃ¼hren. In dieser Ãœbersicht erfahren Sie, wie Sie mit dem Ãœbersetzer intelligente, mehrsprachige LÃ¶sungen fÃ¼r Ihre Anwendungen in allen unterstÃ¼tzten Sprachen erstellen kÃ¶nnen.\u0026#34; } ] }, \u0026#34;tasks\u0026#34;: { \u0026#34;extractiveSummarizationTasks\u0026#34;: [ { \u0026#34;parameters\u0026#34;: { \u0026#34;model-version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;sentenceCount\u0026#34;: 2, \u0026#34;sortBy\u0026#34;: \u0026#34;Rank\u0026#34; } } ] } } r = requests.post(url=f\u0026#34;{endpoint}/text/analytics/v3.2-preview.1/analyze\u0026#34;, headers=headers, json=json) print(f\u0026#34;Status code: {r.status_code}\u0026#34;) if (r.status_code == 202): # Too lazy to query status of the job, so just wait - do not use this in your code :) time.sleep(5) r = requests.get(url=r.headers[\u0026#34;Operation-Location\u0026#34;], headers=headers) print(f\u0026#34;Status code: {r.status_code}\u0026#34;) print(f\u0026#34;Response body: {r.json()}\u0026#34;) If you want to try it out with other services, have a look at this repo.\nAuthenticating to Speech API #For accessing most (potentially all) Cognitive Services directly via their REST API, this approach will work fine. However, Speech API for example offers a Python SDK that relies on their WebSocket streaming interface, so we can\u0026rsquo;t do a plain REST-based request. In this case, the authentication is slightly different.\nFor this, first retrieve the region and the resource id of your Speech resource:\nRetrieve the resource id and region via the \u0026quot;Properties\u0026quot; tab Once retrieved, we can update the following code and use it to authenticate to the Speech API using AAD. Make sure to run pip install azure-cognitiveservices-speech first.\nimport os from azure.identity import AzureCliCredential, ChainedTokenCredential, ManagedIdentityCredential import azure.cognitiveservices.speech as speechsdk # Update with your Speech API resource id resource_id = \u0026#39;/subscriptions/\u0026lt;subscription_id\u0026gt;/resourceGroups/\u0026lt;resource_group\u0026gt;/providers/Microsoft.CognitiveServices/accounts/\u0026lt;speech_resource_name\u0026gt;\u0026#39; region = \u0026#34;westeurope\u0026#34; # update to your region credential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential()) access_token = credential.get_token(\u0026#34;https://cognitiveservices.azure.com/\u0026#34;) authorization_token = \u0026#34;aad#\u0026#34; + resource_id + \u0026#34;#\u0026#34; + access_token.token def from_mic(): speech_config = speechsdk.SpeechConfig(auth_token=authorization_token, region=region) speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config) print(\u0026#34;Say something...\u0026#34;) result = speech_recognizer.recognize_once_async().get() if result.reason == speechsdk.ResultReason.RecognizedSpeech: print(\u0026#34;Recognized: {}\u0026#34;.format(result.text)) elif result.reason == speechsdk.ResultReason.NoMatch: print(\u0026#34;No speech could be recognized: {}\u0026#34;.format(result.no_match_details)) elif result.reason == speechsdk.ResultReason.Canceled: cancellation_details = result.cancellation_details print(\u0026#34;Speech Recognition canceled: {}\u0026#34;.format(cancellation_details.reason)) if cancellation_details.reason == speechsdk.CancellationReason.Error: print(\u0026#34;Error details: {}\u0026#34;.format(cancellation_details.error_details)) from_mic() In this case, the Speech Python SDK does not seem to allow using the custom hostname, hence relies on the resource id to retrieve the correct Cognitive Service to authenticate against.\nSummary #This post showed how we can use Azure Active Directory-based authentication with Azure Cognitive Services. For this we relied on the new Azure Identity library and then used REST-based API calls to access the services. If you want to authenticate to other Azure Cognitive Services or Azure Search have a look at this repo.\nWith this approach, we can keep our code clear of any credentials and do not need to use any credentials during development either. This greatly increases security and removes unnecessary attack vectors (e.g., by using Managed Identities in production).\nIf you want to learn how to use Cognitive Services to read data that is protected by VNETs, have a look at the next post: Using Cognitive Services Read API with data secured by VNET.\n","date":"22 February 2022","permalink":"/posts/azure-active-directory-aad-authentication-for-azure-cognitive-services/","section":"Posts","summary":"","title":"Azure Active Directory (AAD) authentication for Azure Cognitive Services"},{"content":"Introduction #This post discusses how Cognitive Services can be used to process data that is securely stored behind a VNET. This allows to improve security further when processing sensitive data that is stored in Storage Accounts. In this post, we\u0026rsquo;ll look into using Read API (from the Azure Computer Vision API) to analyze documents that are heavily protected using networking rules.\nThis post also applies to other Cognitive Services, such as for example Form Recognizer or Speech API.\nData behind VNET â€“ what does it mean? #In Azure, many users protect their data using a set of security perimeters. While these are typically not all measurements users take to store their data securely, those are the most common ones that most people use:\nAuthentication layer â€“ requirement to authenticate at the storage layer, e.g., via a access key or based on an identity (could be a real user or a system managed identity) Authorization layer â€“ what is the identity allowed to do with the data (just read certain folders, write data, delete, etc.) Networking layer â€“ from where is the identity allowed to access the data (from the internet, from a range of IP addresses, only from within a VNET, from \u0026ldquo;nowhere\u0026rdquo;) For example, your Networking settings on your Storage Account might look like this: Typical Networking settings on a Storage Account In this case, the data can only be access, when the access request originates from within subnet default inside the VNET vnet-test.\nUsing these security measurements makes it incredibly hard/potentially impossible to have an attacker access the data. But this also creates a problem: what if an Azure service, like for example a Cognitive Service needs to access this data? The service might be authenticated (e.g., using a SAS URL), but from a networking perspective, the service is obviously not coming from within your VNET:\nRead API can\u0026rsquo;t access the storage In the drawing above the Read API is:\nAuthenticated to access the Storage Account and read the data (using the SAS URL from the request) Blocked by the networking rules of the Storage account If we would remove the networking rule on the storage account, the data access would obviously be allowed:\nRead API can access the storage However, this is obviously not the desired setup as the Storage Account might hold sensitive data.\nDesired state #Obviously, we want to make sure that we do everything possible to protect our data as much as we can. Furthermore, it would also be even more secure if we did not have to use a SAS URL at all. Sure, this URL might be short lived, but why not get rid of it if we can? But most importantly, we want to make sure we can use our Cognitive Services, despite the data being sitting behind a VNET.\nThe solution â€“ Managed Identity and Resource Service Instances to the rescue! #The solution to make this scenario work requires two components:\nManaged Identities Resource Service Instances Managed Identities #Most Azure services can \u0026ldquo;assume\u0026rdquo; a Managed Identity. A Managed Identity is a system user that is tied to that specific Azure resource. Since it is a \u0026ldquo;user\u0026rdquo;, the identity lives in your Azure Active Directory. That means we can assign that identity other privileges, such as access to storage or other Azure services. This means we can firstly assign our Cognitive Service a Managed Identity. Secondly, we can allow that identity to be able to read/write to our storage account using IAM. As a result, the Cognitive Service can access data without the need for access keys. Instead, it automatically requests an OAuth token from AAD and uses it to authenticate to the storage.\nResource Service Instances #However, the networking layer will still block the request and this is where Resource Service Instances come into play. With this feature we can specify a list of Azure services that are allowed to connect to the storage account regardless of its networking settings. This means, we can explicitly permit our Cognitive Service resource to \u0026ldquo;tunnel through\u0026rdquo; the firewall. Is this a security issue? No, because it is only allowed for the identity of the Cognitive Service itself. And as this identity is a system user, it is automatically secured.\nOverall Architecture #Putting both together, we get to this:\nRead API can access the storage account using its Managed Identity In this case:\nRead API uses its Managed Identity to authenticate at the Storage Account (no SAS URL required!) The Resource Service Instance configuration on the Storage Account allows the Managed Identity to \u0026ldquo;get through\u0026rdquo; the VNET-firewall Step-by-step guide #To try this out, first create a new Computer Vision API (this includes the Read API):\nCreate a new Computer Vision API During the creation, make sure to enable Managed Identity. You can also always later enable it under the Identity tab:\nManaged Identity on Computer Vision API Next, click Azure Role Assignments on the same screen and select Add role assignment. Then, assign the Storage Blob Data Reader role to your Storage Account. Once done, you could send plain storage URLS without SAS tokens to Read API and it could read the data.\nNext, navigate to your Storage Account, select Networking and check the network settings. In our example here, we only allow access from selected networks. Ironically, we did not select any VNET, so the data can\u0026rsquo;t be accessed from anywhere, including Cognitive Services. However, we\u0026rsquo;ll add the Cognitive Services Resource type and then name of our Cognitive Service instance. This means our Cognitive Service can tunnel through this super-restrictive networking setting!\nAllowing our Cognitive Service resource to tunnel through the firewall Don\u0026rsquo;t forget to hit the save button.\nTesting the whole setup #Once done, we can fire a few REST API calls to send a document to the Read API:\n# @name read_document POST https://computer-vision-demo124.cognitiveservices.azure.com/vision/v3.2/read/analyze ?language=en \u0026amp;pages=1 \u0026amp;readingOrder=natural Content-Type: application/json Ocp-Apim-Subscription-Key: \u0026lt;secret key\u0026gt; { \u0026#34;url\u0026#34;: \u0026#34;https://dgjt35hksss.blob.core.windows.net/data/description.png\u0026#34; } ##### This request queries the status of the Read API operation # @name get_results GET {{read_document.response.headers.Operation-Location}} Content-Type: application/json Ocp-Apim-Subscription-Key: \u0026lt;secret key\u0026gt; The result looks successful:\n{ \u0026#34;status\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;createdDateTime\u0026#34;: \u0026#34;2022-02-22T12:52:59Z\u0026#34;, \u0026#34;lastUpdatedDateTime\u0026#34;: \u0026#34;2022-02-22T12:53:00Z\u0026#34;, \u0026#34;analyzeResult\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;3.2.0\u0026#34;, \u0026#34;modelVersion\u0026#34;: \u0026#34;2021-04-12\u0026#34;, \u0026#34;readResults\u0026#34;: [ { \u0026#34;page\u0026#34;: 1, \u0026#34;angle\u0026#34;: -0.7095, ... Despite us just sending a regular URL (https://dgjt35hksss.blob.core.windows.net/data/description.png), the Read API can access the data:\nAuthenticated through its Managed Identity and allowed through the firewall by the Resource Instance configuration If we remove the Resource instance definition, we would get the following message, as the URL would return a 403 error to the Cognitive Service:\n{ \u0026#34;error\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;InvalidImageURL\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Failed to download the image from the submitted URL. The URL may either be invalid or the server hosting the image is experiencing some technical difficulties.\u0026#34; } } Great, looks like it is working fine now!\nBy the way, we can entirely avoid the secret to call the Cognitive Service itself. If you are interested, have a look at this blog post: Azure Active Directory (AAD) authentication for Azure Cognitive Services.\nSummary #Many users want to protect their sensitive data using as many measurements there are available on the Azure platform:\nFirstly, this will be using authentication and authorization for accessing data on storage Secondly, this will include networking rules to limit from where data can be accessed This creates a unique challenge for accessing this VNET-protected data using Cognitive Services. However, combing the usage of Managed Identities and Resource Service Instances solve the problem. They enable users to keep their data well protected, but still allows them to process it by Cognitive Services.\n","date":"22 February 2022","permalink":"/posts/using-cognitive-services-read-api-with-data-secured-by-vnet/","section":"Posts","summary":"","title":"Using Cognitive Services Read API with data secured by VNET"},{"content":"Introduction #The Azure Read API allows you to perform OCR on scanned documents of all kinds. This is great for digitalizing contracts, books, or research documents. With support for features like natural reading order support, it can be used for a wide range of tasks.\nHowever, how we can use Azure Read API for processing many documents at scale? Let\u0026rsquo;s say we need to run 100000 documents through Read API. What is the quickest way that we can do it? What image resolution should we use? What are the tradeoffs we need to consider?\nIn this post, we\u0026rsquo;ll look at all these questions and discuss how things like image resolution and parallel processing affect the overall processing time for large OCR jobs.\nAzure Read API example document Image resolution #Let\u0026rsquo;s first evaluate if the image resolution makes a difference in terms of processing time and accuracy. For comparing recognition results, we will use the Levenshtein distance to measure the distance between the ground truth of the document and the Read API results.\nImage resolution Image size Processing time Levenshtein distance 500Ã—667 100KB 1.2s 11 1000Ã—1333 300KB 1.5s 6 2000Ã—2667 1.3MB 1.7s 3 3000Ã—4000 3.1MB 1.9s 0 A few things come to mind when looking at these numbers:\nLarger image resolution gives better results. We can see that Levenshtein distance drops with larger image size, and that is typically the main thing we\u0026rsquo;ll care about for OCR: the most accuracy recognition results. Azure Read API does not charge extra for larger images, so why not leverage this? Larger image resolution only minorly affects processing time. Despite increasing the total pixel count by 35x, processing time increased only by around 1.5x. We do not know for sure, but the Read API most likely performs some image auto-scaling before processing the document. As a first learning, we should rather use high-resolution images as Read API will produce better OCR results, does not take significantly longer, and most importantly, does not cost more! Next, let\u0026rsquo;s look at optimizing the overall processing time.\nOptimizing processing time #When calling the Read API, we first have to call the analyze API and then subsequently the analyzeResults API to fetch the results. Obviously, processing will happen in between these two calls. But is there anything else? While we do not know for certain, most likely the following two steps will happen in the backend:\nUpon calling analyze, our request is first put into a queue (service-side) An idle worker will fetch our request from the queue an process it Upon calling analyzeResults, the cached results will be returned to us Again, this is speculation and might not happen exactly like this in reality, but one way or the other, it is very likely that there will be some form of queueing happening. Once unqueued, one of the many workers will process it. Having that said, what is the best strategy to use when having to run OCR on many documents?\nTo find it out, let\u0026rsquo;s compare three strategies:\nOption 1 â€“ Sequential: Process each document from start to finish in a gigantic for-loop Option 2 â€“ Optimized: First call analyze for a mini-batch of documents in a for-loop (e.g., 100), then start calling analyzeResults for the mini-batch in a for-loop Option 3 â€“ Multithreading: Run multiple threads, each thread processes one document from start to finish Intuitively, we\u0026rsquo;d suspect that option 3 would be the fastest, followed by option 2, and option 1 should be the slowest. Let\u0026rsquo;s see if this is actually true!\nFor testing this, we run the test code from a VM in Azure in the same region as the Read API endpoint. Test documents are stored on Azure Blob in the same region. As Read API is limited to 10 TPS (transactions per second) per default, we add sleep() statement the code to obey that limit. However, this limit can be increased through a support request or alternatively, we could just provision multiple Read API resources. For the multi-threaded test, we run 10 threads in parallel, each sequentially processing 10 documents using its own Cognitive Services resource (=total of 100 TPS).\nLet\u0026rsquo;s look at the results for processing 100 documents in various sizes with the three different approaches:\nProcessing times for 100 documents compared These results look not fully what we expected, so let\u0026rsquo;s discuss them:\nSame as in our prior tests, image size does not affect processing time in a significant way â€“ everything is within the same ballpark. Sequential execution is the slowest. This was expected, as waiting for each document to finish before moving on does not take advantage of the parallel backend of the Read API. Furthermore, we might get a \u0026ldquo;disadvantageous\u0026rdquo; position in the queue for each new document. Option 2 seems to be the quickest. This seems reasonable, as by starting all analyze calls in one batch, we hopefully will get approximately very similar positions in the queue. Once we start querying for the results, most of them will be finished as they have been processed in parallel by the backend. This results in the overall lowest processing time. Using a multi-threaded approach (option 3) did not perform as fast as expected. This is most likely because each thread will process documents one by one. This will result in \u0026ldquo;disadvantageous\u0026rdquo; positions in the queue, which makes it inferior to option 2. There is some variance in execution time. For option 3, we can see that one of the documents properly got a very deep position in the queue, therefore increasing overall processing time significantly. Let\u0026rsquo;s look at the average processing time per document:\nAverage processing time per image Again, these results look a bit different to what we\u0026rsquo;ve probably expected:\nThere is a large inconstancy in the average processing time. For option 1 and option 3, we should see comparable numbers, but it looks like when the tests for option 3 were executed, the queues was \u0026ldquo;more busy\u0026rdquo; than during the first test. Average processing time for option 2 is obviously much slower. This seems weird, as we expect a FIFO approach by Read API, but it could have been that a document or analyzeResult request \u0026ldquo;got stuck\u0026rdquo;, hence blocking the for-loop. When it comes to learnings, we can see that using an optimized sequential approach where we decouple the analyze and analyzeResult calls provides the overall quickest processing times.\nSummary #When using Azure Read API for processing many documents at scale (e.g., 1000\u0026rsquo;s, 10k\u0026rsquo;s or even 100k\u0026rsquo;s of documents), it makes sense to figure out a good strategy for processing them in parallel. In this post, we figured out a few highlights, so let\u0026rsquo;s summarize them:\nUse the highest available image resolution available. This will provide better recognition results, won\u0026rsquo;t cost more, and only takes slightly longer. Do not process files sequentially, as this does not take advantage of the parallel backend of the Read API. For quickest processing time, start calling the analyze API with a large mini-batch (e.g., 100 documents), then query the results using analyzeResults for the mini-batch in a for-loop. This is a simple strategy, allows for easy retry-mechanism (if needed), and you avoid the hassle of dealing with multi-threading. Leverage multiple Cognitive Services resources to get around the 10 TPS limit. Alternatively, open a support request and ask for an increased throughput limit. With this information, you should be easily able to analyze vast amounts of documents in no time! Lastly, as Form Recognizer uses Read API under the hood, we can probably use the same strategy when recognizing forms!\n","date":"17 November 2021","permalink":"/posts/azure-read-api-for-processing-many-documents-at-scale/","section":"Posts","summary":"","title":"Using Azure Read API for processing many documents at scale"},{"content":"Introduction #This post explains how we can search and analyze call center transcripts using Azure Cognitive Search and PowerBI. This can be helpful to analyze what is going on in your call center and allows to easily answer questions like:\nWhy are people calling? What are typical topics? What is the sentiment? How does sentiment and how do topics change over time? Having these answers can help optimize your call center strategy, but also potentially allows to offload easier tasks to voice-enabled bots.\nBefore we get started, all json documents and data samples can be found in this repo: csiebler/cognitive-search-call-transcripts\nCognitive Search Setup #Data pre-processing #Let\u0026rsquo;s first look at a short data sample that we\u0026rsquo;ll use for this tutorial:\nData sample from our transcripts As you can see, our input data follows a quite simple schema. However, in reality you might want to use transcripts from the Azure Speech-to-Text Batch Transcription API. However, since these transcripts are very detailed and verbose, I\u0026rsquo;d recommend to reduce them down a bit first. For example, you can run this basic scripts against a few of your sample documents: cognitive-search-call-transcripts/preprocess.py. Just put them in a folder, update the path and let the script do its work.\nOne you have the transcripts converted, upload them to a new container in an Azure Storage Account.\nIn case you do not have any documents ready to use, feel free to use my wonderful, hand-written transcripts from this GitHub repo: cognitive-search-call-transcripts/data/2021/11\nRequired Azure Resources #This tutorial assumes that you have already created a Cognitive Search instance, as well as a Storage Account and a Cognitive Services resource.\nRequired services for this tutorial Furthermore, be sure to have the following information at hand:\nThe Connection String, Name, and Access Key of your Storage Account The account key and resource id of your Cognitive Services Resource In general, I\u0026rsquo;d recommend to use Managed Identities for data access, but for the sake of this tutorial it might be easier to just rely on keys. For production, always use Managed Identities if possible!\nFurthermore, for this tutorial I suggest to use the VSCode Cognitive Search extension. This allows you to super easily create indexes, indexers, data sources, skillsets and synonym maps. Furthermore, it allows you to reset and run an indexer with a few clicks, so it makes development much faster!\nCognitive Search VSCode Extension Data Source creation #First, let\u0026rsquo;s upload your transcripts to a new container on the Storage Account. Once done, you can create a new data source in Cognitive Search. For this, update the credentials and container name in the following json document and use the VSCode Cognitive Search extension to create the data source.\nUpdate your data source settings You can find the full json here.\nIndex creation #Next, we create our index. There is not really the need to adapt anything here, unless you want to add more fields:\nIndex creation You can find the full json here.\nSkillset creation #Next, we create our skillset. You will need to add your Cognitive Services resource id and also the key. The resource id is in the following format:\n/subscriptions/\u0026lt;subscription id\u0026gt;/resourceGroups/\u0026lt;resource group\u0026gt;/providers/Microsoft.CognitiveServices/accounts/\u0026lt;cognitive service name\u0026gt; You can find it in the Azure Portal or via the CLI:\nResource ID in Azure Portal Lastly, you will also need to add your Storage Connection String, so Cognitive Search can store its projections in an Azure Table. We\u0026rsquo;ll use this table to later generate real-time dashboards using PowerBI.\nUpdate your skillset definition You can find the full json here.\nIndexer creation #Lastly, we create our indexer. Unless you added more fields to the index, you won\u0026quot;t need to touch this file.\nIndexer creation You can find the full json here.\nFinally, we can run the indexer and check if our index contains documents:\nDocuments have been indexed Perfect, that looks good! Finally, we can run queries against our search index.\nVisualization with PowerBI #Since your skillset creates a Table Projection of the data, we can point PowerBI to read from the Azure Table where the data is stored. From there, we can build a KPI dashboard for our transcripts.\nFollow this tutorial to connect PowerBI to your Azure Table with the projections. Once done, we can start building dashboards. In this case, the dashboards look a bit \u0026ldquo;lonely\u0026rdquo;, but once we\u0026rsquo;d have real and more data in Cognitive Search, we\u0026rsquo;d be able to derive a lot of insight from it!\nPowerBI dashboard example For example, we can easily build queries like:\nWhich key phrases have positive/negative sentiment associated with them? Which key phrases are most common? Are there trends in terms of sentiment or key phrases over time? \u0026hellip;and a lot more! This new insight will hopefully help to identify strong areas, as well as areas for improvement.\nSummary #In this post we showed how Azure Cognitive Search can be used to index call center transcripts (generated from Azure Speech-to-Text). We can use the solution to search for calls (e.g., educational purposes), but then also use this data to build rich KPI dashboards using PowerBI.\n","date":"16 November 2021","permalink":"/posts/analyzing-call-center-transcripts-azure-cognitive-search-powerbi/","section":"Posts","summary":"","title":"Analyzing Call Center Transcripts with Azure Cognitive Search and PowerBI"},{"content":" Introduction #Azure offers a rich set of pre-trained AI models called Cognitive Services which can help you solving a large variety of tasks. For example, services like OCR (Optical Character Recognition), form recognition or Speech-to-Text enable you to automate otherwise labor-intensive business processes.\nLet\u0026rsquo;s take invoice processing as an example. Historically, this has been performed manually and the turnaround time was probably a few hours or even a few days. It was highly asynchronous and it was \u0026ldquo;clear\u0026rdquo; that you had to wait. As we start automating this use case using e.g., Azure Form Recognizer, we can make this process not only significantly faster, but we can make it real-time (synchronous). But what does real-time really mean? 1 second? 5 seconds? 1 minute?\nIf the user is e.g., uploading a document in an app, how long can we have the user wait for the processing? I personally believe it should be either really fast (less than a few seconds), or it should be asynchronous. So if we want to make it fast, how fast can we make it? Cloud hosted APIs might show larger variance in terms of processing times, so does it make sense to self-host if supported? This could potentially give faster and more predictable response times. Let\u0026rsquo;s find out if this is really the case!\nService vs. Self-hosted #For figuring out if it really makes sense to self-host one of the Cognitive Services, we will take the Read, Form Recognizer Invoice and Speech-to-Text APIs as examples. These API are often used for automating a large variety of business processes, such as OCR, invoice processing or call center transcription. All APIs can be consumed as a service in Azure (hosted by Microsoft) or self-hosted as a Docker container (user is responsible for hosting it). So, which one is faster?\nTest Results #All the APIs we have tested are asynchronous, as they might take a few seconds to reply. Therefore, we tested with the following methodology:\nCall API to perform the task (POST) Query until task was finished successfully (GET), with a wait of 10ms between each check Wait for 1 second (probably not needed, but let\u0026rsquo;s avoid running into the rate limit at any cost) Repeat 100 times Read API #For this test, we used a 1600Ã—1200 pixel PNG image (100KB) and we ran it through the Read API for 100 times. We ran these tests from a F16sv2 instance inside Azure, which hosted the Read API container with the recommended resource configuration (8 cores, 24GB of memory). Here are the results, compared to the Azure hosted version:\nHosting Type Azure hosted Container hosted Average processing time 1.257s 1.029s Variance of processing time 0.128 0.003 Minimum processing time 0.852s 0.940s Maximum processing time 3.409s 1.187s Overall, we can see that the self-hosted version is ~20% faster and the variance is ~40x lower. We\u0026rsquo;ve ran this test once in the morning and once in the afternoon (Azure region was West Europe) and observed similar results for both tests.\nIn conclusion, for the Read API we can observe that:\nThe container hosted version is slightly faster and has lower variance, i.e. it is more deterministic in terms of processing time per document (good) At the same time, the container hosted version does not need to handle the vast request amount of requests the hosted version processes every second Hence, it does not have to deal with spikes (at least not in our test) â€“ this obviously explains the lower variance Overall, the average processing time per document is fairly similar, the min/max time are also in a similar ballpark, so there is no big advantage of using the one over the other. However, if your data can\u0026rsquo;t go to the cloud, running the container-hosted version is definitely a big plus point!\nForm Recognizer Invoice API #Next, let\u0026rsquo;s see how the Form Recognizer Invoice API compares for the hosted vs container version. For this, we used a 750Ã—1000 pixel large invoice document (100KB). We ran the required containers on a F32sv2 instance with the recommended settings of:\nLayout container â€“ 8 cores and 24GB of memory (required by the invoice container) Invoice container â€“ 8 cores and 8GB of memory Here are the results after applying the same testing methodology as for the Read API:\nHosting Type Azure hosted Container hosted Average processing time 2.721s 5.527s Variance of processing time 0.457 0.239 Minimum processing time 1.731s 5.084s Maximum processing time 5.318s 6.925s Most notably, the container-hosted version is around 2x slower than the Azure hosted. Wow! This is surprising, given the high resource requirements. Again, processing time variance is lower and minimum and maximum processing time are closer together. This aligns with the results we saw for the Read API. So in summary, unless you want to process data that is not allowed to travel to Azure, relying on the hosted version is just fine.\nSpeech-to-Text Batch Transcription API #For Speech-to-Text Batch Transcription, we\u0026rsquo;ve tested with a single Speech-to-Text container running with various settings. To enable batch transcription, we ran it alongside with the batch transcription container in daemon mode on a F16sv2. We compared this with the average processing time of the hosted batch transcription service in Azure.\nHosting Type Azure hosted Container hosted (4 cores, 4GB of memory) Container hosted (8 cores, 8GB of memory) Processing duration 2x real-time 2x real-time ~3.2x real-time Since the performance looked pretty comparable during my testing, I choose not to publish detailed results. However, especially for transcription of long audio files (e.g., call center recordings), it is important that we can either:\nscale out a self-hosted deployment for transcribing many files in parallel or increase container resources to get a x-fold real-time speed for transcription (or a combination of both) In this case, doubling the resources sped up the transcription by +50%. In this case, it would be more economical to scale out to multiple containers. Again, unless you need to rely on a self-hosted container for data privacy reasons (e.g., transcribing audio containing PII data), there is little value of using the self-hosted version.\nSummary #Overall, the primary reason why Cognitive Services exist in form of containers is to enable scenarios where data privacy is important. Those containers allow to process sensitive data on-premises or on approved cloud providers.\nHowever, our findings show that from a performance perspective the Azure hosted version performs superior and offers much easier access to scale. While the variance in processing time varies a bit, the worst and best-case processing times where all within the same ballpark. Except for extremely time sensitive applications, this probably does not justify the additional effort/cost that is required to host the containers.\n","date":"21 September 2021","permalink":"/posts/azure-cognitive-services-processing-time-comparison/","section":"Posts","summary":"","title":"Azure Cognitive Services Containers processing time comparison"},{"content":"Introduction #This is a quick post for showing how to call Azure Machine Learning Pipelines from Azure Data Factory. This includes passing data dynamically into the Machine Learning Pipeline using DataPath.\nPipeline Creation #First, let\u0026rsquo;s create an AzureML Pipeline that we can use for this example. Please note that this code is syntactically correct, but probably won\u0026rsquo;t run unless you adapt a few parameters, e.g., change the environment, adapt the data path, add a training script, etc.\nimport os import azureml.core from azureml.core import Workspace, Experiment, Dataset, RunConfiguration, Environment from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter from azureml.pipeline.steps import PythonScriptStep from azureml.data.dataset_consumption_config import DatasetConsumptionConfig from azureml.data.datapath import DataPath, DataPathComputeBinding from azureml.pipeline.core import PublishedPipeline, PipelineEndpoint # Connect to workspace ws = Workspace.from_config() # Get default datastore default_datastore = ws.get_default_datastore() # Define default DataPath for training data input and make it configurable via PipelineParameter data_path = DataPath(datastore=default_datastore, path_on_datastore=\u0026#39;training_data/\u0026#39;) datapath_parameter = PipelineParameter(name=\u0026#34;training_data_path\u0026#34;, default_value=data_path) datapath_input = (datapath_parameter, DataPathComputeBinding(mode=\u0026#39;download\u0026#39;)) # Configure runtime environment for our pipeline using AzureML Environment runconfig = RunConfiguration() runconfig.environment = Environment.get(workspace=ws, name=\u0026#39;training-env\u0026#39;) train_step = PythonScriptStep(name=\u0026#34;train-step\u0026#34;, source_directory=\u0026#34;./\u0026#34;, script_name=\u0026#39;train.py\u0026#39;, arguments=[\u0026#39;--data-path\u0026#39;, datapath_input], inputs=[datapath_input], runconfig=runconfig, compute_target=\u0026#39;cpu-cluster\u0026#39;, allow_reuse=False) steps = [train_step] # Create pipeline pipeline = Pipeline(workspace=ws, steps=steps) pipeline.validate() # Publish pipeline to AzureML published_pipeline = pipeline.publish(\u0026#39;prepare-training-pipeline-datapath\u0026#39;) # Publish pipeline to PipelineEndpoint (optional, but recommended when using the pipeline with Azure Data Factory) endpoint_name = \u0026#39;training-pipeline-endpoint\u0026#39; try: print(f\u0026#39;Pipeline Endpoint with name {endpoint_name} already exists, will add pipeline to it\u0026#39;) pipeline_endpoint = PipelineEndpoint.get(workspace=ws, name=endpoint_name) pipeline_endpoint.add_default(published_pipeline) except Exception: print(f\u0026#39;Will create Pipeline Endpoint with name {endpoint_name}\u0026#39;) pipeline_endpoint = PipelineEndpoint.publish(workspace=ws, name=endpoint_name, pipeline=published_pipeline, description=\u0026#34;New Training Pipeline Endpoint\u0026#34;) Most notably, we publish the pipeline as a PublishedPipeline and then add it to a PipelineEndpoint. A PipelineEndpoint acts as a \u0026ldquo;router\u0026rdquo; for multiple PublishedPipelines, and presents a static URL to its callers. As we re-run this code, it\u0026rsquo;ll just add our new pipeline behind the current endpoint and sets it as the new default.\nFurthermore, we are using DataPath and PipelineParameter to make the data input dynamic. DataPath allows us to specify an arbitrary path on a datastore as an input, and PipelineParameter allows to dynamically pass in the DataPath when invoking the pipeline.\nIn the next step, we\u0026rsquo;ll call the PipelineEndpoint from Azure Data Factory.\nSetup in Data Factory #In Data Factory, first create a Linked Service to your Azure Machine Learning Workspace. Then create a new Pipeline and add the Machine Learning Execute Pipeline activity.\nCreating a new ADF Pipeline Next, we can configure the Machine Learning component:\nConfiguring our ADF Pipeline From the workspace, we can first select the pipeline we would like to execute. For this, we select our newly created PiplineEndpoint as it allows swapping out the active AzureML Pipeline in the backend â€“ without touching Azure Data Factory. Under Experiment name, we pass in the name under which the pipeline should be executed in AzureML. Lastly, we need to pass in the DataPath via a Data path assignment. For this, we need to put the name of the pipeline parameter(s) for the DataPath in the big text box, then click the small down arrow left to it and add:\nDataStoreName: point to your AzureML Datastore name RelativePath: point to your path inside the Datastore In this example, training_data_path was defined in our code in line 18 (datapath_parameter = PipelineParameter(name=\u0026quot;training_data_path\u0026quot;, default_value=data_path)).\nFinally, we can publish the ADF pipeline, and run it using Add trigger, then select Trigger now. Once it ran, we should see the results in our experiment in Azure Machine Learning Studio:\nSuccessful Azure Machine Learning Pipeline run Looks good! We can see that the experiment was named properly and that the data was correctly pulled from what we set in Azure Data Factory.\nHope this quick tip was helpful!\n","date":"23 July 2021","permalink":"/posts/azure-machine-learning-pipelines-azure-data-factory-datapath/","section":"Posts","summary":"","title":"Invoking Azure Machine Learning Pipelines from Azure Data Factory using DataPath"},{"content":"Introduction #This post is a quick tip, showing how you can automatically enforce an Init Script during Azure Machine Learning Compute Instance provisioning. This ensures that even when the user did not specify a script, a default script is always being executed.\nFor more details on the init script capabilities, have a look at the documentation.\nInstructions #Firstly, create your desired init script â€“ in our example, we\u0026rsquo;ll just use a simple example:\n#!/bin/bash echo \u0026#34;Hello World\u0026#34; Then base64-encode the script and replace the value in line 26 with your encoded script:\n{ \u0026#34;mode\u0026#34;: \u0026#34;All\u0026#34;, \u0026#34;policyRule\u0026#34;: { \u0026#34;if\u0026#34;: { \u0026#34;allOf\u0026#34;: [ { \u0026#34;field\u0026#34;: \u0026#34;type\u0026#34;, \u0026#34;equals\u0026#34;: \u0026#34;Microsoft.MachineLearningServices/workspaces/computes\u0026#34; }, { \u0026#34;field\u0026#34;: \u0026#34;Microsoft.MachineLearningServices/workspaces/computes/computeType\u0026#34;, \u0026#34;in\u0026#34;: [ \u0026#34;ComputeInstance\u0026#34; ] } ] }, \u0026#34;then\u0026#34;: { \u0026#34;effect\u0026#34;: \u0026#34;append\u0026#34;, \u0026#34;details\u0026#34;: [{ \u0026#34;field\u0026#34;: \u0026#34;Microsoft.MachineLearningServices/workspaces/computes/setupScripts.scripts.creationScript.scriptSource\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;inline\u0026#34; }, { \u0026#34;field\u0026#34;: \u0026#34;Microsoft.MachineLearningServices/workspaces/computes/setupScripts.scripts.creationScript.scriptData\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IyEvYmluL2Jhc2gKCmVjaG8gIkhlbGxvIFdvcmxkIg==\u0026#34; } ] } } } Next, create a new Policy definition in Azure Policy:\nCreate a new Policy definition You\u0026rsquo;ll need to define in which subscription the definition should live in, give it a name and a description and then finally paste the policy JSON into the Policy Rule. Then hit Save.\nOur Policy definition Now that we have a Policy, we can create an assignment. This will apply the policy to Azure resources. Inside Azure Policy, navigate to Assignments and select Assign policy.\nCreating our assignment Then define the scope of the assignment (which subscriptions it should affect), select our Policy definition and hit Create.\nAsssigning our policy That\u0026rsquo;s it, our policy is live!\nMaking sure it works #Lastly, go to Azure Machine Learning Studio and provision a new Compute Instance without selecting an init script. Once the instance has been provisioned, open the instance details and you should see the stdout under the Logs tab.\nOur init script successfully executed Hope this helps!\nHappy instance creating!\n","date":"19 July 2021","permalink":"/posts/enforcing-init-scripts-on-azure-machine-learning-compute-instances/","section":"Posts","summary":"","title":"Enforcing Init Scripts on Azure Machine Learning Compute Instances"},{"content":"Introduction #This post will explain deploying Azure Machine Learning models to Azure App Service. This allows for easier model deployments, especially for those users who do not want to deploy to e.g., Azure Kubernetes Service. In this post, we will follow the proposed approach from the official documentation to a certain degree, but we will also add model telemetry/metrics logging and Model Data Collection. This allows for a \u0026lsquo;more complete\u0026rsquo; approach of model deployment with model monitoring over just running a plain Docker container on App Service.\nOverall, this example can be reused to deploy the Docker images generated by Azure Machine Learning to any platform that is capable of running Docker images.\nDeployment Architecture If we look at the architecture diagram above, we\u0026rsquo;ll focus on the following steps in this post:\nPackaging the model as a Docker image Deploying the image to App Service Adding model telemetry logging to Application Insights Adding model data collection to Azure Blob Consuming the model using its exposed API Getting Started #To get started, we assume that we already have:\na registered model in Azure Machine Learning a score.py scoring script with model data collection enabled (example: score.py) a conda.yml with your model dependencies (example: conda.yml) In short, you already have taken the steps to train and deploy a model. Packaging our model for deployment #First, we want to package our existing model using the registered model, our scoring script and our Conda environment:\nfrom azureml.core import Workspace, Model from azureml.core.model import InferenceConfig from azureml.core.environment import Environment from azureml.core.conda_dependencies import CondaDependencies ws = Workspace.from_config() env = Environment(\u0026#34;inference-env\u0026#34;) env.docker.enabled = True # Replace with your conda enviroment file env.python.conda_dependencies = CondaDependencies(\u0026#34;./conda.yml\u0026#34;) # Replace with your score.py inference_config = InferenceConfig(entry_script=\u0026#34;score.py\u0026#34;, environment=env) # Replace with your model model = Model(ws, \u0026#39;my-model\u0026#39;) package = Model.package(ws, [model], inference_config) package.wait_for_creation(show_output=True) print(f\u0026#34;Packaged model image: {package.location}\u0026#34;) The code will return the URL to the new Docker image:\nPackaged model image: amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx Now that we have the model image built, we can start deploying it to a Docker runtime. If you need a more customized Docker image (e.g., maybe you are required to add a few more complex dependencies), you can follow the tutorial from the last post on building custom Docker images.\nRunning the model locally #Next, we can try running the model locally on our laptop, Compute Instance or where ever you have Docker running. For this, we log in to the Azure Container Registry where our model image has been stored:\ndocker login amldemowexxxxxx.azurecr.io You can easily retrieve the login credentials for the Container Registry through the Azure Portal:\nContainer Registry details From here, we can run the image via Docker by forwarding web service port 5001 to the host:\ndocker run -it --rm \\ -p 5001:5001 \\ amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx And then quickly test if we can call the model successfully:\nimport json, requests test_sample = json.dumps({ \u0026#39;data\u0026#39;: [{ \u0026#34;Age\u0026#34;: 20, \u0026#34;Sex\u0026#34;: \u0026#34;male\u0026#34;, \u0026#34;Job\u0026#34;: 0, \u0026#34;Housing\u0026#34;: \u0026#34;own\u0026#34;, \u0026#34;Saving accounts\u0026#34;: \u0026#34;little\u0026#34;, \u0026#34;Checking account\u0026#34;: \u0026#34;little\u0026#34;, \u0026#34;Credit amount\u0026#34;: 100, \u0026#34;Duration\u0026#34;: 48, \u0026#34;Purpose\u0026#34;: \u0026#34;radio/TV\u0026#34; }] }) url = \u0026#34;http://localhost:5001/score\u0026#34; headers = {\u0026#39;Content-Type\u0026#39;:\u0026#39;application/json\u0026#39;} response = requests.post(url, test_sample, headers=headers) print(response.status_code) print(response.text) Which in our case here, returns the HTTP response code and the mode\u0026rsquo;s predictions:\n200 {\u0026#34;predict_proba\u0026#34;: [[0.6900664207902661, 0.30993357920973386]]} Perfect, our model is up and running, next we\u0026rsquo;ll add some telemetry logging to Application Insights.\nAdding model telemetry logging #Next, we can add model telemetry logging to Application Insights. We can achieve this by setting the appropriate logging-related environment variables in our Docker command:\nAML_APP_INSIGHTS_ENABLED=true AML_APP_INSIGHTS_KEY=\u0026lt;Instrumentation key\u0026gt; WORKSPACE_NAME=\u0026lt;Name of your Workspace\u0026gt; SERVICE_NAME=\u0026lt;arbitrary service name, e.g. deployment name or build Id\u0026gt; You can retrieve the Instrumentation key for Application Insights from the Azure Portal:\nRetrieving our Application Insights key Adding those to our Docker command should look like this:\ndocker run -it --rm \\ -e WORKSPACE_NAME=aml-demo-we \\ -e SERVICE_NAME=build12345 \\ -e AML_APP_INSIGHTS_ENABLED=true \\ -e AML_APP_INSIGHTS_KEY=1f224928-xxxx-xxxx-xxxx-xxxxxxxxx \\ -p 5001:5001 \\ amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx Now, once we run the model and send some data to it, we should see it popping up in Application Insights by going to \u0026ldquo;Log Analytics\u0026rdquo; and querying for requests:\nOur requests in Application Insights Alternatively, we can also query by traces, which will show us STDOUT/STDERR of our model\u0026rsquo;s code that is running the Docker container:\nOur traces in Application Insights Great, now we have our model running and it is reporting back its STDOUT/STDERR and its predictions to Application Insights. Next, we will also add model data collection, to push model input and prediction data back to Azure Blob Storage.\nAdding model data collection #Lastly, we can add model data collection to our model. For this, we first need to have a storage account with a container called modeldata. In this case, we can just create the container using the Azure Portal:\nCreating a new Blob container for our data collection Next, we need to set the Model Data Collection related environment variables:\nAML_MODEL_DC_STORAGE_ENABLED=true AML_MODEL_DC_STORAGE=\u0026lt;Storage Connection String\u0026gt; In this case, AML_MODEL_DC_STORAGE refers to the connection string to your Storage Account. With this, we can re-run our Docker container with the appropriate parameters set:\ndocker run -it --rm \\ -e WORKSPACE_NAME=aml-demo-we \\ -e SERVICE_NAME=build2542 \\ -e AML_APP_INSIGHTS_ENABLED=true \\ -e AML_APP_INSIGHTS_KEY=123445-1234-1234-1234-12345667889 \\ -e AML_MODEL_DC_STORAGE_ENABLED=true \\ -e AML_MODEL_DC_STORAGE=\u0026#34;DefaultEndpointsProtocol=https;AccountName=xxxxxx;AccountKey=xxxxxxxxx;EndpointSuffix=core.windows.net\u0026#34; \\ -p 5001:5001 \\ amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx After a while (it is currently unclear to me how long this takes), our model input and prediction data will show up in our modeldata container in Blob:\nOur data collection is working From here, we can finally start deploying the model to App Service.\nDeployment to Azure App Service #First, let\u0026rsquo;s create an new App Service:\naz group create --name app-service-deployment --location \u0026#34;West Europe\u0026#34; az appservice plan create --name models --resource-group app-service-deployment --sku B1 --is-linux Next, let\u0026rsquo;s deploy our container to the App Service (it won\u0026rsquo;t pull the image yet as we first need to add authentication to our Container Registry):\naz webapp create --resource-group app-service-deployment --plan models --name model1-blog-demo --deployment-container-image-name amldemowexxxxxx.azurecr.io/package@sha256:e755bb6db2e75f66c5ddcd357b111f2d4axxxxxxxxxxxxxxxx Next, let\u0026rsquo;s add the Managed Identity of our new app to the Container Registry, so it can pull the image:\n# Assign Managed Identity to our Web App az webapp identity assign --resource-group app-service-deployment --name model1-blog-demo --query principalId --output tsv # Query the resource id of our Container Registry az acr show -g aml-demo-we -n amldemowexxxxxx --query id --output tsv # Assign Pull permission of our Web App to our Container Registry az role assignment create --assignee \u0026lt;id from first command\u0026gt; --scope \u0026lt;output from second command\u0026gt; --role \u0026#34;AcrPull\u0026#34; Next, we can add the port mapping and our environment variables:\naz webapp config appsettings set --resource-group app-service-deployment --name model1-blog-demo --settings WEBSITES_PORT=5001 az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings WORKSPACE_NAME=\u0026#34;aml-demo-we\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings SERVICE_NAME=\u0026#34;build12345\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings AML_APP_INSIGHTS_ENABLED=\u0026#34;true\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings AML_APP_INSIGHTS_KEY=\u0026#34;123445-1234-1234-1234-12345667889\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings AML_MODEL_DC_STORAGE_ENABLED=\u0026#34;true\u0026#34; az webapp config appsettings set --name model1-blog-demo --resource-group app-service-deployment --settings AML_MODEL_DC_STORAGE=\u0026#34;DefaultEndpointsProtocol=https;AccountName=xxxxx;AccountKey=xxxxxxxx;EndpointSuffix=core.windows.net\u0026#34; Lastly, let\u0026rsquo;s restart our app so it pulls the new settings:\naz webapp restart --resource-group app-service-deployment --name model1-blog-demo From here, we can finally call our endpoint (same code as above) using https://model1-blog-demo.azurewebsites.net. After a few minutes, our telemetry metrics and data collection should start to kick in show our model\u0026rsquo;s telemetry in Azure.\nNext Steps #As a last step, we should consider two open points: authentication and automation.\nFirstly, we will need to enable authentication via an identity provider for our deployed model. The web app is per default publicly exposed and by using e.g., Azure Active Directory, we can force the model consumers to authenticate. Furthermore, we can use the rich networking settings on App Service to further lock down the service if desired.\nSecondly, we obviously should use some form of Continuous Deployment to automate the deployment steps. This can be fairly easily done by using the commands provided in this post and putting them into a CI/CD pipeline of your choice, e.g. in Azure DevOps or GitHub Actions.\nSummary #This post gave a short overview how to packages models to Docker images in Azure Machine Learning. From there, we discussed options how to capture model telemetry and also enable model data collection. With this, we can now deploy models easily to various platform, such as Azure App Service, while still receiving logs in Application Insights and data in Azure Blob.\n","date":"5 May 2021","permalink":"/posts/deploying-azure-machine-learning-models-azure-app-service/","section":"Posts","summary":"","title":"Deploying Azure Machine Learning Models to Azure App Service"},{"content":"Introduction #This posts shows how to use custom images in Azure Machine Learning for model training. \u0026ldquo;Why would I need that?\u0026rdquo; â€“ you might ask â€“ \u0026ldquo;Doesn\u0026rsquo;t AzureML handle all of this?\u0026rdquo; â€“ and yes, you\u0026rsquo;re right, Azure Machine Learning does all of that. However, there are a few scenarios where building your own custom images enables a few new scenarios:\nInclusion of libraries not available via pip/conda (odbc drivers, etc.) Stricter security processes (image creation, scanning, consumption) Enable scenarios, where Azure Machine Learning is not allowed to talk the non-Azure resources on the internet (e.g., PyPI, conda-forge, etc.) These might not be your typical everyday use cases, but especially in more lock-down, secured environments, these scenarios are quite common. So let\u0026rsquo;s get started!\nBuilding Custom Images #The idea of custom images for Azure Machine Learning is that you take one of the base AML images and add your additional Docker image layers on top of them. You can refer to this repo for getting a list of all the base images available for Azure Machine Learning. I typically end up using one of these:\nmcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04 mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu\u0026lt;x\u0026gt;.04 (x=16/18) mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.y-cudnn7-ubuntu\u0026lt;x\u0026gt;.04 (x=16/18, y=0/1/2) But wait, can\u0026rsquo;t I build a fully-customized image from scratch? Yes, you can, but I would not recommend it as using the AML base images will ensure that your resulting image will (very likely) stay compatible with AML.\nFor this post, I\u0026rsquo;ve created this example repo as a starting point: https://github.com/csiebler/aml-custom-image â€“ feel free to clone it and get started.\nTo keep it simple, I\u0026rsquo;ve just put a simple requirements.txt that gets installed into the image (see Dockerfile):\nFROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04 COPY requirements.txt /tmp/requirements.txt RUN pip install -r /tmp/requirements.txt From here, add all the stuff you want to package into your custom image.\nOnce done, let\u0026rsquo;s build this image and put it to work! I assume you already have a AML workspace up again running. So first, let\u0026rsquo;s log in to your Azure Container Registry via CLI:\naz acr login --name amldemowe12345 docker login amldemowe12345.azurecr.io You can easily retrieve the login credentials for the Container Registry through the Azure Portal:\nACR credentials in Azure Portal Next, you can build the image and push it to the registry:\ndocker build . -t amldemowe12345.azurecr.io/azureml-images/scikit-learn:0.23.2 docker push amldemowe12345.azurecr.io/azureml-images/scikit-learn:0.23.2 Great, now we\u0026rsquo;ve pushed the image, we can get started to run some training jobs with it!\nTraining using Custom Images #Let\u0026rsquo;s look how we can use the new image and run a training job with it, either with the CLI or the Python SDK.\nUsing the az ml CLI #You can run a training job in AML by editing the training.runconfig from the repo and update the docker -\u0026gt; baseImage section to point to your newly created image:\n... docker: enabled: true baseImage: amldemowe12345.azurecr.io/azureml-images/scikit-learn:0.23.2 ... Next, we can use the az ml CLI to attach to the workspace. This will authenticate us and set the resource group/workspace as default for subsequent CLI commands. From there, we can kick off the training using the training.runconfig:\naz ml folder attach -g aml-demo -w aml-demo az ml run submit-script -c training -e custom-image-training If you look at the training run in the ML Studio, you can see that the image has been pulled correctly, without AML building its own image:\nAzureML pulling our custom image Using the Python SDK #Alternatively, you can run this example via the Python SDK:\nfrom azureml.core import Workspace, Environment from azureml.core import ScriptRunConfig from azureml.core.compute import ComputeTarget from azureml.core import Experiment ws = Workspace.from_config() custom_env = Environment(\u0026#34;custom_env\u0026#34;) custom_env.docker.enabled = True custom_env.python.user_managed_dependencies = True custom_env.docker.base_image = \u0026#34;amldemowe12345.azurecr.io/azureml-images/scikit-learn:0.23.2\u0026#34; cluster_name = \u0026#34;cpu-cluster\u0026#34; compute_target = ComputeTarget(workspace=ws, name=cluster_name) src = ScriptRunConfig(source_directory=\u0026#39;./train-example\u0026#39;, script=\u0026#39;train.py\u0026#39;, compute_target=compute_target, environment=custom_env) run = Experiment(ws,\u0026#39;custom-image-training\u0026#39;).submit(src) run.wait_for_completion(show_output=True) Again, use the log output from the training run in the Studio UI to confirm, that AML used your custom image!\nNext Steps #From here, I would recommend to automate building your custom Docker images through e.g., Azure DevOps or GitHub Actions. This will also allow you to further harden the image by e.g., integrating with third-party image scanning capabilities and others.\nSummary #In conclusion, using custom images with Azure Machine Learning is a great way to control which packages are being during model training. Typically, AML does this automatically for us, but in more restrictive environments, it is often desired to do this outside in a CI/CD pipeline. Furthermore, custom images help us to enable environments where internet access for AML can be safely disabled.\nI hope this post was helpful â€“ feel free to reuse the code from https://github.com/csiebler/aml-custom-image.\n","date":"3 May 2021","permalink":"/posts/azure-machine-learning-custom-images/","section":"Posts","summary":"","title":"Azure Machine Learning Custom Images (for Advanced Scenarios)"},{"content":"Introduction #This post outlines how you can mount a Dataset to a Compute Instance in Azure Machine. This can help exploring file-based datasets in Jupyter, especially for large datasets where download to the disk of the Compute Instance is impractical. Furthermore, this method can also help during exploration phase, where you probably want to read only a subset of the data.\nPrerequisites #All we need to get started is a Workspace with a file-based Dataset, as well as a Compute Instance. Im testing this on a STANDARD_DS3_V2 instance in West Europe.\nDataset in Azure Machine Learning Mounting a Dataset to a Compute Instance #Lets start up Jupyter or JupyterLab on the Compute Instance. You can execute the following code to mount the dataset to the machine, access the data, and then later unmount it:\nimport os import pandas as pd from azureml.core import Workspace, Dataset # Connect to Workspace and reference Dataset ws = Workspace.from_config() dataset = ws.datasets[\u0026#34;german-credit-train-tutorial\u0026#34;] # Create mountcontext and mount the dataset mount_ctx = dataset.mount() mount_ctx.start() # Get the mount point dataset_mount_folder = mount_ctx.mount_point print(dataset_mount_folder) # List the files in the mount point files = os.listdir(dataset_mount_folder) print(files) # Read some data df = pd.read_csv(os.path.join(dataset_mount_folder, \u0026#39;german_credit_data.csv\u0026#39;)) # Do some more stuff with the data.... # Unmount the dataset from the instance mount_ctx.stop() If you want to mount the dataset to a specific folder, you can also specify the mount path, e.g., mount(mount_point='/mnt/dataset1'). In this case, the path already needs to exist. See the API documentation for more details.\nMounting a Dataset in Juypter In case you forget to stop the mount context (i.e., unmounting it), no worries! You can also do it from the command line later:\nazureuser@clemens-vm:/$ mount | grep /tmp _DPrepFuse on /tmp/tmp89tgbd31 type fuse (rw,nosuid,nodev,relatime,user_id=1001,group_id=1002) azureuser@clemens-vm:/$ sudo umount /tmp/tmp89tgbd31 azureuser@clemens-vm:/$ mount | grep /tmp Restarting the Compute Instance will also remove the mount. Azure Machine Learning uses fuse to mount the Storage Account. Therefore, there is no drawback on the Storage Account in case you forget to unmount it. The dataset is mounted as read-only, there you cannot cause any inconsistencies anyway.\nPerformance #In my quick test, I was using a 10 GB file in Blob Storage and just read it into the Compute Instance:\n$ dd if=test_10gb.tmp of=/dev/null bs=64k 163840+0 records in 163840+0 records out 10737418240 bytes (11 GB, 10 GiB) copied, 89.567 s, 120 MB/s The results are very much in line with the expected performance for reading a single file in Blob (Standard tier). Typically, we should see around ~80-120MB/s per file (sometimes even a bit more). Reading files in parallel, leveraging Premium Blob, ADLSg2, or using a larger Compute Instance size, etc. could obviously improve performance even more.\nSummary #Mounting a Dataset to a Compute Instance in Azure Machine Learning is easy and can dramatically help during data exploration and when dealing with large datasets.\nStay safe and let me know if you have any questions!\n","date":"7 December 2020","permalink":"/posts/mount-datasets-compute-instance-azure-machine-learning/","section":"Posts","summary":"","title":"Mounting Datasets to a Compute Instance in Azure Machine Learning"},{"content":"I\u0026rsquo;m Clemens â€“ I studied Computer Science at the Karlsruhe Institute of Technology, Germany and spend a year researching at Carnegie Mellon University in Pittsburgh, PA, USA. In my professional life I worked in several roles as a Software Engineer, Cloud Solution Architect, and also as a manager of a team of Solution Architects. Over the past couple of years, I\u0026rsquo;ve published multiple patents and research papers. I am currently working as a Specialist for AI \u0026amp; ML at Microsoft.\nIn my spare time I enjoy traveling and keeping up with the civil aviation industry, hiking up the Bavarian Alps, and last but not least, spend as much time as possible cooking in the kitchen.\n","date":null,"permalink":"/about/","section":"Clemens Siebler's Blog","summary":"","title":"About"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" Speaking in front of small and large audiences has been my passion for many years â€“ if you want to see me on stage, check out my upcoming talks. If you want to host me for a talk, feel free to reach out to me!\nPast events #Microsoft Envision AI Connection DACH - Munich, Germany (March 2024) # Innovativ sein mit Generative AI: Ein Praxisleitfaden zur Umsetzung - mit Deutsche Telekom Big-Data.AI Summit 2023 - Berlin, Germany (October 2023) # Scaling Generative AI to All Employees and beyond (YouTube) Medientage MÃ¼nchen #MTM22 â€“ Munich, Germany (October 2022) # Synthetische Stimmen bei ntvâ€‹ - Sprachtechnologie als strategisches Instrument fÃ¼r Reichweiten- und Produktentwicklungâ€‹ (together with Tatjana Anisimov) Microsoft Business Summit 2019 â€“ Leipzig, Germany (October 2019) # KÃ¼nstliche Intelligenz anhand von Praxisbeispielen besser verstehen KÃ¼nstliche Intelligenz fÃ¼r alle â€“ wie der Einstieg in KI durch die Azure Plattform immer einfacher wird Intelligent ERP: Vom Systemintegrator (SI) zum SaaS-Anbieter Global AI Nights 2019 â€“ Munich, Germany (September 2019) # Automated Machine Learning on Azure Microsoft #DPK18 â€“ Leipzig, Germany (October 2018) # AI \u0026amp; ML: Das benÃ¶tigen Sie, um technische Projekte erfolgreich umzusetzen AI \u0026amp; ML: Eindrucksvolle Demos, mit denen Sie neue Projekte generieren Microsoft Tech Briefing â€“ Munich, Germany (May 2018) # DevOps, Containers \u0026amp; Serverless Computing NetApp Insight 2018 â€“ Barcelona, Spain (April 2018) # Microsoft: Migrate to Azure with Confidence: Start Moving your Data, Applications, and Infrastructure Today Microsoft Tech Briefing â€“ Munich, Germany (April 2018) # Artificial Intelligence \u0026amp; Machine Learning on Azure NetApp Insight 2017 â€“ Berlin, Germany (November 2017) # Building Scalable Web Applications with NetApp StorageGRID Webscale Everything You Need To Know About the S3 Protocol in NetApp StorageGRID Webscale Red Hat Forum Austria 2017 â€“ Vienna, Austria (October 2017) # Running Stateful Container Workloads with NetApp (Slides) NetApp Insight 2017 â€“ Las Vegas, USA (October 2017) # Building Scalable Web Applications with NetApp StorageGRID Webscale Everything You Need To Know About the S3 Protocol in NetApp StorageGRID Webscale Large-scale StorageGRID Deployments â€“ Best Practices from the Field NetApp Insight 2016 â€“ Berlin, Germany (November 2016) # Building Scalable Web Applications with NetApp StorageGRID Webscale Everything You Need To Know About the S3 Protocol in NetApp StorageGRID Webscale NetApp Insight 2016 â€“ Las Vegas, USA (October 2016) # Building Scalable Web Applications with NetApp StorageGRID Webscale Everything You Need To Know About the S3 Protocol in NetApp StorageGRID Webscale Docker User Group Stuttgart #4 (August 2016) # Persistent Storage for Containers NetApp Insight 2015 â€“ Berlin, Germany (November 2015) # Building Scalable Web Applications with NetApp StorageGRID Webscale How to Deploy NetApp Private Storage for Microsoft Azure NetApp Insight 2015 â€“ Las Vegas, USA (October 2015) # Building Scalable Web Applications with NetApp StorageGRID Webscale ","date":null,"permalink":"/talks/","section":"Clemens Siebler's Blog","summary":"","title":"Talks"}]