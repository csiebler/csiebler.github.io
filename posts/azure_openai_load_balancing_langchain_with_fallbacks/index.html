<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><link rel=alternate type=application/rss+xml title="Clemens Siebler's Blog RSS Feed" href=https://clemenssiebler.com//posts/index.xml><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 sm:px-14 md:px-24 lg:px-32 dark:bg-neutral-800 dark:text-neutral"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 print:hidden sm:py-10 dark:text-neutral"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>Clemens Siebler&rsquo;s Blog</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Posts</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/talks/ title=Talks><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Talks</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">About</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=https://github.com/csiebler title target=_blank><span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=https://www.linkedin.com/in/csiebler/ title target=_blank><span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=https://clemenssiebler.com/posts/index.xml title target=_blank><span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path d="M0 64C0 46.3 14.3 32 32 32c229.8.0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7.0 64zM0 416a64 64 0 11128 0A64 64 0 110 416zM32 160c159.1.0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7.0-32-14.3-32-32s14.3-32 32-32z"/></svg></span></span></a></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Using LangChain with Azure OpenAI to implement load balancing via fallbacks</h1><div class="mb-12 mt-8 text-base text-neutral-500 print:hidden dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2023-08-24 00:00:00 +0000 UTC">24 August 2023</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">6 mins</span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#solutions>Solutions</a><ul><li><a href=#falling-back-to-a-larger-model>Falling back to a larger model</a></li><li><a href=#falling-back-to-multiple-but-the-same-models>Falling back to multiple, but the same models</a></li></ul></li><li><a href=#considerations>Considerations</a></li><li><a href=#key-takeaways>Key Takeaways</a></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><h2 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>In this article we&rsquo;ll be looking into two simple and quick ways to implement a &ldquo;kind-of load balancing&rdquo; for Azure OpenAI using LangChain. In my last poste <a href=https://clemenssiebler.com/posts/understanding-azure-openai-rate-limits-monitoring/ target=_blank rel=noreferrer>A Guide to Azure OpenAI Service&rsquo;s Rate Limits and Monitoring</a> we discussed how rate limits can negatively affect throughput and latency when running completions, so in this post we&rsquo;ll dive deeper into two practical solutions.</p><p>Both approaches outlined in this post achieve similar results as using an external load balancer, but are significantly easier to implement when using LangChain. However, neither approach splits up the load between multiple instances equally (e.g., using round-robin). Instead, they shift the load to a subsequent model once the current one hits its limit.</p><h2 id=solutions class="relative group">Solutions <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#solutions aria-label=Anchor>#</a></span></h2><p>To run the code examples, make sure you have the latest versions of <code>openai</code> and <code>langchain</code> installed:</p><pre tabindex=0><code>pip install openai --upgrade
pip install langchain --upgrade
</code></pre><p>In this post, we&rsquo;ll be using <code>openai==0.27.9</code> and <code>langchain==0.0.271</code>.</p><h3 id=falling-back-to-a-larger-model class="relative group">Falling back to a larger model <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#falling-back-to-a-larger-model aria-label=Anchor>#</a></span></h3><p>One simple way to deal with the token or rate limit is to use a different or larger model when this happens. The rational behind it is that larger models often give larger token limits per minute (so for example does Azure). So instead of retrying and waiting for the same model to become &ldquo;useable&rdquo; again, we just switch to a larger model.</p><p><figure><img src=/images/fallback_larger_model.png alt="Fallback to a larger model" class="mx-auto my-0 rounded-md"></figure></p><p>This approach is especially useful when using <code>gpt4</code> on Azure OpenAI as the default TPM (token per minute) limit for <code>gpt4-8k</code> is only 20k TPM. However, since we get an additional 60k TPM for <code>gpt4-32k</code>, why not use them? Surely, there is some additional, higher cost associated with this (the <code>32k</code> model is more expensive for both prompt and completion tokens), but if throughput is the limiting factor, this is an easy fix.</p><p>To get this going, let&rsquo;s first make sure we have both models deployed:<figure><img src=/images/gpt4_8k_32k_deployments.png alt="Azure OpenAI gpt4-8k and gpt4-32k model deployments" class="mx-auto my-0 rounded-md"></figure></p><p>Then create an <code>.env</code> with the credentials to the endpoint:</p><pre tabindex=0><code>AOAI_ENDPOINT_01=https://xxxxxxx.openai.azure.com/
AOAI_KEY_01=xxxxxx
</code></pre><p>And lastly run the following Python code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.chat_models</span> <span class=kn>import</span> <span class=n>AzureChatOpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.prompts.chat</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>ChatPromptTemplate</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>SystemMessagePromptTemplate</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>HumanMessagePromptTemplate</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>kwargs</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;openai_api_type&#34;</span><span class=p>:</span> <span class=s2>&#34;azure&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;openai_api_base&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;AOAI_ENDPOINT_01&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;openai_api_key&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;AOAI_KEY_01&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;openai_api_version&#34;</span><span class=p>:</span> <span class=s2>&#34;2023-05-15&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;max_tokens&#34;</span><span class=p>:</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a connection to gpt4-8k and gpt4-32k models</span>
</span></span><span class=line><span class=cl><span class=n>llm_8k</span> <span class=o>=</span> <span class=n>AzureChatOpenAI</span><span class=p>(</span><span class=n>deployment_name</span><span class=o>=</span><span class=s2>&#34;gpt4-8k&#34;</span><span class=p>,</span> <span class=n>max_retries</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llm_32k</span> <span class=o>=</span> <span class=n>AzureChatOpenAI</span><span class=p>(</span><span class=n>deployment_name</span><span class=o>=</span><span class=s2>&#34;gpt4-32k&#34;</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># This is where the magic happens</span>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>llm_8k</span><span class=o>.</span><span class=n>with_fallbacks</span><span class=p>([</span><span class=n>llm_32k</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>system_message_prompt</span> <span class=o>=</span> <span class=n>SystemMessagePromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=s2>&#34;You are an AI assistant that tells jokes.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>human_message_prompt</span> <span class=o>=</span> <span class=n>HumanMessagePromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=s2>&#34;</span><span class=si>{text}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>chat_prompt</span> <span class=o>=</span> <span class=n>ChatPromptTemplate</span><span class=o>.</span><span class=n>from_messages</span><span class=p>([</span><span class=n>system_message_prompt</span><span class=p>,</span> <span class=n>human_message_prompt</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>chat_prompt</span> <span class=o>|</span> <span class=n>llm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>({</span><span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;Tell me a dad joke&#34;</span><span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div><p>Easy! This code will run happily and choose the <code>8k</code> model, but in case it fails (we set <code>max_retries=0</code>), it will fall back to the <code>32k</code> model and will keep retrying until it gives up (per default 6 tries). All we needed to do was create an <code>AzureChatOpenAI</code> for each model, and then configure the fallback. The remainder of the LangChain code stayed the same, so adding this to an existing projects is pretty easy. The other benefit is that it allows to use higher level concepts like <code>ConversationalRetrievalChain</code> in LangChain.</p><p>In summary, this approach has the following benefits:</p><ul><li>Only requires one Azure OpenAI resource</li><li>Great for <code>gpt4</code>, where the quota is lower and quota increases are harder to get</li></ul><p>Its downsides are:</p><ul><li>Typically more costly</li><li>Limited scalability (mostly due to the fact that getting access to <code>gpt4</code> can still take some time)</li><li>Does not solve the noisy neighbor problem</li></ul><h3 id=falling-back-to-multiple-but-the-same-models class="relative group">Falling back to multiple, but the same models <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#falling-back-to-multiple-but-the-same-models aria-label=Anchor>#</a></span></h3><p>If we do not want to use a larger model, we can try an equally simple approach: falling back to the same model, but deployed in a different region and/or subscription.</p><p><figure><img src=/images/fallback_different_region.png alt="Fallback to the same model in a different region or subscription" class="mx-auto my-0 rounded-md"></figure></p><p>For example, with <code>gpt-35-turbo</code> we&rsquo;ll get 240k TPM per subscription and region, so we if leverage four regions at once, we get close to 1m TPM (at least theoretically)! All we need to do is create an Azure OpenAI resource in each region and deploy our <code>gpt-35-turbo</code> model.</p><p>For this, create an <code>.env</code> pointing to our Azure OpenAI endpoints in the different regions (could be all in the same or different subscriptions):</p><pre tabindex=0><code>AOAI_ENDPOINT_01=https://xxxxxxx.openai.azure.com/
AOAI_KEY_01=xxxxxx
AOAI_ENDPOINT_02=https://xxxxxxx.openai.azure.com/
AOAI_KEY_02=xxxxxx
AOAI_ENDPOINT_03=https://xxxxxxx.openai.azure.com/
AOAI_KEY_03=xxxxxx
</code></pre><p>Next, let&rsquo;s fire up Python:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.chat_models</span> <span class=kn>import</span> <span class=n>AzureChatOpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.prompts.chat</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>ChatPromptTemplate</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>SystemMessagePromptTemplate</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>HumanMessagePromptTemplate</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># get all credentials for connections from .env</span>
</span></span><span class=line><span class=cl><span class=n>keys</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span> <span class=ow>in</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>key</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&#34;AOAI_ENDPOINT_&#34;</span><span class=p>)</span> <span class=ow>and</span> <span class=n>value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>num</span> <span class=o>=</span> <span class=n>key</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34;_&#34;</span><span class=p>)[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>keys</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>value</span><span class=p>,</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=sa>f</span><span class=s2>&#34;AOAI_KEY_</span><span class=si>{</span><span class=n>num</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Init connections to LLMs</span>
</span></span><span class=line><span class=cl><span class=n>llms</span> <span class=o>=</span> <span class=p>[</span><span class=n>AzureChatOpenAI</span><span class=p>(</span><span class=n>openai_api_type</span><span class=o>=</span><span class=s2>&#34;azure&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>openai_api_base</span><span class=o>=</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                      <span class=n>openai_api_key</span><span class=o>=</span><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                      <span class=n>openai_api_version</span><span class=o>=</span><span class=s2>&#34;2023-05-15&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>deployment_name</span><span class=o>=</span><span class=s2>&#34;gpt-35-turbo&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                      <span class=n>max_retries</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>keys</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Let&#39;s keep the last LLM as the backup with retries, in case all other LLMs failed</span>
</span></span><span class=line><span class=cl><span class=n>llms</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max_retries</span> <span class=o>=</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create our main LLM object that can fall back to all other LLMs</span>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>llms</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>with_fallbacks</span><span class=p>(</span><span class=n>llms</span><span class=p>[</span><span class=mi>1</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Perform prompting as normal</span>
</span></span><span class=line><span class=cl><span class=n>system_message_prompt</span> <span class=o>=</span> <span class=n>SystemMessagePromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=s2>&#34;You are an AI assistant that tells jokes.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>human_message_prompt</span> <span class=o>=</span> <span class=n>HumanMessagePromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=s2>&#34;</span><span class=si>{text}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>chat_prompt</span> <span class=o>=</span> <span class=n>ChatPromptTemplate</span><span class=o>.</span><span class=n>from_messages</span><span class=p>([</span><span class=n>system_message_prompt</span><span class=p>,</span> <span class=n>human_message_prompt</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>chat_prompt</span> <span class=o>|</span> <span class=n>llm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>({</span><span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;Tell me a dad joke&#34;</span><span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div><p>Same as before, we use the <code>with_fallbacks()</code> option from LangChain to fall back to the next model, then the next model, and so on. For the last model, we keep the <code>max_retries</code> at <code>6</code> in order to have a last resort for retries.</p><p>In summary, this approach has the following benefits:</p><ul><li>Very large scalability</li><li>Might solve the noisy neighbor problem, in case other regions are less crowded (requires to set a low <code>request_timeout</code>)</li></ul><p>Its downsides are:</p><ul><li>Requires multiple Azure OpenAI resources, potentially multiple subscriptions</li></ul><h2 id=considerations class="relative group">Considerations <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#considerations aria-label=Anchor>#</a></span></h2><p>When using either strategy, it&rsquo;s vital to determine the specific scenarios where the <code>with_fallbacks()</code> method comes into play. For instance, in our provided example, we default to a fallback during any error. However, in some scenarios, you might want to activate this only during token or rate limiting incidents. This customization can be done seamlessly by incorporating the <code>exceptions_to_handle</code> parameter.</p><p>Another aspect to keep an eye on is defining the <code>request_timeout</code> for the model. To prevent prolonged API interactions, consider an immediate transition to an alternative model after a stipulated timeout, say 10 seconds. This can be realized by initializing the <code>AzureChatOpenAI()</code> connections using the <code>request_timeout=10</code> parameter.</p><p>Lastly, it&rsquo;s always prudent to surround our Azure OpenAI interactions with a try/catch mechanism, ensuring a safe exit strategy should any unexpected issues arise.</p><h2 id=key-takeaways class="relative group">Key Takeaways <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h2><p>In this article, we&rsquo;ve explored two straightforward methods to establish fallback strategies with LangChain, effectively circumventing token or rate limitations in Azure OpenAI. While these methods excel in addressing token or rate restrictions, if your goal is to delve into genuine load-balancing at the http level, I recommend reading <a href=https://www.raffertyuy.com/raztype/azure-openai-load-balancing/ target=_blank rel=noreferrer>this detailed post</a>.</p></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=flex><div class=place-self-center><div class="text-2xl sm:text-lg"></div></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.facebook.com/sharer/sharer.php?u=https://clemenssiebler.com/posts/azure_openai_load_balancing_langchain_with_fallbacks/&amp;quote=Using%20LangChain%20with%20Azure%20OpenAI%20to%20implement%20load%20balancing%20via%20fallbacks" title="Share on Facebook" aria-label="Share on Facebook" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://clemenssiebler.com/posts/azure_openai_load_balancing_langchain_with_fallbacks/&amp;text=Using%20LangChain%20with%20Azure%20OpenAI%20to%20implement%20load%20balancing%20via%20fallbacks" title="Tweet on Twitter" aria-label="Tweet on Twitter" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clemenssiebler.com/posts/azure_openai_load_balancing_langchain_with_fallbacks/&amp;description=Using%20LangChain%20with%20Azure%20OpenAI%20to%20implement%20load%20balancing%20via%20fallbacks" title="Pin on Pinterest" aria-label="Pin on Pinterest" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M496 256c0 137-111 248-248 248-25.6.0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8.0 128.7-68.8 128.7-154.3.0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1.0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6.0-54.7 41.4-107.6 112-107.6 60.9.0 103.6 41.5 103.6 100.9.0 67.1-33.9 113.6-78 113.6-24.3.0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6.0-19-10.2-34.9-31.4-34.9-24.9.0-44.9 25.7-44.9 60.2.0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9.0 361.1.0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://clemenssiebler.com/posts/azure_openai_load_balancing_langchain_with_fallbacks/&amp;resubmit=true&amp;title=Using%20LangChain%20with%20Azure%20OpenAI%20to%20implement%20load%20balancing%20via%20fallbacks" title="Submit to Reddit" aria-label="Submit to Reddit" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://clemenssiebler.com/posts/azure_openai_load_balancing_langchain_with_fallbacks/&amp;title=Using%20LangChain%20with%20Azure%20OpenAI%20to%20implement%20load%20balancing%20via%20fallbacks" title="Share on LinkedIn" aria-label="Share on LinkedIn" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://clemenssiebler.com/posts/azure_openai_load_balancing_langchain_with_fallbacks/&amp;subject=Using%20LangChain%20with%20Azure%20OpenAI%20to%20implement%20load%20balancing%20via%20fallbacks" title="Send via email" aria-label="Send via email" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/understanding-azure-openai-rate-limits-monitoring/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">A Guide to Azure OpenAI Service's Rate Limits and Monitoring</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-08-01 00:00:00 +0000 UTC">1 August 2023</time>
</span></span></a></span><span><a class="group flex text-right" href=/posts/smart-loadbalancing-for-azure-openai-with-api-management/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Smart Load-Balancing for Azure OpenAI with Azure API Management</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-12-20 00:00:00 +0000 UTC">20 December 2023</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".prose img"));images.forEach(e=>{mediumZoom(e,{margin:5,scrollOffset:40,container:null,template:null})})</script></footer></div></body></html>