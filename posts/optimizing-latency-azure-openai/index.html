<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><meta http-equiv=x-ua-compatible content="ie=edge"><title>Optimizing latency for Azure OpenAI Service &#183; Clemens Siebler's Blog</title><meta name=title content="Optimizing latency for Azure OpenAI Service &#183; Clemens Siebler's Blog"><meta name=description content="Introduction # In this post we&rsquo;ll be looking into measuring and optimizing Azure OpenAI Service response latency by evaluating the deployed endpoints Azure OpenAI endpoints on a global scale."><link rel=canonical href=https://clemenssiebler.com/posts/optimizing-latency-azure-openai/><link type=text/css rel=stylesheet href=/css/main.bundle.min.211dc1fea886d31bdd406064f79e70c214370433eafb69692f1742be3225e4a4.css integrity="sha256-IR3B/qiG0xvdQGBk955wwhQ3BDPq+2lpLxdCvjIl5KQ="><script type=text/javascript src=/js/appearance.min.022d0ebc3b46a335eb1c7ef79b7f2de143d7cd5156d433638592ef1ce5f8554e.js integrity="sha256-Ai0OvDtGozXrHH73m38t4UPXzVFW1DNjhZLvHOX4VU4="></script>
<script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.3e80d0528c59616e7de9b0ca9316d9afd314b49ed9e5e6b88873731486ad67ee.js integrity="sha256-PoDQUoxZYW596bDKkxbZr9MUtJ7Z5ea4iHNzFIatZ+4=" data-copy=Copy data-copied=Copied></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="Optimizing latency for Azure OpenAI Service"><meta property="og:description" content="Introduction # In this post we&rsquo;ll be looking into measuring and optimizing Azure OpenAI Service response latency by evaluating the deployed endpoints Azure OpenAI endpoints on a global scale."><meta property="og:type" content="article"><meta property="og:url" content="https://clemenssiebler.com/posts/optimizing-latency-azure-openai/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-01T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Optimizing latency for Azure OpenAI Service"><meta name=twitter:description content="Introduction # In this post we&rsquo;ll be looking into measuring and optimizing Azure OpenAI Service response latency by evaluating the deployed endpoints Azure OpenAI endpoints on a global scale."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Optimizing latency for Azure OpenAI Service","headline":"Optimizing latency for Azure OpenAI Service","abstract":"Introduction # In this post we\u0026rsquo;ll be looking into measuring and optimizing Azure OpenAI Service response latency by evaluating the deployed endpoints Azure OpenAI endpoints on a global scale.","inLanguage":"en","url":"https:\/\/clemenssiebler.com\/posts\/optimizing-latency-azure-openai\/","author":{"@type":"Person","name":"Clemens Siebler"},"copyrightYear":"2023","dateCreated":"2023-08-01T00:00:00\u002b00:00","datePublished":"2023-08-01T00:00:00\u002b00:00","dateModified":"2023-08-01T00:00:00\u002b00:00","mainEntityOfPage":"true","wordCount":"1026"}]</script><meta name=author content="Clemens Siebler"><link href=https://github.com/csiebler rel=me><link href=https://instagram.com/csiebler rel=me><link href=https://linkedin.com/in/csiebler rel=me><link href=https://twitter.com/clemenssiebler rel=me><script async src="https://www.googletagmanager.com/gtag/js?id=G-HB3D5YJ128"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-HB3D5YJ128",{anonymize_ip:!1})}</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold pe-2 text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral print:hidden sm:py-10"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>Clemens Siebler&rsquo;s Blog</a></div><ul class="flex list-none flex-col ltr:text-right rtl:text-left sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Posts</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/talks/ title=Talks><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Talks</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">About</span></a></li></ul></nav></header><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Optimizing latency for Azure OpenAI Service</h1><div class="mt-8 mb-12 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2023-08-01 00:00:00 +0000 UTC">1 August 2023</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">5 mins</span></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="-ms-5 block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="-ms-5 border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#ideas-for-optimizing-latency>Ideas for optimizing latency</a></li><li><a href=#access-configuration>Access configuration</a></li><li><a href=#testing-latency-via-python>Testing latency via Python</a></li><li><a href=#moving-forward>Moving forward</a></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-prose grow"><h2 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>In this post we&rsquo;ll be looking into measuring and optimizing <a href=https://azure.microsoft.com/en-us/products/cognitive-services/openai-service target=_blank rel="noreferrer noopener">Azure OpenAI Service</a> response latency by evaluating the deployed endpoints Azure OpenAI endpoints on a global scale. By optimizing latency, we can enable more real-time use cases, as well as maximize throughput for batch workloads. Our main goal in this exercise to avoid latency peaks that might show up here and there if any of the regions experiences significant load (noisy neighbors) or if we&rsquo;re running into API rate limits.</p><p>This can be used to optimize latency for <code>gpt-35-turbo</code>, but can also be applied to <code>gpt-4</code> model series.</p><h2 id=ideas-for-optimizing-latency class="relative group">Ideas for optimizing latency <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ideas-for-optimizing-latency aria-label=Anchor>#</a></span></h2><p>If we want to build a &ldquo;latency-optimized&rdquo; app using Azure OpenAI, we could do the following approach:</p><ul><li>Measure latency against a range of worldwide regions using a short test prompt</li><li>Based on the call&rsquo;s status code, latency, and rolling average latency (for instance, a decay rate of <code>0.8</code>), select the fastest regions for the actual API call</li><li>Execute the API calls</li><li>Repeat this check at intervals between 10 and 60 seconds</li></ul><p>But what about the latency added by using an Azure region far from our application? Yes, this can cause additional latency. However, the main goal here is to prevent abrupt latency spikes. To give you some idea, here are a few quick tests:</p><ul><li>Latency from central Germany to <code>canadaeast</code>: &lt;110ms</li><li>Latency from central Germany to <code>uksouth</code>: &lt;20ms</li><li>Latency from central Germany to <code>japaneast</code>: &lt;250ms</li></ul><p>Even considering a long distance, such as from the East coast to Singapore, the worst-case scenario is ~300ms of latency. However, if your app runs on Azure, you should experience significantly lower latency due to the use of the Microsoft backbone, as opposed to the public internet.</p><p>In context, running a prompt with 1000 input tokens and 200 completion tokens likely takes between half a second and two seconds to complete, so adding 100ms, 200ms, or 300ms doesn&rsquo;t significantly impact our aim to prevent spikes.</p><h2 id=access-configuration class="relative group">Access configuration <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#access-configuration aria-label=Anchor>#</a></span></h2><p>First, let&rsquo;s create an <code>accounts.json</code> that holds the endpoints and access keys for all the regions we want to test. In this case, I&rsquo;ve just created Azure OpenAI resources in all regions where I still had capacity left:</p><pre tabindex=0><code>[
  {
    &#34;endpoint&#34;: &#34;https://canadaeast.api.cognitive.microsoft.com/&#34;,
    &#34;key&#34;: &#34;...&#34;
  },
  {
    &#34;endpoint&#34;: &#34;https://eastus2.api.cognitive.microsoft.com/&#34;,
    &#34;key&#34;: &#34;...&#34;
  },
  {
    &#34;endpoint&#34;: &#34;https://francecentral.api.cognitive.microsoft.com/&#34;,
    &#34;key&#34;: &#34;...&#34;
  },
  {
    &#34;endpoint&#34;: &#34;https://japaneast.api.cognitive.microsoft.com/&#34;,
    &#34;key&#34;: &#34;...&#34;
  },
  {
    &#34;endpoint&#34;: &#34;https://northcentralus.api.cognitive.microsoft.com/&#34;,
    &#34;key&#34;: &#34;...&#34;
  },
  {
    &#34;endpoint&#34;: &#34;https://uksouth.api.cognitive.microsoft.com/&#34;,
    &#34;key&#34;: &#34;...&#34;
  }
]
</code></pre><h2 id=testing-latency-via-python class="relative group">Testing latency via Python <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#testing-latency-via-python aria-label=Anchor>#</a></span></h2><p>To begin, install <code>requests</code>:</p><pre tabindex=0><code>pip install requests
</code></pre><p>We opt for simple HTTP requests over the <code>openai</code> SDK due to easier management of call timeouts and status codes.</p><p>Here&rsquo;s a sample script for the job:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>decay_rate</span> <span class=o>=</span> <span class=mf>0.8</span>
</span></span><span class=line><span class=cl><span class=n>http_timeout</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>test_interval</span> <span class=o>=</span> <span class=mi>15</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;accounts.json&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>accounts</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>f</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_latency_for_endpoint</span><span class=p>(</span><span class=n>endpoint</span><span class=p>,</span> <span class=n>key</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>url</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>endpoint</span><span class=si>}</span><span class=s2>/openai/deployments/gpt-35-turbo/chat/completions?api-version=2023-05-15&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>headers</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Content-Type&#34;</span><span class=p>:</span> <span class=s2>&#34;application/json&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;api-key&#34;</span><span class=p>:</span> <span class=n>key</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;max_tokens&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;messages&#34;</span><span class=p>:[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>},{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Hi&#34;</span><span class=p>}]}</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>t_start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>headers</span><span class=o>=</span><span class=n>headers</span><span class=p>,</span> <span class=n>json</span><span class=o>=</span><span class=n>data</span><span class=p>,</span> <span class=n>timeout</span><span class=o>=</span><span class=n>http_timeout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>latency</span> <span class=o>=</span> <span class=p>(</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>t_start</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>status</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>status_code</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>status</span> <span class=o>=</span> <span class=mi>500</span>
</span></span><span class=line><span class=cl>        <span class=n>latency</span> <span class=o>=</span> <span class=n>http_timeout</span>
</span></span><span class=line><span class=cl>    <span class=c1># print(response.json())    </span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Endpoint: </span><span class=si>{</span><span class=n>endpoint</span><span class=si>}</span><span class=s2>, Status: </span><span class=si>{</span><span class=n>status</span><span class=si>}</span><span class=s2>, Latency: </span><span class=si>{</span><span class=n>latency</span><span class=si>}</span><span class=s2>s&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;ts&#34;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>strftime</span><span class=p>(</span><span class=s2>&#34;%Y-%m-</span><span class=si>%d</span><span class=s2> %H:%M:%S&#34;</span><span class=p>,</span> <span class=n>time</span><span class=o>.</span><span class=n>localtime</span><span class=p>()),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;status&#34;</span><span class=p>:</span> <span class=n>status</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;latency&#34;</span><span class=p>:</span> <span class=n>latency</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>stat</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>account</span> <span class=ow>in</span> <span class=n>accounts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>stat</span><span class=p>[</span><span class=n>account</span><span class=p>[</span><span class=s1>&#39;endpoint&#39;</span><span class=p>]]</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;last_updated&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;status&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;latency&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;latency_ra&#39;</span><span class=p>:</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>while</span><span class=p>(</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>account</span> <span class=ow>in</span> <span class=n>accounts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>endpoint</span> <span class=o>=</span> <span class=n>account</span><span class=p>[</span><span class=s1>&#39;endpoint&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>key</span> <span class=o>=</span> <span class=n>account</span><span class=p>[</span><span class=s1>&#39;key&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=n>get_latency_for_endpoint</span><span class=p>(</span><span class=n>endpoint</span><span class=p>,</span> <span class=n>key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>stat</span><span class=p>[</span><span class=n>endpoint</span><span class=p>][</span><span class=s1>&#39;last_updated&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;ts&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>stat</span><span class=p>[</span><span class=n>endpoint</span><span class=p>][</span><span class=s1>&#39;status&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;status&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>stat</span><span class=p>[</span><span class=n>endpoint</span><span class=p>][</span><span class=s1>&#39;latency&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;latency&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>stat</span><span class=p>[</span><span class=n>endpoint</span><span class=p>][</span><span class=s1>&#39;latency_ra&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>decay_rate</span> <span class=o>*</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;latency&#39;</span><span class=p>]</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>decay_rate</span><span class=p>)</span> <span class=o>*</span> <span class=n>stat</span><span class=p>[</span><span class=n>endpoint</span><span class=p>][</span><span class=s1>&#39;latency_ra&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>stat</span><span class=p>,</span><span class=n>indent</span><span class=o>=</span><span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=n>test_interval</span><span class=p>)</span>
</span></span></code></pre></div><p>In this script, endpoints are checked every <code>15</code> seconds, with timeouts set at <code>10</code> seconds. A rolling average with a decay rate of <code>0.8</code> is calculated.</p><p>A response from a single prompt will look like this:</p><pre tabindex=0><code>{
  &#34;id&#34;:&#34;chatcmpl-.....&#34;,
  &#34;object&#34;:&#34;chat.completion&#34;,
  &#34;created&#34;:1690872556,
  &#34;model&#34;:&#34;gpt-35-turbo&#34;,
  &#34;choices&#34;:[
    {
      &#34;index&#34;:0,
      &#34;finish_reason&#34;:&#34;length&#34;,
      &#34;message&#34;:{
        &#34;role&#34;:&#34;assistant&#34;,
        &#34;content&#34;:&#34;Hello&#34;
      }
    }
  ],
  &#34;usage&#34;:{
    &#34;completion_tokens&#34;:1,
    &#34;prompt_tokens&#34;:14,
    &#34;total_tokens&#34;:15
  }
}
</code></pre><p>Overall cost per call is 15 tokens, which would cost us <code>30 days * 24 hours * 60 minutes * 4 requests/minute * 15 tokens * $0.002 / 1000 tokens = $5.2 / month</code> per region in a month. Not sure if we need to test every every 15 seconds, or if every minute is sufficient. In terms of requests/minute, Azure OpenAI gives 1440 requests/minute per region and subscription, so sacrificing 4 calls is less than 0.3%.</p><p>Running the script over a period yields data such as:</p><pre tabindex=0><code>{
    &#34;https://canadaeast.api.cognitive.microsoft.com/&#34;: {
        &#34;last_updated&#34;: &#34;2023-08-01 09:36:25&#34;,
        &#34;status&#34;: 200,
        &#34;latency&#34;: 0.5866355895996094,
        &#34;latency_ra&#34;: 0.5867746781616211
    },
    &#34;https://eastus2.api.cognitive.microsoft.com/&#34;: {
        &#34;last_updated&#34;: &#34;2023-08-01 09:36:25&#34;,
        &#34;status&#34;: 200,
        &#34;latency&#34;: 0.5309584140777588,
        &#34;latency_ra&#34;: 0.5271010751342773
    },
    &#34;https://francecentral.api.cognitive.microsoft.com/&#34;: {
        &#34;last_updated&#34;: &#34;2023-08-01 09:36:26&#34;,
        &#34;status&#34;: 200,
        &#34;latency&#34;: 0.725212812423706,
        &#34;latency_ra&#34;: 0.6279167041015624
    },
    &#34;https://japaneast.api.cognitive.microsoft.com/&#34;: {
        &#34;last_updated&#34;: &#34;2023-08-01 09:36:27&#34;,
        &#34;status&#34;: 200,
        &#34;latency&#34;: 1.0203375816345215,
        &#34;latency_ra&#34;: 1.0150870689697267
    },
    &#34;https://northcentralus.api.cognitive.microsoft.com/&#34;: {
        &#34;last_updated&#34;: &#34;2023-08-01 09:36:28&#34;,
        &#34;status&#34;: 200,
        &#34;latency&#34;: 0.7335877418518066,
        &#34;latency_ra&#34;: 0.7090948748168945
    },
    &#34;https://uksouth.api.cognitive.microsoft.com/&#34;: {
        &#34;last_updated&#34;: &#34;2023-08-01 09:36:28&#34;,
        &#34;status&#34;: 200,
        &#34;latency&#34;: 0.2238612174987793,
        &#34;latency_ra&#34;: 0.22408719714355468
    }
}
</code></pre><p>We can clearly see that <code>japaneast</code> is the slowest, but as discussed before, latency from my machine to this region is already ~250ms, which probably explains it.</p><h2 id=moving-forward class="relative group">Moving forward <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#moving-forward aria-label=Anchor>#</a></span></h2><p>While the above script is functional, a practical application should account for:</p><ol><li><strong>Execution:</strong> The script could be executed with a timer in an Azure Function, persisting results into Azure Blob or Azure CosmosDB. The app would then query the current status periodically, caching responses and making regional choices based on current latency and the rolling average.</li><li><strong>Rate-limiting:</strong> Azure OpenAI Service defaults to 240k tokens per minute (TPMs) for gpt-35-turbo per region and subscription (as of 08/01/2023). If the test prompt encounters a limit for a region, it will be marked with status <code>429</code>. Consequently, the app should then pick the next best option.</li><li><strong>Fallback measures:</strong> In case limits are reached across most regions, ensure it&rsquo;s not because of the http timeout set for the <code>POST</code> request. In such unlikely scenarios, temporarily increase the http timeout value to identify regions still responding.</li></ol><h2 id=summary class="relative group">Summary <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#summary aria-label=Anchor>#</a></span></h2><p>This post has presented an easy approach to measure Azure OpenAI response latency across the globe. By sending a tiny prompt, waiting for its completion, and then choosing the best-performing region, we can optimize our actual API calls and hopefully minimize latency spikes.</p></div></section><footer class="pt-8 max-w-prose print:hidden"><div class=flex><img class="!mb-0 !mt-0 me-4 h-24 w-24 rounded-full" width=96 height=96 alt="Clemens Siebler" src=/images/clemens_huceff515921bfe06d8db38bd985c1ff75_12982_192x192_fill_q75_box_center.jpeg loading=lazy><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Clemens Siebler</div><div class="text-sm text-neutral-700 dark:text-neutral-400">Clemens loves being part of the tech industry where he currently helps clients to bring their Machine Learning projects to life. He currently works as a Senior Specialist for AI in the Global Black Belt at Microsoft.</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://github.com/csiebler target=_blank aria-label=Github rel="me noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://instagram.com/csiebler target=_blank aria-label=Instagram rel="me noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M224.1 141c-63.6.0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1.0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9.0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9.0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9.0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9.0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8.0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg></span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://linkedin.com/in/csiebler target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://twitter.com/clemenssiebler target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></a></div></div></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.facebook.com/sharer/sharer.php?u=https://clemenssiebler.com/posts/optimizing-latency-azure-openai/&amp;quote=Optimizing%20latency%20for%20Azure%20OpenAI%20Service" title="Share on Facebook" aria-label="Share on Facebook" target=_blank rel="noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg></span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://clemenssiebler.com/posts/optimizing-latency-azure-openai/&amp;text=Optimizing%20latency%20for%20Azure%20OpenAI%20Service" title="Tweet on Twitter" aria-label="Tweet on Twitter" target=_blank rel="noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clemenssiebler.com/posts/optimizing-latency-azure-openai/&amp;description=Optimizing%20latency%20for%20Azure%20OpenAI%20Service" title="Pin on Pinterest" aria-label="Pin on Pinterest" target=_blank rel="noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M496 256c0 137-111 248-248 248-25.6.0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8.0 128.7-68.8 128.7-154.3.0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1.0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6.0-54.7 41.4-107.6 112-107.6 60.9.0 103.6 41.5 103.6 100.9.0 67.1-33.9 113.6-78 113.6-24.3.0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6.0-19-10.2-34.9-31.4-34.9-24.9.0-44.9 25.7-44.9 60.2.0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9.0 361.1.0 256 0 119 111 8 248 8s248 111 248 248z"/></svg></span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://clemenssiebler.com/posts/optimizing-latency-azure-openai/&amp;resubmit=true&amp;title=Optimizing%20latency%20for%20Azure%20OpenAI%20Service" title="Submit to Reddit" aria-label="Submit to Reddit" target=_blank rel="noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg></span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://clemenssiebler.com/posts/optimizing-latency-azure-openai/&amp;title=Optimizing%20latency%20for%20Azure%20OpenAI%20Service" title="Share on LinkedIn" aria-label="Share on LinkedIn" target=_blank rel="noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://clemenssiebler.com/posts/optimizing-latency-azure-openai/&amp;subject=Optimizing%20latency%20for%20Azure%20OpenAI%20Service" title="Send via email" aria-label="Send via email" target=_blank rel="noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/azure-search-vector-search-openai-langchain/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Using Azure Search for vector search with Azure OpenAI and LangChain</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-07-27 00:00:00 +0000 UTC">27 July 2023</time></span></span></a></span>
<span><a class="group flex text-right" href=/posts/understanding-azure-openai-rate-limits-monitoring/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">A Guide to Azure OpenAI Service's Rate Limits and Monitoring</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-08-01 00:00:00 +0000 UTC">1 August 2023</time></span></span>
<span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2023
Clemens Siebler</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://git.io/hugo-congo target=_blank rel="noopener noreferrer">Congo</a></p></div></div></footer></div></body></html>