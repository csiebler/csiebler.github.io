<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><title>Sharing Azure OpenAI Provisioned Throughput (PTU) for multiple use cases with Azure API Management &#183; Clemens Siebler's Blog</title>
<meta name=title content="Sharing Azure OpenAI Provisioned Throughput (PTU) for multiple use cases with Azure API Management &#183; Clemens Siebler's Blog"><script type=text/javascript src=/js/appearance.min.74ad8406faea02f3e186ba5126249aaeed9073629e04b05037b903396b188724.js integrity="sha256-dK2EBvrqAvPhhrpRJiSaru2Qc2KeBLBQN7kDOWsYhyQ="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.7c1e6d40b23627ab31090b2d8fcdf80392861650296ac97e1d0d4d184dee1930.css integrity="sha256-fB5tQLI2J6sxCQstj834A5KGFlApasl+HQ1NGE3uGTA="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.3e80d0528c59616e7de9b0ca9316d9afd314b49ed9e5e6b88873731486ad67ee.js integrity="sha256-PoDQUoxZYW596bDKkxbZr9MUtJ7Z5ea4iHNzFIatZ+4=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        
      
    "><link rel=canonical href=https://clemenssiebler.com/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://clemenssiebler.com/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/"><meta property="og:site_name" content="Clemens Siebler's Blog"><meta property="og:title" content="Sharing Azure OpenAI Provisioned Throughput (PTU) for multiple use cases with Azure API Management"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-25T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-25T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Sharing Azure OpenAI Provisioned Throughput (PTU) for multiple use cases with Azure API Management"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Sharing Azure OpenAI Provisioned Throughput (PTU) for multiple use cases with Azure API Management","headline":"Sharing Azure OpenAI Provisioned Throughput (PTU) for multiple use cases with Azure API Management","inLanguage":"en","url":"https:\/\/clemenssiebler.com\/posts\/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management\/","author":{"@type":"Person","name":""},"copyrightYear":"2024","dateCreated":"2024-06-25T00:00:00\u002b00:00","datePublished":"2024-06-25T00:00:00\u002b00:00","dateModified":"2024-06-25T00:00:00\u002b00:00","mainEntityOfPage":"true","wordCount":"2498"}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-HB3D5YJ128"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-HB3D5YJ128")}</script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 sm:px-14 md:px-24 lg:px-32 dark:bg-neutral-800 dark:text-neutral"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 print:hidden sm:py-10 dark:text-neutral"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>Clemens Siebler&rsquo;s Blog</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Posts</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/talks/ title=Talks><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Talks</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">About</span></a></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Sharing Azure OpenAI Provisioned Throughput (PTU) for multiple use cases with Azure API Management</h1><div class="mb-12 mt-8 text-base text-neutral-500 print:hidden dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2024-06-25 00:00:00 +0000 UTC">25 June 2024</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">12 mins</span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#typical-scenarios>Typical scenarios</a><ul><li><a href=#scenario-1---multiple-use-cases-with-the-same-priority>Scenario 1 - Multiple use cases with the same priority</a></li><li><a href=#scenario-2---multiple-use-cases-with-different-priorities>Scenario 2 - Multiple use cases with different priorities</a><ul><li><a href=#solution-1---two-ptu-deployments>Solution 1 - Two PTU deployments</a></li><li><a href=#solution-2---using-a-time-based-approach>Solution 2 - Using a time-based approach</a></li><li><a href=#solution-3---token-limiting>Solution 3 - Token Limiting</a></li><li><a href=#solution-4---a-healthy-combination>Solution 4 - A healthy combination</a></li></ul></li></ul></li><li><a href=#implementation>Implementation</a><ul><li><a href=#enable-token-tracking-via-application-insights>Enable Token Tracking via Application Insights</a></li><li><a href=#configure-time-based-routing>Configure time-based routing</a></li><li><a href=#configure-token-limiting>Configure token limiting</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><h2 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>This post explains how multiple use cases can safely co-exist on Azure OpenAI&rsquo;s <a href=https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/provisioned-throughput target=_blank rel=noreferrer>Provisioned Throughput</a> (PTU) offering so that we can:</p><ul><li>Get higher utilization of the Provisioned Throughput resource (instead of having to buy one PTU per use case)</li><li>Track token consumption per use case</li><li>Ensure use cases don&rsquo;t step on each others&rsquo; toes</li><li>Prioritize certain use cases by<ul><li>Limiting their throughput</li><li>Limiting their access to PTUs during certain times of the day or days of the week</li></ul></li></ul><p>And obviously, our goal is to make this work for both streaming and non-streaming chat completion calls.</p><p>This post is a continuation of my previous post <a href=https://clemenssiebler.com/posts/smart-loadbalancing-for-azure-openai-with-api-management/ target=_blank rel=noreferrer>Smart Load-Balancing for Azure OpenAI with Azure API Management</a> and builds upon the discussion there.</p><h2 id=typical-scenarios class="relative group">Typical scenarios <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#typical-scenarios aria-label=Anchor>#</a></span></h2><p>So when planning to run multiple use cases on one PTU deployment, our main motivation is typically to save cost. At the same time, we need to make sure that the benefits of the PTU (low latency responses, high throughput) are not lost due to use cases aggressively &ldquo;eating up&rdquo; the whole throughput capacity (noisy neighbors).</p><p>Hence, when sharing one PTU deployment across multiple use cases we should differentiate between two scenarios:</p><ol><li>All use cases have the same priority</li><li>Some use cases have lower priority than others (therefore could potentially become noisy neighbors)</li></ol><p>For example, if we have two customer-facing chatbots or copilots, we want the same, quick, real-time response behavior for both of them. Both use cases have likely the same priority and therefore could run on the same PTU. In case we run out of throughput capacity, we&rsquo;d surely need scale up, or in case we&rsquo;re underutilized, we could add more similar use cases or scale down. Easy!</p><p>However, let&rsquo;s say we have one batch use case that processes a lot of documents and one customer-facing chatbot. The batch processing workload could easily eat up the whole PTU throughput capacity during main business hours, which would obviously defeat most of the value prop of the PTU. Hence, we need to be a bit more careful in this scenario.</p><p>So what we&rsquo;ll do now is to discuss those two scenarios as they pose different architectural challenges.</p><h3 id=scenario-1---multiple-use-cases-with-the-same-priority class="relative group">Scenario 1 - Multiple use cases with the same priority <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#scenario-1---multiple-use-cases-with-the-same-priority aria-label=Anchor>#</a></span></h3><p>This scenario assumes that all use cases sharing the PTU have similar priority. In most cases, this means all use cases have the same quick, real-time response requirements, such as e.g., end-user facing chatbots or copilots and therefore want to use the PTU for low-latency responses. Azure OpenAI PAYGO resources should only be used to economically handle occasional peaks.</p><p>One way to address this scenario is through the following architecture:</p><p><figure><img src=/images/multiple_use_cases_same_priority.png alt="Multiple use cases with the same priority" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Multiple use cases with the same priority</figcaption></figure></p><p>We use a single instance of API Management (APIM) with a single API endpoint. All use cases connect to that one endpoint using their own APIM subscription keys. We do this so we can later track tokens per subscription key, and therefore per use case. This can be useful for showback or chargeback within the organization. To achieve this technically, APIM is configured to emit token metrics per use case to Application Insights, where they are stored for later analysis. With a simple Log Analytics query, we can then get the consumed tokens per subscription key for any desired time frame.</p><p>Behind APIM, we leverage multiple Azure OpenAI resources. In our example we configure:</p><ul><li>An AOAI resource that hosts our PTU (priority 1) - This means all traffic should go to the PTU first, as long as there are no <code>429</code> errors. This makes sure that as many requests as possible are processed by the PTU, which enables fast responses and drives up utilization, hence saves money.</li><li>An AOAI resource with PAYGO (priority 2) - This is a PAYGO deployment within the same region as the PTU, but in a separate AOAI resource. This allows us to have all model deployments be called the same, e.g. <code>gpt-4o</code>. In case the PTU is out of throughput, we can fall back to this PAYGO instance first. Surely, this means slower response times, but gives us a safety net and also allows us to deal with occasional peaks for which the PTU might not be economically feasible.</li><li>An AOAI resource with PAYGO (priority 3) - This is another PAYGO deployment, but within a different region as the prior two resources. This is to cover the case that our PTU and the PAYGO resource (either only our resource or the whole data center) are at capacity. This is our double safety net and ideally, this resource should never get called.</li></ul><p>Lastly, we track the PTU utilization in Azure Monitor. This allows us to see if we still have good headroom or if it is potentially time to scale up our PTU deployment (i.e., buy more capacity). At the same time, this also gives us an indicator if we have too many PTUs deployed and could potentially scale down or deploy more use cases on them.</p><h3 id=scenario-2---multiple-use-cases-with-different-priorities class="relative group">Scenario 2 - Multiple use cases with different priorities <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#scenario-2---multiple-use-cases-with-different-priorities aria-label=Anchor>#</a></span></h3><p>In this scenario we assume that our use cases have different latency requirements:</p><ul><li>Some might require fast responses (often customer-facing)</li><li>Some use cases maybe might not care too much (but won&rsquo;t complain if they are fast too), and</li><li>Some purely would use PTU to save money (e.g., batch workloads)</li></ul><p>Here, the core idea is to utilize the PTU as much as possible (save cost) but without creating a noisy neighbor problem. This could easily arise if a large batch workload is starting to send many API requests to the PTU, thus using up all its capacity. A latency sensitive use case then might need to revert back to PAYGO, thus suffering from slower response times. This scenario is hard to solve, as we typically do not know when the low-priority workload is starting (surely, in same cases we are in control of it).</p><h4 id=solution-1---two-ptu-deployments class="relative group">Solution 1 - Two PTU deployments <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#solution-1---two-ptu-deployments aria-label=Anchor>#</a></span></h4><p>As said, solving this scenario <strong>perfectly</strong> is not easy. However, we can make some tradeoffs and get to a reasonably good solution using this simple architecture:</p><p><figure><img src=/images/multiple_use_cases_two_ptus.png alt="Serving multiple use cases with two PTUs" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Serving multiple use cases with two PTUs</figcaption></figure></p><p>Again, we use a single APIM instance but we deploy two API endpoints:</p><ul><li>One endpoint for high-priority workloads</li><li>One endpoint for low-priority workloads</li></ul><p>(Alternatively we could also use one single endpoint that routes based on the APIM subscription keys, but this requires a bit more coding and configuration!)</p><p>Same as before, all use cases connect to their respective endpoints using their own APIM subscription key. Within APIM, we can limit API access for the low-priority use case to just be allowed to access the low-priority endpoint.</p><p>Now the main idea here is that instead of using one shared PTU resource, we split it up into two AOAI resources, each holding half of the PTUs. This obviously requires twice the capacity, so for very small workloads this might not be economically feasible. However, with models becoming cheaper and faster, it is more feasible than ever. Now to architect this, we define the following access pattern:</p><ul><li>Our high-priority endpoint gets access both PTUs</li><li>Our low-priority endpoint gets access only one PTU</li></ul><p>This effectively means that:</p><ul><li>Our high-priority workloads can leverage up to 100% of the overall PTU capacity, but are guaranteed at least 50%</li><li>Our low-priority workloads can leverage up to 50% of the overall PTU capacity</li></ul><p>Obviously, these percentage numbers can be tweaked, as long as each PTU has the minimum amounts of PTUs required for a given model.</p><p>Behind APIM, we configure the following Azure OpenAI resources:</p><p><strong>High-priority endpoint</strong></p><ul><li>An AOAI resource that hosts the first PTU (priority 1)</li><li>An AOAI resource that hosts the second PTU (priority 1) - This should be in the same region, but requires an additional AOAI resource, so that both PTU resources can name their model deployments the same (e.g., <code>gpt-4o</code>)</li><li>An AOAI resource with PAYGO (priority 2) - For fallback to PAYGO within the same region</li></ul><p><strong>Low-priority endpoint</strong></p><ul><li>Points to the AOAI resource that hosts the second PTU (priority 1)</li><li>Points to the AOAI resource with PAYGO (priority 2)</li></ul><p>Both endpoints should also have a secondary PAYGO resource in a different region as a secondary fall back (as discussed earlier), but for sake of simplicity this is not shown here.</p><p>Yes, this architecture is by no means perfect (requires at least 2 PTU deployments to get started), but it is easy and quick to implement, and yet flexible enough to handle growth over time.</p><h4 id=solution-2---using-a-time-based-approach class="relative group">Solution 2 - Using a time-based approach <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#solution-2---using-a-time-based-approach aria-label=Anchor>#</a></span></h4><p>Often enough, our use cases show different usage patterns during the course of a day. For example, a chatbot might get used heavily from 8am to 10pm, but does not receive much traffic during the night. At the same time, batch workloads might be happy to primarily run at night, especially when they are not time-critical. We can leverage these day/night patterns to our advantage and limit access to PTUs based on time.</p><p>To safely enable this scenario, we can deploy the following architecture:</p><p><figure><img src=/images/multiple_use_cases_time_based.png alt="Serving multiple use cases with a time-based routing" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Serving multiple use cases with a time-based routing</figcaption></figure></p><p>Again, we have two endpoints:</p><ul><li>Our high-priority endpoint that can access all AOAI resources at any time</li><li>Our low-priority endpoint that can only access the PTU during certain times of day or days of the week</li></ul><p>With this, we ensure safety for our high-priority workload(s) if a low-priority use case needs to get a workload done during the day (will be routed to PAYGO), but at night it may use the PTU resource as much as needed. Surely, it could still fully hog it, but at least it wouldn&rsquo;t happen during core business hours.</p><h4 id=solution-3---token-limiting class="relative group">Solution 3 - Token Limiting <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#solution-3---token-limiting aria-label=Anchor>#</a></span></h4><p>One additional solution is to limit the amount of tokens a single use case can consume per minute (measured in Tokens per Minute - TPM).</p><p>For example, we could say:</p><ul><li>High-priority use cases: No limitations</li><li>Low-priority use cases: 10k TPMs/use case</li></ul><p>Using the <a href=https://oai.azure.com/portal/calculator target=_blank rel=noreferrer>Azure OpenAI Capacity calculator</a>, we can roughly estimate how much throughput in terms of TPMs we get for a given model per PTU. With this, we can make an educated decision how many TPMs we want to grand to the low-priority use cases. The architecture looks very simple:</p><p><figure><img src=/images/tpm_limits_per_subscription_key.png alt="Token limits per subscription key" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Token limits per subscription key</figcaption></figure></p><p>However, this token limit is implemented on the APIM layer, thus it is evaluated as the requests come. This is <strong>before</strong> APIM routes them to the PTU or PAYGO backends. This means that the token limit is independent of the backends and does not exclusively restrict PTU access, but also potentially PAYGO access.</p><h4 id=solution-4---a-healthy-combination class="relative group">Solution 4 - A healthy combination <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#solution-4---a-healthy-combination aria-label=Anchor>#</a></span></h4><p>Lastly, we may combine multiple approaches into one. For example we can combine a time-based approach with the multiple PTU approach:</p><p><figure><img src=/images/multiple_use_cases_combination.png alt="Combining both approaches" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Combining both approaches</figcaption></figure></p><p>This ensures that during business hours our PTUs can only be used by high-priority use cases. Outside of the business hours, we nevertheless offer some part of them to the low-priority use cases, thus saving cost, while still ensuring smooth operations for occasional requests that are high priority.</p><p>Furthermore, we also could combine this with token limiting, but for sake of simplicity this is not shown here.</p><h2 id=implementation class="relative group">Implementation <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implementation aria-label=Anchor>#</a></span></h2><p>Let&rsquo;s discuss how these scenarios can be implemented with APIM.</p><h3 id=enable-token-tracking-via-application-insights class="relative group">Enable Token Tracking via Application Insights <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#enable-token-tracking-via-application-insights aria-label=Anchor>#</a></span></h3><p>To start tracking token consumption per inference call, we need to first add Application Insights to our APIM instance:</p><p><figure><img src=/images/add_appinsights_to_apim.png alt="Add Application Insights to our APIM" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Add Application Insights to our APIM</figcaption></figure></p><p>We can disable both settings, as we do not need them:</p><ul><li>Use as default and enable basic logging for all APIs</li><li>Add availability monitor</li></ul><p>Next, we want to enable Custom Metrics in Application Insights, otherwise Application Insights won&rsquo;t accept the incoming token consumption feed from APIM:</p><p><figure><img src=/images/enable_custom_metrics.png alt="Enable Custom Metrics in Application Insights" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Enable Custom Metrics in Application Insights</figcaption></figure></p><p>We need to enable <code>With Dimension</code> so we get all the data.</p><p>Now, we need to go to <code>All APIs</code> in APIM, select <code>Settings</code> and enable Application Insights for all requests:</p><p><figure><img src=/images/apim_settings.png alt="Configure APIM to log all requests" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Configure APIM to log all requests</figcaption></figure></p><p>It is important to set:</p><ul><li>Sampling to 100% (otherwise we&rsquo;ll only track a fraction of the tokens)</li><li>Verbosity to <code>Information</code> (if we only log errors, we won&rsquo;t see tokens for the successfully finished calls)</li></ul><p>Next, we want to update our APIM policy to emit the token metric (paste it at the end of the <code>&lt;inbound></code> section):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;azure-openai-emit-token-metric&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;dimension</span> <span class=na>name=</span><span class=s>&#34;SubscriptionId&#34;</span> <span class=na>value=</span><span class=s>&#34;@(context.Subscription.Id)&#34;</span> <span class=nt>/&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/azure-openai-emit-token-metric&gt;</span>
</span></span></code></pre></div><p>A complete policy example can be <a href=https://gist.github.com/csiebler/7542631f9b9b837b1e1c654e56626d56#file-policy-with-token-tracking-xml target=_blank rel=noreferrer>found here</a>.</p><p>Then, we can configure multiple subscription keys. In this case, the <code>name</code> will be used during logging of the tokens:</p><p><figure><img src=/images/apim_subscriptions.png alt="Configuration of multiple subscription keys" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Configuration of multiple subscription keys</figcaption></figure></p><p>Lastly, after firing a few API calls, we can go Log Analytics and query our <code>customMetrics</code> table:</p><pre tabindex=0><code class=language-kql data-lang=kql>customMetrics
| extend subId = tostring(parse_json(customDimensions).SubscriptionId)
| summarize totalTokens = sum(valueSum), totalCalls = sum(valueCount) by name, subId
</code></pre><p>If everything worked out, we should see the token consumption per subscription name:</p><p><figure><img src=/images/tokens_per_apim_subscription_key.png alt="Configure APIM to log all requests" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Tokens per APIM</figcaption></figure></p><h3 id=configure-time-based-routing class="relative group">Configure time-based routing <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#configure-time-based-routing aria-label=Anchor>#</a></span></h3><p>To configure time-based routing, we need to add a new attribute in our endpoint configuration to indicate that a certain resource (e.g., PTU) is not allowed during certain time-windows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-js data-lang=js><span class=line><span class=cl><span class=nx>backends</span><span class=p>.</span><span class=nx>Add</span><span class=p>(</span><span class=k>new</span> <span class=nx>JObject</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;url&#34;</span><span class=p>,</span> <span class=s2>&#34;https://abcdefg.openai.azure.com/&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;priority&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;isThrottling&#34;</span><span class=p>,</span> <span class=kc>false</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;retryAfter&#34;</span><span class=p>,</span> <span class=nx>DateTime</span><span class=p>.</span><span class=nx>MinValue</span> <span class=p>}</span> <span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;hasTimeRestriction&#34;</span><span class=p>,</span> <span class=kc>true</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>});</span>
</span></span></code></pre></div><p>And then in our backend selection code, we check if the current time falls outside the defined window:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-js data-lang=js><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=o>&lt;</span><span class=nx>set</span><span class=o>-</span><span class=nx>variable</span> <span class=nx>name</span><span class=o>=</span><span class=s2>&#34;backendIndex&#34;</span> <span class=nx>value</span><span class=o>=</span><span class=s2>&#34;@{
</span></span></span><span class=line><span class=cl><span class=s2>    // define during which times we want the PTU to be accessible
</span></span></span><span class=line><span class=cl><span class=s2>    bool IsWithinAllowedHours()
</span></span></span><span class=line><span class=cl><span class=s2>    {
</span></span></span><span class=line><span class=cl><span class=s2>        TimeZoneInfo timeZoneInfo = TimeZoneInfo.FindSystemTimeZoneById(&#34;</span><span class=nx>W</span><span class=p>.</span> <span class=nx>Europe</span> <span class=nx>Standard</span> <span class=nx>Time</span><span class=s2>&#34;);  
</span></span></span><span class=line><span class=cl><span class=s2>        DateTime currentTime = TimeZoneInfo.ConvertTimeFromUtc(DateTime.UtcNow, timeZoneInfo);  
</span></span></span><span class=line><span class=cl><span class=s2>        TimeSpan startTime = new TimeSpan(8, 0, 0);
</span></span></span><span class=line><span class=cl><span class=s2>        TimeSpan endTime = new TimeSpan(22, 0, 0);
</span></span></span><span class=line><span class=cl><span class=s2>        return !(currentTime.TimeOfDay &gt;= startTime &amp;&amp; currentTime.TimeOfDay &lt;= endTime);
</span></span></span><span class=line><span class=cl><span class=s2>    }
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    JArray backends = (JArray)context.Variables[&#34;</span><span class=nx>listBackends</span><span class=s2>&#34;];
</span></span></span><span class=line><span class=cl><span class=s2>    bool isWithinAllowedHours = IsWithinAllowedHours();
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    int selectedPriority = Int32.MaxValue;
</span></span></span><span class=line><span class=cl><span class=s2>    List&lt;int&gt; availableBackends = new List&lt;int&gt;();
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    for (int i = 0; i &lt; backends.Count; i++)
</span></span></span><span class=line><span class=cl><span class=s2>    {
</span></span></span><span class=line><span class=cl><span class=s2>        JObject backend = (JObject)backends[i];
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        if (!backend.Value&lt;bool&gt;(&#34;</span><span class=nx>isThrottling</span><span class=s2>&#34;))
</span></span></span><span class=line><span class=cl><span class=s2>        {
</span></span></span><span class=line><span class=cl><span class=s2>            int backendPriority = backend.Value&lt;int&gt;(&#34;</span><span class=nx>priority</span><span class=s2>&#34;);
</span></span></span><span class=line><span class=cl><span class=s2>            bool hasTimeRestriction = backend.Value&lt;bool?&gt;(&#34;</span><span class=nx>hasTimeRestriction</span><span class=err>&#34;</span><span class=p>)</span> <span class=o>??</span> <span class=kc>false</span><span class=p>;</span>  
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1>// Check the time condition for the backend with time restriction  
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>if</span> <span class=p>(</span><span class=nx>hasTimeRestriction</span> <span class=o>&amp;&amp;</span> <span class=o>!</span><span class=nx>isWithinAllowedHours</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>  
</span></span><span class=line><span class=cl>                <span class=k>continue</span><span class=p>;</span> <span class=c1>// Skip this backend if it&#39;s restricted by time and we&#39;re not within the allowed hours  
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=p>}</span>  
</span></span><span class=line><span class=cl><span class=p>...</span>
</span></span></code></pre></div><p>A full policy example can be <a href=https://gist.github.com/csiebler/7542631f9b9b837b1e1c654e56626d56#file-policy-with-time-restriction-xml target=_blank rel=noreferrer>found here</a>.</p><h3 id=configure-token-limiting class="relative group">Configure token limiting <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#configure-token-limiting aria-label=Anchor>#</a></span></h3><p>To limit the amount of tokens a use case can consume per minute, we can leverage the new <code>azure-openai-token-limit</code> statement in APIM. So in our <code>&lt;inbound></code> policy, we can use an <code>choose</code> statement to define an <code>azure-openai-token-limit</code> per subscription id:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;choose&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;when</span> <span class=na>condition=</span><span class=s>&#34;@(context.Subscription.Id.Equals(&#34;</span><span class=err>UseCase1&#34;))&#34;</span><span class=nt>&gt;</span>
</span></span><span class=line><span class=cl>        <span class=nt>&lt;azure-openai-token-limit</span> <span class=na>counter-key=</span><span class=s>&#34;@(context.Subscription.Id)&#34;</span> <span class=na>tokens-per-minute=</span><span class=s>&#34;50000&#34;</span> <span class=na>estimate-prompt-tokens=</span><span class=s>&#34;false&#34;</span> <span class=na>remaining-tokens-header-name=</span><span class=s>&#34;x-tokens-remaining&#34;</span> <span class=na>tokens-consumed-header-name=</span><span class=s>&#34;x-tokens-consumed&#34;</span> <span class=na>remaining-tokens-variable-name=</span><span class=s>&#34;remainingTokens&#34;</span> <span class=nt>/&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;/when&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;when</span> <span class=na>condition=</span><span class=s>&#34;@(context.Subscription.Id.Equals(&#34;</span><span class=err>UseCase2&#34;))&#34;</span><span class=nt>&gt;</span>
</span></span><span class=line><span class=cl>        <span class=nt>&lt;azure-openai-token-limit</span> <span class=na>counter-key=</span><span class=s>&#34;@(context.Subscription.Id)&#34;</span> <span class=na>tokens-per-minute=</span><span class=s>&#34;10000&#34;</span> <span class=na>estimate-prompt-tokens=</span><span class=s>&#34;false&#34;</span> <span class=na>remaining-tokens-header-name=</span><span class=s>&#34;x-tokens-remaining&#34;</span> <span class=na>tokens-consumed-header-name=</span><span class=s>&#34;x-tokens-consumed&#34;</span> <span class=na>remaining-tokens-variable-name=</span><span class=s>&#34;remainingTokens&#34;</span> <span class=nt>/&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;/when&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/choose&gt;</span>
</span></span></code></pre></div><p>In this example, use case 1 receives 50k TPMs, while use case 2 only receives 10k TPMs. If we do not specify a limit for a given subscription, it will just give it unlimited throughput. And yes, this code heavily should be made much less repetitive!</p><h2 id=summary class="relative group">Summary <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#summary aria-label=Anchor>#</a></span></h2><p>Sharing multiple use cases on one PTU deployment is definitively possible, but requires good planning and also making a few tradeoffs. In this post we&rsquo;ve went through several approaches, that together, can get us to a (hopefully) well functioning solution.</p></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=flex><div class=place-self-center><div class="text-2xl sm:text-lg"></div></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.facebook.com/sharer/sharer.php?u=https://clemenssiebler.com/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/&amp;quote=Sharing%20Azure%20OpenAI%20Provisioned%20Throughput%20%28PTU%29%20for%20multiple%20use%20cases%20with%20Azure%20API%20Management" title="Share on Facebook" aria-label="Share on Facebook" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://clemenssiebler.com/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/&amp;text=Sharing%20Azure%20OpenAI%20Provisioned%20Throughput%20%28PTU%29%20for%20multiple%20use%20cases%20with%20Azure%20API%20Management" title="Tweet on Twitter" aria-label="Tweet on Twitter" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clemenssiebler.com/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/&amp;description=Sharing%20Azure%20OpenAI%20Provisioned%20Throughput%20%28PTU%29%20for%20multiple%20use%20cases%20with%20Azure%20API%20Management" title="Pin on Pinterest" aria-label="Pin on Pinterest" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M496 256c0 137-111 248-248 248-25.6.0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8.0 128.7-68.8 128.7-154.3.0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1.0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6.0-54.7 41.4-107.6 112-107.6 60.9.0 103.6 41.5 103.6 100.9.0 67.1-33.9 113.6-78 113.6-24.3.0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6.0-19-10.2-34.9-31.4-34.9-24.9.0-44.9 25.7-44.9 60.2.0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9.0 361.1.0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://clemenssiebler.com/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/&amp;resubmit=true&amp;title=Sharing%20Azure%20OpenAI%20Provisioned%20Throughput%20%28PTU%29%20for%20multiple%20use%20cases%20with%20Azure%20API%20Management" title="Submit to Reddit" aria-label="Submit to Reddit" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://clemenssiebler.com/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/&amp;title=Sharing%20Azure%20OpenAI%20Provisioned%20Throughput%20%28PTU%29%20for%20multiple%20use%20cases%20with%20Azure%20API%20Management" title="Share on LinkedIn" aria-label="Share on LinkedIn" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://clemenssiebler.com/posts/sharing-azure-openai-provisioned-throughput-ptu-multiple-use-cases-api-management/&amp;subject=Sharing%20Azure%20OpenAI%20Provisioned%20Throughput%20%28PTU%29%20for%20multiple%20use%20cases%20with%20Azure%20API%20Management" title="Send via email" aria-label="Send via email" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/understanding-azure-openai-x-ratelimit-remaining-tokens-x-ratelimit-remaining-requests-headers/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Understanding Azure OpenAI's x-ratelimit-remaining-tokens and x-ratelimit-remaining-requests headers</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-06-05 00:00:00 +0000 UTC">5 June 2024</time>
</span></span></a></span><span><a class="group flex text-right" href=/posts/versioning-azure-openai-endpoints-behind-api-management/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Versioning Azure OpenAI Endpoints behind Azure API Management</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-06-27 00:00:00 +0000 UTC">27 June 2024</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".prose img"));images.forEach(e=>{mediumZoom(e,{margin:5,scrollOffset:40,container:null,template:null})})</script></footer></div></body></html>