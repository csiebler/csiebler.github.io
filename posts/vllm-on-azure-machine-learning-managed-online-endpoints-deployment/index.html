<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><title>Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints &#183; Clemens Siebler's Blog</title>
<meta name=title content="Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints &#183; Clemens Siebler's Blog"><script type=text/javascript src=/js/appearance.min.74ad8406faea02f3e186ba5126249aaeed9073629e04b05037b903396b188724.js integrity="sha256-dK2EBvrqAvPhhrpRJiSaru2Qc2KeBLBQN7kDOWsYhyQ="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.7c1e6d40b23627ab31090b2d8fcdf80392861650296ac97e1d0d4d184dee1930.css integrity="sha256-fB5tQLI2J6sxCQstj834A5KGFlApasl+HQ1NGE3uGTA="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.3e80d0528c59616e7de9b0ca9316d9afd314b49ed9e5e6b88873731486ad67ee.js integrity="sha256-PoDQUoxZYW596bDKkxbZr9MUtJ7Z5ea4iHNzFIatZ+4=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        
      
    "><link rel=canonical href=https://clemenssiebler.com/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://clemenssiebler.com/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/"><meta property="og:site_name" content="Clemens Siebler's Blog"><meta property="og:title" content="Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-09T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-09T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints","headline":"Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints","inLanguage":"en","url":"https:\/\/clemenssiebler.com\/posts\/vllm-on-azure-machine-learning-managed-online-endpoints-deployment\/","author":{"@type":"Person","name":""},"copyrightYear":"2024","dateCreated":"2024-10-09T00:00:00\u002b00:00","datePublished":"2024-10-09T00:00:00\u002b00:00","dateModified":"2024-10-09T00:00:00\u002b00:00","mainEntityOfPage":"true","wordCount":"1826"}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-HB3D5YJ128"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-HB3D5YJ128")}</script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 sm:px-14 md:px-24 lg:px-32 dark:bg-neutral-800 dark:text-neutral"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 print:hidden sm:py-10 dark:text-neutral"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>Clemens Siebler&rsquo;s Blog</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Posts</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/talks/ title=Talks><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Talks</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">About</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=https://github.com/csiebler title target=_blank><span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=https://clemenssiebler.com/posts/index.xml title target=_blank><span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"></span></a></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Deploying vLLM models on Azure Machine Learning with Managed Online Endpoints</h1><div class="mb-12 mt-8 text-base text-neutral-500 print:hidden dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2024-10-09 00:00:00 +0000 UTC">9 October 2024</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">9 mins</span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#huggingface-model-deployment>HuggingFace Model Deployment</a><ul><li><a href=#step-1-create-a-custom-environment-for-vllm-on-azureml>Step 1: Create a custom Environment for vLLM on AzureML</a></li><li><a href=#step-2-deploy-the-azureml-managed-online-endpoint>Step 2: Deploy the AzureML Managed Online Endpoint</a></li><li><a href=#step-3-testing-the-deployment>Step 3: Testing the deployment</a></li></ul></li><li><a href=#custom-model-deployment>Custom Model Deployment</a><ul><li><a href=#step-1-create-a-custom-environment-for-vllm-on-azureml-1>Step 1: Create a custom Environment for vLLM on AzureML</a></li><li><a href=#step-2-register-custom-model-in-model-registry>Step 2: Register custom model in Model Registry</a></li><li><a href=#step-3-deploy-the-azureml-managed-online-endpoint>Step 3: Deploy the AzureML Managed Online Endpoint</a></li><li><a href=#step-4---testing-the-deployment>Step 4 - Testing the deployment</a></li></ul></li><li><a href=#autoscaling-our-vllm-endpoint>Autoscaling our vLLM endpoint</a></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><h1 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h1><p>In this post, we&rsquo;ll explain how to deploy LLMs on vLLM using Azure Machine Learning&rsquo;s Managed Online Endpoints for efficient, scalable, and secure real-time inference. To get started, let&rsquo;s briefly look into what vLLM and Managed Online Endpoints are.</p><p>You can find the full code examples on <a href=https://github.com/csiebler/vllm-on-azure-machine-learning target=_blank rel=noreferrer>csiebler/vllm-on-azure-machine-learning</a>.</p><h1 id=introduction-to-vllm class="relative group">Introduction to vLLM <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction-to-vllm aria-label=Anchor>#</a></span></h1><p><a href=https://github.com/vllm-project/vllm target=_blank rel=noreferrer>vLLM</a> is a high-throughput and memory-efficient inference and serving engine designed for large language models (LLMs). It optimizes the serving and execution of LLMs by utilizing advanced memory management techniques, such as PagedAttention, which efficiently manages attention key and value memory. This allows for continuous batching of incoming requests and fast model execution, making vLLM a powerful tool for deploying and serving LLMs at scale.</p><p>vLLM supports seamless integration with popular Hugging Face models and offers various decoding algorithms, including parallel sampling and beam search. It also supports tensor parallelism and pipeline parallelism for distributed inference, making it a flexible and easy-to-use solution for LLM inference (see <a href=https://docs.vllm.ai/en/latest/ target=_blank rel=noreferrer>full docs</a>).</p><h1 id=managed-online-endpoints-in-azure-machine-learning class="relative group">Managed Online Endpoints in Azure Machine Learning <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#managed-online-endpoints-in-azure-machine-learning aria-label=Anchor>#</a></span></h1><p>Managed Online Endpoints in Azure Machine Learning provide a streamlined and scalable way to deploy machine learning models for real-time inference. These endpoints handle the complexities of serving, scaling, securing, and monitoring models, allowing us to focus on building and improving your models without worrying about infrastructure management.</p><h2 id=huggingface-model-deployment class="relative group">HuggingFace Model Deployment <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#huggingface-model-deployment aria-label=Anchor>#</a></span></h2><p>Let&rsquo;s go through deploying a HuggingFace model on Azure Machine Learning&rsquo;s Managed Online Endpoints. For this, we&rsquo;ll use a custom Dockerfile and configuration files to set up the deployment. As a model, we&rsquo;ll be using <a href=https://huggingface.co/meta-llama/Llama-3.2-11B-Vision target=_blank rel=noreferrer>meta-llama/Llama-3.2-11B-Vision</a> on a single <code>Standard_NC24ads_A100_v4</code> instance.</p><h3 id=step-1-create-a-custom-environment-for-vllm-on-azureml class="relative group">Step 1: Create a custom Environment for vLLM on AzureML <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-1-create-a-custom-environment-for-vllm-on-azureml aria-label=Anchor>#</a></span></h3><p>First, we create a <code>Dockerfile</code> to define the environment for our model. For this, we&rsquo;ll be using vllm&rsquo;s base container that has all the dependencies and drivers included:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>FROM</span><span class=s> vllm/vllm-openai:latest</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> MODEL_NAME facebook/opt-125m<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENTRYPOINT</span> python3 -m vllm.entrypoints.openai.api_server --model <span class=nv>$MODEL_NAME</span> <span class=nv>$VLLM_ARGS</span><span class=err>
</span></span></span></code></pre></div><p>The idea here is that we can pass a model name via an ENV variable, so that we can easily define which model we want to deploy during deployment time.</p><p>Next, we log into our Azure Machine Learning workspace:</p><pre tabindex=0><code class=language-cli data-lang=cli>az account set --subscription &lt;subscription ID&gt;
az configure --defaults workspace=&lt;Azure Machine Learning workspace name&gt; group=&lt;resource group&gt;
</code></pre><p>Now, we create an <code>environment.yml</code> file to specify the environment settings:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>$schema</span><span class=p>:</span><span class=w> </span><span class=l>https://azuremlschemas.azureedge.net/latest/environment.schema.json</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vllm</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>dockerfile_path</span><span class=p>:</span><span class=w> </span><span class=l>Dockerfile</span><span class=w>
</span></span></span></code></pre></div><p>Then let&rsquo;s build the environment:</p><pre tabindex=0><code class=language-cli data-lang=cli>az ml environment create -f environment.yml
</code></pre><h3 id=step-2-deploy-the-azureml-managed-online-endpoint class="relative group">Step 2: Deploy the AzureML Managed Online Endpoint <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-2-deploy-the-azureml-managed-online-endpoint aria-label=Anchor>#</a></span></h3><p>Time for deployment, so let&rsquo;s first create an <code>endpoint.yml</code> file to define the Managed Online Endpoint:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>$schema</span><span class=p>:</span><span class=w> </span><span class=l>https://azuremlsdk2.blob.core.windows.net/latest/managedOnlineEndpoint.schema.json</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vllm-hf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>auth_mode</span><span class=p>:</span><span class=w> </span><span class=l>key</span><span class=w>
</span></span></span></code></pre></div><p>Let&rsquo;s create it:</p><pre tabindex=0><code class=language-cli data-lang=cli>az ml online-endpoint create -f endpoint.yml
</code></pre><p>For the next step, we&rsquo;ll need the address of the Docker image address we created. We can quickly get it from AzureML Studio -> Environments -> vllm:</p><p><figure><img src=/images/vllm_docker_image_link.png alt="Docker Image address" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Docker Image address</figcaption></figure></p><p>Finally, we create a <code>deployment.yml</code> file to configure the deployment settings and deploy our desired model from HuggingFace via vLLM:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>$schema</span><span class=p>:</span><span class=w> </span><span class=l>https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>current</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>endpoint_name</span><span class=p>:</span><span class=w> </span><span class=l>vllm-hf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>environment_variables</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>MODEL_NAME</span><span class=p>:</span><span class=w> </span><span class=l>meta-llama/Llama-3.2-11B-Vision</span><span class=w> </span><span class=c># define the model name using the identifier from HG</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>VLLM_ARGS</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;--max-num-seqs 16 --enforce-eager&#34;</span><span class=w> </span><span class=c># optional args for vLLM runtime</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>HUGGING_FACE_HUB_TOKEN</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Your HF token&gt;</span><span class=w> </span><span class=c># use this, if you want to authenticate to HF</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>xxxxxx.azurecr.io/azureml/azureml_xxxxxxxx</span><span class=w> </span><span class=c># paste Docker image address here</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>inference_config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>liveness_route</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/health</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>readiness_route</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/health</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>scoring_route</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>instance_type</span><span class=p>:</span><span class=w> </span><span class=l>Standard_NC24ads_A100_v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>instance_count</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>request_settings</span><span class=p>:</span><span class=w> </span><span class=c># This section is optional, yet important for optimizing throughput</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>max_concurrent_requests_per_instance</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>request_timeout_ms</span><span class=p>:</span><span class=w> </span><span class=m>10000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>liveness_probe</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>initial_delay</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>period</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>timeout</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>success_threshold</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>failure_threshold</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>readiness_probe</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>initial_delay</span><span class=p>:</span><span class=w> </span><span class=m>120</span><span class=w> </span><span class=c># wait for 120s before we start probing, so the model can load peacefully</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>period</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>timeout</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>success_threshold</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>failure_threshold</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span></span></span></code></pre></div><p>Since vLLM does not support separate probes for readiness and liveness, we&rsquo;ll need to make sure that the model has fully loaded before the fire the first probe. This is why we increased <code>readiness_probe.initial_delay</code> to 120s. For larger models, we should also follow <a href=https://docs.vllm.ai/en/v0.6.1/serving/distributed_serving.html target=_blank rel=noreferrer>vLLM&rsquo;s documentation</a> for using tensor parallel inference (model on single node but spanning multiple GPUs) by adding <code>--tensor-parallel-size &lt;NUM_OF_GPUs></code> to <code>VLLM_ARGS</code>. Since we&rsquo;re using a single A100 GPU in our example (<code>Standard_NC24ads_A100_v4</code>), this is not required though.</p><p>The <code>request_settings</code> depend a bit on our instance type/size and might require some manual tuning to get the model run properly and efficiently. Goal is to find a good tradeoff between concurrency (<code>max_concurrent_requests_per_instance</code>) and queue time in order to avoid either hitting <code>request_timeout_ms</code> from the endpoint side, or any HTTP-timeouts on the client side. Both these scenarios result in <code>HTTP 429</code>, and the client would need to implement exponential backoff (e.g. via <code>tenacity</code> library).</p><p>Lastly, we can deploy the model:</p><pre tabindex=0><code class=language-cli data-lang=cli>az ml online-deployment create -f deployment.yml --all-traffic
</code></pre><p>By following these steps, we have deployed a HuggingFace model on Azure Machine Learningâ€™s Managed Online Endpoints, ensuring efficient and scalable real-time inference. Time to test it!</p><h3 id=step-3-testing-the-deployment class="relative group">Step 3: Testing the deployment <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-3-testing-the-deployment aria-label=Anchor>#</a></span></h3><p>First, let&rsquo;s get the endpoint&rsquo;s scoring uri and the api keys:</p><pre tabindex=0><code class=language-cli data-lang=cli>az ml online-endpoint show -n vllm-hf
az ml online-endpoint get-credentials -n vllm-hf
</code></pre><p>For completion models, we can then call the endpoint using this Python code snippet:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>url</span> <span class=o>=</span> <span class=s2>&#34;https://vllm-hf.polandcentral.inference.ml.azure.com/v1/completions&#34;</span>
</span></span><span class=line><span class=cl><span class=n>headers</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Content-Type&#34;</span><span class=p>:</span> <span class=s2>&#34;application/json&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Authorization&#34;</span><span class=p>:</span> <span class=s2>&#34;Bearer xxxxxxxxxxxx&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=s2>&#34;meta-llama/Llama-3.2-11B-Vision&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;prompt&#34;</span><span class=p>:</span> <span class=s2>&#34;San Francisco is a&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;max_tokens&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.7</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>headers</span><span class=o>=</span><span class=n>headers</span><span class=p>,</span> <span class=n>json</span><span class=o>=</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>())</span>
</span></span></code></pre></div><p>Response:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;id&#34;</span><span class=p>:</span><span class=s2>&#34;cmpl-74bd153fff5740b3ac070e324f99494c&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;object&#34;</span><span class=p>:</span><span class=s2>&#34;text_completion&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;created&#34;</span><span class=p>:</span><span class=mi>1728460457</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;model&#34;</span><span class=p>:</span><span class=s2>&#34;meta-llama/Llama-3.2-11B-Vision&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;choices&#34;</span><span class=p>:[</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;index&#34;</span><span class=p>:</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;text&#34;</span><span class=p>:</span><span class=s2>&#34; top tourist destination known for its iconic landmarks, vibrant neighborhoods, and cultural attractions. Whether you&#39;re interested in history, art, music, or food, there&#39;s something for everyone in this amazing city. Here are some of the top things to do in San Francisco:...,&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;logprobs&#34;</span><span class=p>:</span><span class=s2>&#34;None&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;finish_reason&#34;</span><span class=p>:</span><span class=s2>&#34;length&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;stop_reason&#34;</span><span class=p>:</span><span class=s2>&#34;None&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;prompt_logprobs&#34;</span><span class=p>:</span><span class=s2>&#34;None&#34;</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>   <span class=p>],</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;usage&#34;</span><span class=p>:{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;prompt_tokens&#34;</span><span class=p>:</span><span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;total_tokens&#34;</span><span class=p>:</span><span class=mi>205</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;completion_tokens&#34;</span><span class=p>:</span><span class=mi>200</span>
</span></span><span class=line><span class=cl>   <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Works!</p><h2 id=custom-model-deployment class="relative group">Custom Model Deployment <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#custom-model-deployment aria-label=Anchor>#</a></span></h2><p>So let&rsquo;s shift gears and deploy our own fine-tuned or own even pre-trained model. How can we use vLLM to deploy this? In short, there are two options:</p><ol><li>In case of being able to upload the model to HuggingFace, we can follow the deployment steps from above or</li><li>If we want to keep the model fully private, we can directly deploy it via AzureML</li></ol><p>In this section, we&rsquo;ll discuss the second option. In order to do so, we&rsquo;ll perform the following steps:</p><ol><li>Register our custom model in Azure Machine Learning&rsquo;s Model Registry</li><li>Create a custom vLLM container that supports local model loading</li><li>Deploy the model to Managed Online Endpoints</li></ol><h3 id=step-1-create-a-custom-environment-for-vllm-on-azureml-1 class="relative group">Step 1: Create a custom Environment for vLLM on AzureML <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-1-create-a-custom-environment-for-vllm-on-azureml-1 aria-label=Anchor>#</a></span></h3><p>First, let&rsquo;s create a custom vLLM <code>Dockerfile</code> that takes a <code>MODEL_PATH</code> as input. This path will be used by AzureML to mount our custom model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>FROM</span><span class=s> vllm/vllm-openai:latest</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> MODEL_PATH <span class=s2>&#34;/models/opt-125m&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENTRYPOINT</span> python3 -m vllm.entrypoints.openai.api_server --model <span class=nv>$MODEL_PATH</span> <span class=nv>$VLLM_ARGS</span><span class=err>
</span></span></span></code></pre></div><p>Then, let&rsquo;s log into our Azure Machine Learning workspace:</p><pre tabindex=0><code class=language-cli data-lang=cli>az account set --subscription &lt;subscription ID&gt;
az configure --defaults workspace=&lt;Azure Machine Learning workspace name&gt; group=&lt;resource group&gt;
</code></pre><p>Next, we create an <code>environment.yml</code> file to specify the environment settings:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>$schema</span><span class=p>:</span><span class=w> </span><span class=l>https://azuremlschemas.azureedge.net/latest/environment.schema.json</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vllm-custom-model</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>dockerfile_path</span><span class=p>:</span><span class=w> </span><span class=l>Dockerfile</span><span class=w>
</span></span></span></code></pre></div><p>Then let&rsquo;s build the environment:</p><pre tabindex=0><code class=language-cli data-lang=cli>az ml environment create -f environment.yml
</code></pre><h3 id=step-2-register-custom-model-in-model-registry class="relative group">Step 2: Register custom model in Model Registry <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-2-register-custom-model-in-model-registry aria-label=Anchor>#</a></span></h3><p>Before we continue, we need to register our model. For this, we can go to AzureML Studio, select Models, then select Register. There, we can register our model as <code>Unspecified type</code> and reference the whole folder, which contains all our model&rsquo;s artifacts:</p><p><figure><img src=/images/upload_model.png alt="Upload model folder" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Upload model folder</figcaption></figure></p><p>Next, let&rsquo;s name our model:</p><p><figure><img src=/images/name_model.png alt="Name our custom model" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Name our custom model</figcaption></figure></p><p>And check the final summary before uploading it:</p><p><figure><img src=/images/model_registration_summary.png alt="Model registration summary" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Model registration summary</figcaption></figure></p><p>The folder name will later determine the model&rsquo;s name during inference API calls. In our case, this will be <code>demo-model-125m</code>, derived from the base folder name of the model.</p><h3 id=step-3-deploy-the-azureml-managed-online-endpoint class="relative group">Step 3: Deploy the AzureML Managed Online Endpoint <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-3-deploy-the-azureml-managed-online-endpoint aria-label=Anchor>#</a></span></h3><p>It&rsquo;s deployment time! First, we create our <code>endpoint.yml</code> file to define the Managed Online Endpoint:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>$schema</span><span class=p>:</span><span class=w> </span><span class=l>https://azuremlsdk2.blob.core.windows.net/latest/managedOnlineEndpoint.schema.json</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vllm-hf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>auth_mode</span><span class=p>:</span><span class=w> </span><span class=l>key</span><span class=w>
</span></span></span></code></pre></div><p>Then, we create it:</p><pre tabindex=0><code class=language-cli data-lang=cli>az ml online-endpoint create -f endpoint.yml
</code></pre><p>For the next step, we&rsquo;ll need the Docker image address, which we can quickly get from AzureML Studio -> Environments -> <code>vllm-custom-model</code>:</p><p><figure><img src=/images/vllm_docker_image_link.png alt="Docker Image address" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Docker Image address</figcaption></figure></p><p>Finally, we create a <code>deployment.yml</code> file to configure the deployment settings and deploy our desired model from HuggingFace via vLLM:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>$schema</span><span class=p>:</span><span class=w> </span><span class=l>https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>current</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>endpoint_name</span><span class=p>:</span><span class=w> </span><span class=l>vllm-hf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>model</span><span class=p>:</span><span class=w> </span><span class=l>azureml:demo-model-125m:1</span><span class=w> </span><span class=c># specify our registered model</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>model_mount_path</span><span class=p>:</span><span class=w> </span><span class=l>/models</span><span class=w> </span><span class=c># mount to /models path, so model will show up /models/demo-model-125m </span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>environment_variables</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>MODEL_PATH</span><span class=p>:</span><span class=w> </span><span class=l>/models/demo-model-125m</span><span class=w> </span><span class=c># this will need to be set, so vLLM knows where to find the model</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>VLLM_ARGS</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>xxxxxx.azurecr.io/azureml/azureml_xxxxxxxx</span><span class=w> </span><span class=c># paste Docker image address here</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>inference_config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>liveness_route</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/health</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>readiness_route</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/health</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>scoring_route</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>instance_type</span><span class=p>:</span><span class=w> </span><span class=l>Standard_NC24ads_A100_v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>instance_count</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>request_settings</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>max_concurrent_requests_per_instance</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>request_timeout_ms</span><span class=p>:</span><span class=w> </span><span class=m>10000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>liveness_probe</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>initial_delay</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>period</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>timeout</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>success_threshold</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>failure_threshold</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>readiness_probe</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>initial_delay</span><span class=p>:</span><span class=w> </span><span class=m>120</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>period</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>timeout</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>success_threshold</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>failure_threshold</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span></span></span></code></pre></div><p>Here, our focus should be on top the top section:</p><ul><li><code>model: azureml:demo-model-125m:1</code> - This is the identifier under which our model was registered (<code>azureml:&lt;name>:&lt;version></code>)</li><li><code>model_mount_path: /models</code> - This is to tell our Managed Online Endpoint, under which mount point it should mount the model</li><li><code>environment_variables</code> &ndash;> <code>MODEL_PATH: /models/demo-model-125m</code> - This is the path were our vLLM Docker container will look for the model&rsquo;s files</li><li><code>environment_variables</code> &ndash;> <code>VLLM_ARGS: ""</code> - Any additional args for vLLM (see section above)</li></ul><p>For configuring the <code>request_settings</code> section properly, see the steps above.</p><p>Lastly, we can deploy the model:</p><pre tabindex=0><code class=language-cli data-lang=cli>az ml online-deployment create -f deployment.yml --all-traffic
</code></pre><h3 id=step-4---testing-the-deployment class="relative group">Step 4 - Testing the deployment <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-4---testing-the-deployment aria-label=Anchor>#</a></span></h3><p>Again, let&rsquo;s get the endpoints scoring uri and the api keys:</p><pre tabindex=0><code class=language-cli data-lang=cli>az ml online-endpoint show -n vllm-hf
az ml online-endpoint get-credentials -n vllm-hf
</code></pre><p>We can then call the endpoint using this Python code snippet:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>url</span> <span class=o>=</span> <span class=s2>&#34;https://vllm-hf.polandcentral.inference.ml.azure.com/v1/completions&#34;</span>
</span></span><span class=line><span class=cl><span class=n>headers</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Content-Type&#34;</span><span class=p>:</span> <span class=s2>&#34;application/json&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Authorization&#34;</span><span class=p>:</span> <span class=s2>&#34;Bearer xxxxxxxxxxxx&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=s2>&#34;/models/demo-model-125m&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;prompt&#34;</span><span class=p>:</span> <span class=s2>&#34;San Francisco is a&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;max_tokens&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.7</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>headers</span><span class=o>=</span><span class=n>headers</span><span class=p>,</span> <span class=n>json</span><span class=o>=</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>())</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;id&#34;</span><span class=p>:</span><span class=s2>&#34;cmpl-50b8b30f820b418689576bc23ece3d16&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;object&#34;</span><span class=p>:</span><span class=s2>&#34;text_completion&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;created&#34;</span><span class=p>:</span><span class=mi>1728471381</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;model&#34;</span><span class=p>:</span><span class=s2>&#34;/models/demo-model-125m&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;choices&#34;</span><span class=p>:[</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;index&#34;</span><span class=p>:</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;text&#34;</span><span class=p>:</span><span class=s2>&#34; great place to live.\nI&#39;ve heard of San Francisco, but I&#39;ve never been.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;logprobs&#34;</span><span class=p>:</span><span class=s2>&#34;None&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;finish_reason&#34;</span><span class=p>:</span><span class=s2>&#34;stop&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;stop_reason&#34;</span><span class=p>:</span><span class=s2>&#34;None&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=nt>&#34;prompt_logprobs&#34;</span><span class=p>:</span><span class=s2>&#34;None&#34;</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>   <span class=p>],</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;usage&#34;</span><span class=p>:{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;prompt_tokens&#34;</span><span class=p>:</span><span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;total_tokens&#34;</span><span class=p>:</span><span class=mi>25</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;completion_tokens&#34;</span><span class=p>:</span><span class=mi>20</span>
</span></span><span class=line><span class=cl>   <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=autoscaling-our-vllm-endpoint class="relative group">Autoscaling our vLLM endpoint <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#autoscaling-our-vllm-endpoint aria-label=Anchor>#</a></span></h2><p>Autoscaling Managed Online Endpoint deployments in Azure Machine Learning allows us to dynamically adjust the number of instances allocated to our endpoints based on real-time metrics and schedules. This ensures that our application can handle varying loads efficiently without manual intervention. By integrating with Azure Monitor, you can set up rules to scale out when the CPU or GPU utilization exceeds a certain threshold or scale in during off-peak hours. For detailed guidance on configuring autoscaling, you can refer to the <a href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-autoscale-endpoints?view=azureml-api-2&amp;tabs=cli" target=_blank rel=noreferrer>official documentation</a>.</p><h2 id=summary class="relative group">Summary <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#summary aria-label=Anchor>#</a></span></h2><p>In this post, we&rsquo;ve discussed how to deploy vLLM models using Azure Machine Learning&rsquo;s Managed Online Endpoints for efficient real-time inference. We introduced vLLM as a high-throughput, memory-efficient inference engine for LLMs, with the focus of deploying models from HuggingFace. The guide outlined the steps for creating a custom environment, defining the endpoint, and deploying a model using Azure CLI commands. We also looked at examples for testing the deployed model. Additionally, we explored how to deploy custom models while keeping them private.</p></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=flex><div class=place-self-center><div class="text-2xl sm:text-lg"></div></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.facebook.com/sharer/sharer.php?u=https://clemenssiebler.com/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/&amp;quote=Deploying%20vLLM%20models%20on%20Azure%20Machine%20Learning%20with%20Managed%20Online%20Endpoints" title="Share on Facebook" aria-label="Share on Facebook" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://clemenssiebler.com/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/&amp;text=Deploying%20vLLM%20models%20on%20Azure%20Machine%20Learning%20with%20Managed%20Online%20Endpoints" title="Tweet on Twitter" aria-label="Tweet on Twitter" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clemenssiebler.com/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/&amp;description=Deploying%20vLLM%20models%20on%20Azure%20Machine%20Learning%20with%20Managed%20Online%20Endpoints" title="Pin on Pinterest" aria-label="Pin on Pinterest" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M496 256c0 137-111 248-248 248-25.6.0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8.0 128.7-68.8 128.7-154.3.0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1.0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6.0-54.7 41.4-107.6 112-107.6 60.9.0 103.6 41.5 103.6 100.9.0 67.1-33.9 113.6-78 113.6-24.3.0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6.0-19-10.2-34.9-31.4-34.9-24.9.0-44.9 25.7-44.9 60.2.0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9.0 361.1.0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://reddit.com/submit/?url=https://clemenssiebler.com/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/&amp;resubmit=true&amp;title=Deploying%20vLLM%20models%20on%20Azure%20Machine%20Learning%20with%20Managed%20Online%20Endpoints" title="Submit to Reddit" aria-label="Submit to Reddit" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://clemenssiebler.com/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/&amp;title=Deploying%20vLLM%20models%20on%20Azure%20Machine%20Learning%20with%20Managed%20Online%20Endpoints" title="Share on LinkedIn" aria-label="Share on LinkedIn" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://clemenssiebler.com/posts/vllm-on-azure-machine-learning-managed-online-endpoints-deployment/&amp;subject=Deploying%20vLLM%20models%20on%20Azure%20Machine%20Learning%20with%20Managed%20Online%20Endpoints" title="Send via email" aria-label="Send via email" target=_blank rel="noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/versioning-azure-openai-endpoints-behind-api-management/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Versioning Azure OpenAI Endpoints behind Azure API Management</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-06-27 00:00:00 +0000 UTC">27 June 2024</time>
</span></span></a></span><span><a class="group flex text-right" href=/posts/deploying-bge-m3-embedding-models-azure-machine-learning-managed-online-endpoints/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Deploying BGE-M3 and other embedding models on Azure Machine Learning with Managed Online Endpoints</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-10-22 00:00:00 +0000 UTC">22 October 2024</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".prose img"));images.forEach(e=>{mediumZoom(e,{margin:5,scrollOffset:40,container:null,template:null})})</script></footer></div></body></html>